Soot started on Fri Sep 11 19:34:52 CDT 2020
Soot finished on Fri Sep 11 19:34:55 CDT 2020
Soot has run for 0 min. 2 sec.
Soot started on Fri Sep 11 19:34:55 CDT 2020
Soot finished on Fri Sep 11 19:34:57 CDT 2020
Soot has run for 0 min. 2 sec.
The sink $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ms") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l9 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l10 = $l9 - l8
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l10)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ms")
The sink if z0 == 0 goto virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: void setUri(java.net.URI,boolean)>(r1, z0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.delegation.token.binding", "") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.delegation.token.binding", "")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>($r1)
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if z0 == 0 goto virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: void setUri(java.net.URI,boolean)>(r1, z0)
The sink $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("") in method <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("")
The sink if $i0 > 0 goto return in method <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.delegation.token.service.urls") in method <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.delegation.token.service.urls")
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: java.lang.String[] dtServiceUrls> = $r4
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = r0.<org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: java.lang.String[] dtServiceUrls>
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = lengthof $r7
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> if $i0 > 0 goto return
The sink staticinvoke <java.lang.Thread: void sleep(long)>(l10) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l9 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l10 = $l9 - l8
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> staticinvoke <java.lang.Thread: void sleep(long)>(l10)
The sink r11 = virtualinvoke $r9.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r10) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r9 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.output.format", "csv") in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.output.format", "csv")
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r11 = virtualinvoke $r9.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r10)
The sink if $b11 <= 0 goto $r22 = new java.lang.StringBuilder in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l9 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l10 = $l9 - l8
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $b11 = l10 cmp 0L
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $b11 <= 0 goto $r22 = new java.lang.StringBuilder
The sink if $b6 < 0 goto $l11 = l1 in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f3 = $f2 / f4
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l17 = (long) $f3
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r40.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l17, $l8, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l5 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $b6 = $l5 cmp 0L
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> if $b6 < 0 goto $l11 = l1
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f1 = $f0 / f5
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l18 = (long) $f1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r43.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l18, $l2, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l5 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $b6 = $l5 cmp 0L
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> if $b6 < 0 goto $l11 = l1
The sink specialinvoke r0.<java.lang.Thread: void <init>(java.lang.String)>(r2) in method <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)> was called with values from the following sources:
- $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F) in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F)
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: float rateFactor> = $f0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>(r1, "StressJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: org.apache.hadoop.mapred.gridmix.StressJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> specialinvoke r0.<java.lang.Thread: void <init>(java.lang.String)>(r2)
The sink $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r19) in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r19)
The sink interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>($r39) in method <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)> was called with values from the following sources:
- i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getResourceForContainer(java.util.Map)>
		 -> return r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> specialinvoke $r39.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r33, l3, r10, i7, r34, r35, l10, l12)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>($r39)
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getResourceForContainer(java.util.Map)>
		 -> return r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> specialinvoke $r39.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r33, l3, r10, i7, r34, r35, l10, l12)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>($r39)
The sink if $z5 == 0 goto $r38 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: org.apache.hadoop.conf.Configuration sessionConfiguration> in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $z4 = virtualinvoke $r30.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.autothrottling.enable", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z4 = virtualinvoke $r30.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.autothrottling.enable", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean autoThrottlingEnabled> = $z4
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z5 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean autoThrottlingEnabled>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> if $z5 == 0 goto $r38 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: org.apache.hadoop.conf.Configuration sessionConfiguration>
The sink $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("array") in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> r1 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("array")
The sink $r26 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i21) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $i12 = virtualinvoke $r14.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.retries", 30) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i12 = virtualinvoke $r14.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.retries", 30)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxRetries> = $i12
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i21 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxRetries>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r26 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i21)
The sink $r7 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r7 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1)
The sink $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>()
The sink virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r10) in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r5 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Field[] getDeclaredFields()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r6 = r5
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r7 = r6[i3]
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r10)
The sink $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" defined in ") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" defined in ")
The sink virtualinvoke $r1.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding: void start()>() in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStart()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStart()>()
	 -> <org.apache.hadoop.service.AbstractService: void serviceStart()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> virtualinvoke $r1.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding: void start()>()
The sink $z1 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: boolean hasAttribute(java.lang.String)>("is_dir") in method <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":last_updated", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r5 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r5.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_deleted", 1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r0, $r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3> = $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r4 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r5 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r2, $r3, $r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.ItemCollection scan(java.lang.String,java.lang.String,java.util.Map,java.util.Map)>(r1, r2, null, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r10 = (com.amazonaws.services.dynamodbv2.document.ItemCollection) $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> return $r10
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r14 = virtualinvoke r13.<com.amazonaws.services.dynamodbv2.document.ItemCollection: com.amazonaws.services.dynamodbv2.document.internal.IteratorSupport iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r19 = interfaceinvoke r14.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r20 = (com.amazonaws.services.dynamodbv2.document.Item) $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r22 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>(r20, $r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> $z1 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: boolean hasAttribute(java.lang.String)>("is_dir")
The sink if z2 == 0 goto $r8 = new org.apache.hadoop.fs.s3a.select.SelectInputStream in method <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)> was called with values from the following sources:
- z2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", $z1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)>
		 -> z2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", $z1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)>
		 -> if z2 == 0 goto $r8 = new org.apache.hadoop.fs.s3a.select.SelectInputStream
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l3) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l3 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l3)
The sink virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyHost(java.lang.String)>(r1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.proxy.host", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.proxy.host", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyHost(java.lang.String)>(r1)
The sink $r5 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_2__187: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r4) in method <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r15 = (java.lang.String) $r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>(r16)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> r11 = virtualinvoke $r9.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withTags(java.util.Collection)>($r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r20 = virtualinvoke $r19.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table createTable(com.amazonaws.services.dynamodbv2.model.CreateTableRequest)>(r11)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r20
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r22 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>(r0, r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: com.amazonaws.services.dynamodbv2.document.Table cap1> = $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_2__187: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r4)
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r6 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r59 = virtualinvoke $r6.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.model.TableDescription describe()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r8 = virtualinvoke r59.<com.amazonaws.services.dynamodbv2.model.TableDescription: java.lang.String getTableArn()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableArn> = $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>(r0, r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_2__187: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r4)
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r33 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r6 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r59 = virtualinvoke $r6.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.model.TableDescription describe()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r8 = virtualinvoke r59.<com.amazonaws.services.dynamodbv2.model.TableDescription: java.lang.String getTableArn()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableArn> = $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>(r0, r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_waitForTableActive_1__139: org.apache.hadoop.fs.s3a.Invoker$VoidOperation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.Table)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_2__187: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r4)
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", numMaps:")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i1)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(". Reduce numMaps or decrease split-ratio to proceed.")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $z4 = virtualinvoke $r10.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.identity.transformer.service.principal.substitution.list", "") in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.identity.transformer.service.principal.substitution.list", "")
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: java.lang.String serviceWhiteList> = $r5
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r10 = r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: java.lang.String serviceWhiteList>
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z4 = virtualinvoke $r10.<java.lang.String: boolean isEmpty()>()
The sink specialinvoke $r9.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r10) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r10 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r9.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r10)
The sink $r0 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f0) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)> was called with values from the following sources:
- $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>($f2)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>(f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $r0 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f0)
- $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>($f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $r0 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f0)
The sink $r7 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Signer class [%s] not found for signer [%s]", $r5) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r9 = r6[0]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>($r9, $r8, $r7)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r5[1] = r0
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r7 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Signer class [%s] not found for signer [%s]", $r5)
The sink $r14 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>(r6) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r14 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>(r6)
The sink $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r25) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" defined in ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(": ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r25)
The sink $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
The sink $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r18)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\"")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z1 == 0 goto return in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.authorization.caching.enable", 0) in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.authorization.caching.enable", 0)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: boolean isEnabled> = $z0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: boolean isEnabled>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto return
The sink $r3 = staticinvoke <java.lang.String: java.lang.String valueOf(int)>(i0) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r2 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String valueOf(int)>(i0)
The sink $r8 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l1) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l1)
The sink $r10 = virtualinvoke r1.<org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation: org.apache.hadoop.yarn.api.records.Resource getCapacityAtTime(long)>($l12) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $i11 = i17 * i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l12 = (long) $i11
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation: org.apache.hadoop.yarn.api.records.Resource getCapacityAtTime(long)>($l12)
The sink $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("")
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir") in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir")
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
The sink $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.service.Service$STATE getServiceState()>() in method <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceState(org.apache.hadoop.service.Service$STATE)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStart()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStart()>()
	 -> <org.apache.hadoop.service.AbstractService: void serviceStart()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.security.token.Token selectTokenFromFSOwner()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.security.token.Token selectTokenFromFSOwner()>
		 -> return $r6
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void deployUnbonded()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void deployUnbonded()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void requireServiceStarted()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceStarted()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceState(org.apache.hadoop.service.Service$STATE)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: boolean isInState(org.apache.hadoop.service.Service$STATE)>(r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceState(org.apache.hadoop.service.Service$STATE)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.service.Service$STATE getServiceState()>()
The sink specialinvoke r0.<java.lang.Object: void <init>()>() in method <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)> was called with values from the following sources:
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.heartbeat.interval.ms", 1000) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.heartbeat.interval.ms", 1000)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> specialinvoke $r38.<org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>(r0, r13, r30, i0, f0, r31)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$2: int val$heartbeatInterval> = i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
- f0 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("yarn.sls.nm.resource.utilization.ratio", -1.0F) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> f0 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("yarn.sls.nm.resource.utilization.ratio", -1.0F)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> specialinvoke $r38.<org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>(r0, r13, r30, i0, f0, r31)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$2: float val$resourceUtilizationRatio> = f0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> throw $r30
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> specialinvoke $r38.<org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>(r0, r13, r30, i0, f0, r31)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$2: org.apache.hadoop.yarn.sls.SLSRunner this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
The sink $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r23 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
The sink $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":mod_time", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r13 = virtualinvoke $r12.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_dir", 1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21)
The sink $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)> was called with values from the following sources:
- $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: long startTimestampMs> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $l1 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l1)
The sink if i4 < 0 goto r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- i4 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.proxy.port", -1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> i4 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.proxy.port", -1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if i4 < 0 goto r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "")
The sink $r3 = staticinvoke <java.lang.String: java.lang.String valueOf(java.lang.Object)>(r2) in method <org.apache.hadoop.tools.util.DistCpUtils: void publish(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Object)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i5 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r3 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i5)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: void publish(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Object)>($r2, "mapred.num.entries.per.chunk", $r3)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: void publish(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Object)>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String valueOf(java.lang.Object)>(r2)
The sink if $b3 > 0 goto $r5 = new org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1 in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l2 = virtualinvoke $r4.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l1, $r3)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $b3 = l2 cmp 0L
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> if $b3 > 0 goto $r5 = new org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1
The sink $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(int)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r1 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(int)>($i0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(int)>
		 -> $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink $f2 = staticinvoke <java.lang.Math: float min(float,float)>(1.0F, $f1) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f2 = staticinvoke <java.lang.Math: float min(float,float)>(1.0F, $f1)
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyWorkstation(java.lang.String)>($r28) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $r28 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.workstation") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r28 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.workstation")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyWorkstation(java.lang.String)>($r28)
The sink $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
The sink if i0 < 0 goto i1 = -1 in method <org.apache.hadoop.mapred.gridmix.Statistics: org.apache.hadoop.mapred.gridmix.Statistics$JobStats generateJobStats(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.rumen.JobStory)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: org.apache.hadoop.mapred.gridmix.Statistics$JobStats generateJobStats(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.rumen.JobStory)>
		 -> if i0 < 0 goto i1 = -1
The sink specialinvoke r0.<org.apache.hadoop.fs.PathIOException: void <init>(java.lang.String,java.lang.String)>($r4, r2) in method <org.apache.hadoop.fs.s3a.commit.PathCommitException: void <init>(org.apache.hadoop.fs.Path,java.lang.String)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r18)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\"")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r25.<org.apache.hadoop.fs.s3a.commit.PathCommitException: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r10, $r15)
	 -> <org.apache.hadoop.fs.s3a.commit.PathCommitException: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.PathIOException: void <init>(java.lang.String,java.lang.String)>($r4, r2)
The sink if i0 >= 0 goto $r2 = <org.apache.hadoop.mapred.gridmix.Statistics: java.util.Map submittedJobsMap> in method <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> if i0 >= 0 goto $r2 = <org.apache.hadoop.mapred.gridmix.Statistics: java.util.Map submittedJobsMap>
The sink $z2 = virtualinvoke r4.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)>
		 -> $z2 = virtualinvoke r4.<java.lang.String: boolean isEmpty()>()
The sink if $z5 == 0 goto $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.track.missing.source") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $z5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.atomic.copy", 0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> $z5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.atomic.copy", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> if $z5 == 0 goto $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.track.missing.source")
The sink if $b6 <= 0 goto $i7 = (int) l8 in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- l8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)>("fs.s3a.connection.request.timeout", 0L, $r3, $r2) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> l8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)>("fs.s3a.connection.request.timeout", 0L, $r3, $r2)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $b6 = l8 cmp 2147483647L
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> if $b6 <= 0 goto $i7 = (int) l8
The sink r6 = virtualinvoke r7.<java.net.URI: java.lang.String getHost()>() in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<java.net.URI: void <init>(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r7 = $r3
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r6 = virtualinvoke r7.<java.net.URI: java.lang.String getHost()>()
The sink if $z1 == 0 goto return in method <org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: void setConf(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.sls.metrics.switch", 1) in method <org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.sls.metrics.switch", 1)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: boolean metricsON> = $z0
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = r0.<org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: boolean metricsON>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto return
The sink $d3 = staticinvoke <java.lang.Math: double ceil(double)>($d2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = (float) i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> i4 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $i6 = i3 * i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f3 = (float) $i6
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f5 = $f4 / $f3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d2 = (double) $f5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d3 = staticinvoke <java.lang.Math: double ceil(double)>($d2)
The sink if $z3 == 0 goto $r38 = r0.<org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.conf.Configuration val$conf> in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> if $z3 == 0 goto $r38 = r0.<org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.conf.Configuration val$conf>
The sink if r4 == null goto virtualinvoke r23.<java.util.jar.JarOutputStream: void close()>() in method <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> if r4 == null goto virtualinvoke r23.<java.util.jar.JarOutputStream: void close()>()
The sink interfaceinvoke $r3.<java.util.concurrent.ExecutorService: void execute(java.lang.Runnable)>($r1) in method <org.apache.hadoop.tools.util.ProducerConsumer: void addWorker(org.apache.hadoop.tools.util.WorkRequestProcessor)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>(r1, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $i0 = r5.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> specialinvoke $r4.<org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>($i0)
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>
		 -> $r4 = staticinvoke <java.util.concurrent.Executors: java.util.concurrent.ExecutorService newFixedThreadPool(int)>(i0)
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>
		 -> r0.<org.apache.hadoop.tools.util.ProducerConsumer: java.util.concurrent.ExecutorService executor> = $r4
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> r6 = $r4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> virtualinvoke r6.<org.apache.hadoop.tools.util.ProducerConsumer: void addWorker(org.apache.hadoop.tools.util.WorkRequestProcessor)>($r47)
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void addWorker(org.apache.hadoop.tools.util.WorkRequestProcessor)>
		 -> $r3 = r0.<org.apache.hadoop.tools.util.ProducerConsumer: java.util.concurrent.ExecutorService executor>
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void addWorker(org.apache.hadoop.tools.util.WorkRequestProcessor)>
		 -> interfaceinvoke $r3.<java.util.concurrent.ExecutorService: void execute(java.lang.Runnable)>($r1)
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $i0 = r5.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> specialinvoke $r4.<org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>($i0)
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>
		 -> $r4 = staticinvoke <java.util.concurrent.Executors: java.util.concurrent.ExecutorService newFixedThreadPool(int)>(i0)
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>
		 -> r0.<org.apache.hadoop.tools.util.ProducerConsumer: java.util.concurrent.ExecutorService executor> = $r4
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void <init>(int)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> r6 = $r4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> virtualinvoke r6.<org.apache.hadoop.tools.util.ProducerConsumer: void addWorker(org.apache.hadoop.tools.util.WorkRequestProcessor)>($r47)
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void addWorker(org.apache.hadoop.tools.util.WorkRequestProcessor)>
		 -> $r3 = r0.<org.apache.hadoop.tools.util.ProducerConsumer: java.util.concurrent.ExecutorService executor>
	 -> <org.apache.hadoop.tools.util.ProducerConsumer: void addWorker(org.apache.hadoop.tools.util.WorkRequestProcessor)>
		 -> interfaceinvoke $r3.<java.util.concurrent.ExecutorService: void execute(java.lang.Runnable)>($r1)
The sink if r19 == null goto $r1 = staticinvoke <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>(r0) in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> if r19 == null goto $r1 = staticinvoke <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>(r0)
The sink if $z1 == 0 goto $r2 = new org.apache.hadoop.fs.Path in method <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("")
	 -> <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto $r2 = new org.apache.hadoop.fs.Path
The sink interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>($r7) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r15 = (java.lang.String) $r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>(r16)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>($r7)
The sink r4 = staticinvoke <org.apache.commons.lang3.StringUtils: java.lang.String join(java.lang.Iterable,char)>(r3, 44) in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("hadoop.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("hadoop.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r3.<java.util.List: boolean addAll(java.util.Collection)>(r2)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r4 = staticinvoke <org.apache.commons.lang3.StringUtils: java.lang.String join(java.lang.Iterable,char)>(r3, 44)
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r3 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList(java.lang.Iterable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r4 = staticinvoke <org.apache.commons.lang3.StringUtils: java.lang.String join(java.lang.Iterable,char)>(r3, 44)
The sink $r23 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i18) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $i9 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.min.backoff.interval", 3000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i9 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.min.backoff.interval", 3000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int minBackoff> = $i9
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i18 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int minBackoff>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r23 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i18)
The sink if $z10 == 0 goto specialinvoke r5.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path)>(r34, r33, r35) in method <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)> was called with values from the following sources:
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $z10 = r5.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> if $z10 == 0 goto specialinvoke r5.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path)>(r34, r33, r35)
The sink if $r1 == null goto $z0 = 0 in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("createfile.num-mappers") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("createfile.num-mappers")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> if $r1 == null goto $z0 = 0
The sink virtualinvoke $r37.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void start()>() in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L) in method <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: long sleepTime> = $l0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher> = $r44
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r35 = r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher>
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r45.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>($r47, $r35)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: org.apache.hadoop.mapred.gridmix.Progressive progress> = r2
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter reporter> = $r45
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r37 = r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter reporter>
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r37.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void start()>()
The sink $r7 = virtualinvoke $r5.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withAttributeDefinitions(java.util.Collection)>($r6) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r2 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r3 = virtualinvoke $r0.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withTableName(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r5 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withKeySchema(java.util.Collection)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r7 = virtualinvoke $r5.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withAttributeDefinitions(java.util.Collection)>($r6)
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r33 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r2 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r3 = virtualinvoke $r0.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withTableName(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r5 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withKeySchema(java.util.Collection)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r7 = virtualinvoke $r5.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withAttributeDefinitions(java.util.Collection)>($r6)
The sink interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("REMOTE_STORAGE_PATH", $r33) in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("REMOTE_STORAGE_PATH", $r33)
The sink $r12 = interfaceinvoke $r11.<com.google.common.cache.Cache: java.lang.Object getIfPresent(java.lang.Object)>(r32) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)> was called with values from the following sources:
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> interfaceinvoke $r9.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r16, null)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r5 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r11 = r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r12 = interfaceinvoke $r11.<com.google.common.cache.Cache: java.lang.Object getIfPresent(java.lang.Object)>(r32)
The sink r13 = virtualinvoke r11.<java.lang.Class: java.lang.reflect.Constructor getDeclaredConstructor(java.lang.Class[])>($r12) in method <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- r11 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("distcp.copy.listing.class", class "Lorg/apache/hadoop/tools/GlobbedCopyListing;", class "Lorg/apache/hadoop/tools/CopyListing;") in method <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("distcp.copy.listing.class", class "Lorg/apache/hadoop/tools/GlobbedCopyListing;", class "Lorg/apache/hadoop/tools/CopyListing;")
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
		 -> r13 = virtualinvoke r11.<java.lang.Class: java.lang.reflect.Constructor getDeclaredConstructor(java.lang.Class[])>($r12)
The sink interfaceinvoke r32.<java.util.concurrent.ExecutorService: java.util.concurrent.Future submit(java.lang.Runnable)>($r38) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()> was called with values from the following sources:
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.heartbeat.interval.ms", 1000) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.heartbeat.interval.ms", 1000)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> specialinvoke $r38.<org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>(r0, r13, r30, i0, f0, r31)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$2: int val$heartbeatInterval> = i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> interfaceinvoke r32.<java.util.concurrent.ExecutorService: java.util.concurrent.Future submit(java.lang.Runnable)>($r38)
- f0 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("yarn.sls.nm.resource.utilization.ratio", -1.0F) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> f0 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("yarn.sls.nm.resource.utilization.ratio", -1.0F)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> specialinvoke $r38.<org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>(r0, r13, r30, i0, f0, r31)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$2: float val$resourceUtilizationRatio> = f0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$2: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails,java.util.Random,int,float,java.util.Set)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> interfaceinvoke r32.<java.util.concurrent.ExecutorService: java.util.concurrent.Future submit(java.lang.Runnable)>($r38)
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> throw $r30
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $i1 = r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> i12 = staticinvoke <java.lang.Math: int max(int,int)>($i1, 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> r32 = staticinvoke <java.util.concurrent.Executors: java.util.concurrent.ExecutorService newFixedThreadPool(int)>(i12)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> interfaceinvoke r32.<java.util.concurrent.ExecutorService: java.util.concurrent.Future submit(java.lang.Runnable)>($r38)
The sink $i0 = virtualinvoke r1.<java.lang.String: int length()>() in method <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $i0 = virtualinvoke r1.<java.lang.String: int length()>()
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setUserAgent(java.lang.String)>($r13) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r9 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", Hadoop/")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r12 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r11)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setUserAgent(java.lang.String)>($r13)
The sink if i2 >= $i0 goto return r0 in method <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $i0 = virtualinvoke r1.<java.lang.String: int length()>()
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> if i2 >= $i0 goto return r0
The sink if $i16 != 0 goto r72 = virtualinvoke r44.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.security.token.Token[] addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)>(r70, r68) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal") in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal")
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $i16 = virtualinvoke r70.<java.lang.String: int length()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> if $i16 != 0 goto r72 = virtualinvoke r44.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.security.token.Token[] addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)>(r70, r68)
The sink $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>($r7) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r7 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>($r7)
The sink if $i0 > 0 goto $z2 = r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: boolean isKerberosSupportEnabled> in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.cred.service.urls") in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.cred.service.urls")
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: java.lang.String[] commaSeparatedUrls> = $r4
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: java.lang.String[] commaSeparatedUrls>
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = lengthof $r7
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $i0 > 0 goto $z2 = r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: boolean isKerberosSupportEnabled>
The sink $r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0)
The sink $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i19) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB> = $i9
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i19 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i19)
The sink specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r4) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> return r1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r4)
The sink if $z0 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("sequential")
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> if $z0 == 0 goto (branch)
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("sequential")
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> if $z0 == 0 goto (branch)
The sink if $z4 == 0 goto $z5 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$BooleanConfigurationValidatorAnnotation;") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r5 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Field[] getDeclaredFields()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r6 = r5
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r7 = r6[i3]
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $z4 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$Base64StringConfigurationValidatorAnnotation;")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> if $z4 == 0 goto $z5 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$BooleanConfigurationValidatorAnnotation;")
The sink if r14 != null goto return r14 in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> r14 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0, r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r4 = virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("getVersionMarkerItem", "../VERSION", 1, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r1 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r3 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r2 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.Item getItem(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r5 = (com.amazonaws.services.dynamodbv2.document.Item) $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> if r14 != null goto return r14
The sink $r17 = virtualinvoke $r16.<com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder: java.lang.Object build()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: com.amazonaws.services.dynamodbv2.AmazonDynamoDB createDynamoDBClient(java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)> was called with values from the following sources:
- r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> return r15
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: com.amazonaws.services.dynamodbv2.AmazonDynamoDB createDynamoDBClient(java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r15 = virtualinvoke $r14.<com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder: com.amazonaws.client.builder.AwsClientBuilder withRegion(java.lang.String)>(r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: com.amazonaws.services.dynamodbv2.AmazonDynamoDB createDynamoDBClient(java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r16 = (com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder) $r15
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: com.amazonaws.services.dynamodbv2.AmazonDynamoDB createDynamoDBClient(java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r17 = virtualinvoke $r16.<com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder: java.lang.Object build()>()
The sink if $z0 == 0 goto $z8 = 0 in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $z0 == 0 goto $z8 = 0
The sink if $z0 == 0 goto $r21 = r0.<org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.conf.Configuration val$conf> in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()> was called with values from the following sources:
- $z0 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.job-submission.use-queue-in-trace", 0) in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> $z0 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.job-submission.use-queue-in-trace", 0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> if $z0 == 0 goto $r21 = r0.<org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.conf.Configuration val$conf>
The sink $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" to ") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r3.<org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>(r1)
	 -> <org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.fs.Path: java.net.URI toUri()>()
	 -> <org.apache.hadoop.fs.Path: java.net.URI toUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r3)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" to ")
The sink $r19 = interfaceinvoke r14.<java.util.Iterator: java.lang.Object next()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":mod_time", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r13 = virtualinvoke $r12.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_dir", 1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r0, $r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3> = $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r4 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r5 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r2, $r3, $r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.ItemCollection scan(java.lang.String,java.lang.String,java.util.Map,java.util.Map)>(r1, r2, null, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r10 = (com.amazonaws.services.dynamodbv2.document.ItemCollection) $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> return $r10
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r14 = virtualinvoke r13.<com.amazonaws.services.dynamodbv2.document.ItemCollection: com.amazonaws.services.dynamodbv2.document.internal.IteratorSupport iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r19 = interfaceinvoke r14.<java.util.Iterator: java.lang.Object next()>()
The sink $r41 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z7) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.io.read.tolerate.concurrent.append", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.io.read.tolerate.concurrent.append", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean tolerateOobAppends> = $z0
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z7 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean tolerateOobAppends>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r41 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z7)
The sink virtualinvoke r8.<java.util.ArrayList: boolean add(java.lang.Object)>($r17) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> l4 = l0 / $l3
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> specialinvoke $r17.<org.apache.hadoop.mapred.gridmix.GenerateData$GenSplit: void <init>(long,java.lang.String[])>(l4, $r18)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenSplit: void <init>(long,java.lang.String[])>
		 -> specialinvoke r0.<org.apache.hadoop.mapred.gridmix.GenerateData$GenSplit: void <init>(long,int,java.lang.String[])>(l0, $i1, r1)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenSplit: void <init>(long,int,java.lang.String[])>
		 -> r0.<org.apache.hadoop.mapred.gridmix.GenerateData$GenSplit: long bytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenSplit: void <init>(long,int,java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenSplit: void <init>(long,java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> virtualinvoke r8.<java.util.ArrayList: boolean add(java.lang.Object)>($r17)
The sink if $r13 != $r14 goto $z0 = interfaceinvoke r5.<java.util.Iterator: boolean hasNext()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r8 = r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$1: org.apache.hadoop.yarn.sls.SLSRunner val$se> = r2
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm> = $r24
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> throw $r40
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>
		 -> throw $r10
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r2 = virtualinvoke $r1.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.RMContext getRMContext()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r3 = interfaceinvoke $r2.<org.apache.hadoop.yarn.server.resourcemanager.RMContext: java.util.concurrent.ConcurrentMap getRMNodes()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r4 = interfaceinvoke $r3.<java.util.concurrent.ConcurrentMap: java.util.Collection values()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> r5 = interfaceinvoke $r4.<java.util.Collection: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r11 = interfaceinvoke r5.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> r12 = (org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode) $r11
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r13 = interfaceinvoke r12.<org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode: org.apache.hadoop.yarn.api.records.NodeState getState()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> if $r13 != $r14 goto $z0 = interfaceinvoke r5.<java.util.Iterator: boolean hasNext()>()
The sink $r6 = interfaceinvoke $r3.<java.util.Map: java.lang.Object remove(java.lang.Object)>($r5) in method <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r5 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i0)
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r6 = interfaceinvoke $r3.<java.util.Map: java.lang.Object remove(java.lang.Object)>($r5)
The sink specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r8) in method <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r15 = staticinvoke <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>(r1, r2, null)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r8)
The sink $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/jobruntime.csv") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r23 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/jobruntime.csv")
The sink if i0 <= 0 goto r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = 0 in method <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> if i0 <= 0 goto r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = 0
The sink r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>(r1, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> throw r40
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r4, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> throw r44
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r4, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> throw r44
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.CopyFilter copyFilter> = $r9
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.tools.CopyListing simpleListing> = $r3
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r22
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/oss") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/oss")
The sink interfaceinvoke r4.<java.util.Collection: boolean add(java.lang.Object)>($r12) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> interfaceinvoke r4.<java.util.Collection: boolean add(java.lang.Object)>($r12)
- $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean errorsIncludeSql> = $z1
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> interfaceinvoke r4.<java.util.Collection: boolean add(java.lang.Object)>($r12)
The sink $r13 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f8) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f8 = $f5 * $f6
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r13 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f8)
The sink staticinvoke <com.amazonaws.auth.SignerFactory: com.amazonaws.auth.Signer getSignerByTypeAndService(java.lang.String,java.lang.String)>(r0, null) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r9 = r6[0]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>($r9, $r8, $r7)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <com.amazonaws.auth.SignerFactory: com.amazonaws.auth.Signer getSignerByTypeAndService(java.lang.String,java.lang.String)>(r0, null)
The sink $r13 = virtualinvoke r6.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> $r13 = virtualinvoke r6.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
The sink if i1 >= i0 goto return r3 in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.num-mappers", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.num-mappers", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> if i1 >= i0 goto return r3
The sink r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r32 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r27 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r38 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r39 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>("initTable", $r38, r70)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r47 = virtualinvoke $r46.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[1] = $r48
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r33 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r38 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r39 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>("initTable", $r38, r70)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r47 = virtualinvoke $r46.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[1] = $r48
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r7 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink if $r2 == null goto $z0 = 0 in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("createfile.duration-min") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("createfile.duration-min")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> if $r2 == null goto $z0 = 0
The sink $r37 = staticinvoke <java.util.Arrays: java.util.List asList(java.lang.Object[])>($r36) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r36 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.azure.daemon.userlist", $r35) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r36 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.azure.daemon.userlist", $r35)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r37 = staticinvoke <java.util.Arrays: java.util.List asList(java.lang.Object[])>($r36)
The sink $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(char)>(c1) in method <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $c1 = virtualinvoke r1.<java.lang.String: char charAt(int)>(i2)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $r2 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>($c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(char)>(c1)
The sink $r91 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r42, $r41, r2, $r39)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> $r91 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink if r39 != null goto (branch) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> if r39 != null goto (branch)
The sink specialinvoke r0.<org.apache.hadoop.service.CompositeService: void serviceStop()>() in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void serviceStop()> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>(r14)
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> throw r12
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r15, r0)
	 -> <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>(r0)
	 -> <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>
		 -> interfaceinvoke r0.<org.apache.hadoop.service.Service: void stop()>()
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>($r4)
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> return r3
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStop()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> $r7 = r1.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r8, $r7)
	 -> <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>(r0)
	 -> <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>
		 -> interfaceinvoke r0.<org.apache.hadoop.service.Service: void stop()>()
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>($r4)
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>()
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> return $r2
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> specialinvoke r0.<org.apache.hadoop.service.AbstractService: void recordLifecycleEvent()>()
	 -> <org.apache.hadoop.service.AbstractService: void recordLifecycleEvent()>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>()
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> $r1 = r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.ServiceStateModel stateModel>
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> $r2 = virtualinvoke $r1.<org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE getState()>()
	 -> <org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE getState()>
		 -> $r1 = r0.<org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE state>
	 -> <org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE getState()>
		 -> return $r1
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> return $r2
	 -> <org.apache.hadoop.service.AbstractService: void recordLifecycleEvent()>
		 -> r1.<org.apache.hadoop.service.LifecycleEvent: org.apache.hadoop.service.Service$STATE state> = $r3
	 -> <org.apache.hadoop.service.AbstractService: void recordLifecycleEvent()>
		 -> interfaceinvoke $r4.<java.util.List: boolean add(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.service.AbstractService: void recordLifecycleEvent()>
		 -> $r4 = r2.<org.apache.hadoop.service.AbstractService: java.util.List lifecycleHistory>
	 -> <org.apache.hadoop.service.AbstractService: void recordLifecycleEvent()>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> return r3
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStop()>()
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void serviceStop()>
		 -> specialinvoke r0.<org.apache.hadoop.service.CompositeService: void serviceStop()>()
The sink $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f2)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the reduce output data.")
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if i4 <= 0 goto $i24 = 90 in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- i4 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.storage.timeout", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> i4 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.storage.timeout", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> if i4 <= 0 goto $i24 = 90
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke $r2.<java.net.URI: void <init>(java.lang.String)>(r1) in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("test.fs.swift.name") in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("test.fs.swift.name")
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r2.<java.net.URI: void <init>(java.lang.String)>(r1)
The sink $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r7) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r7)
- r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r7)
The sink $r7 = virtualinvoke $r5.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterWrite(long,java.util.concurrent.TimeUnit)>($l2, $r6) in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.authorization.caching.maxentries", 512) in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.authorization.caching.maxentries", 512)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = (long) $i0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke $r4.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r5.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterWrite(long,java.util.concurrent.TimeUnit)>($l2, $r6)
- l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15) in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15)
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l7 = l4
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l8 = $l7
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>(l8, "SASKEY")
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes> = l0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache> = $r10
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache>
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r11.<org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $l2 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r5.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterWrite(long,java.util.concurrent.TimeUnit)>($l2, $r6)
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2) in method <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2)
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod> = $l0
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>()
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> $l0 = r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod>
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> return $l0
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>($l0, "SASKEY")
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes> = l0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache> = $r3
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = r0.<org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache>
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r4.<org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>(r1)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $l2 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r5.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterWrite(long,java.util.concurrent.TimeUnit)>($l2, $r6)
The sink $r24 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i19) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $i11 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.backoff.interval", 3000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i11 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.backoff.interval", 3000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int deltaBackoff> = $i11
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i19 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int deltaBackoff>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r24 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i19)
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("fs.oss.buffer.dir", $r11) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/oss")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("fs.oss.buffer.dir", $r11)
The sink if i6 >= $i4 goto return in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads> = $i1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>($r0)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> $r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper cap0> = $r1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: java.util.function.Function relativeToAbsoluteTimestamp> = $r11
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>($r0)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i4 = r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if i6 >= $i4 goto return
The sink specialinvoke $r3.<org.apache.hadoop.fs.FSDataInputStream: void <init>(java.io.InputStream)>($r4) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,int)> was called with values from the following sources:
- $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> = $l1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>(0L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> throw r43
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> return
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,int)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.FSDataInputStream: void <init>(java.io.InputStream)>($r4)
The sink if $r5 != class "Lorg/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler;" goto $r6 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()> was called with values from the following sources:
- r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class") in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class")
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r5 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> if $r5 != class "Lorg/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler;" goto $r6 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
The sink $z1 = interfaceinvoke r11.<java.util.Set: boolean contains(java.lang.Object)>(r10) in method <org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.AWSCredentialProviderList buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r7[0] = $r8
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = staticinvoke <com.google.common.collect.Sets: java.util.HashSet newHashSet(java.lang.Object[])>($r7)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r10 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.AWSCredentialProviderList buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set)>(r4, r1, "fs.s3a.assumed.role.credentials.provider", $r6, $r9)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.AWSCredentialProviderList buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set)>
		 -> $z1 = interfaceinvoke r11.<java.util.Set: boolean contains(java.lang.Object)>(r10)
The sink r7 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run(java.lang.String[])>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: int run()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>
		 -> r7 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run(java.lang.String[])>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: int run()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>
		 -> r7 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
The sink virtualinvoke $r28.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void start()>() in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L) in method <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: long sleepTime> = $l0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher> = $r33
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r27 = r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher>
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> specialinvoke $r34.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>($r37, $r27)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: org.apache.hadoop.mapred.gridmix.Progressive progress> = r2
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter reporter> = $r34
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r28 = r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter reporter>
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> virtualinvoke $r28.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void start()>()
The sink if $z0 == 0 goto return null in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.store.EtagChecksum getFileChecksum(org.apache.hadoop.fs.Path,long)> was called with values from the following sources:
- $z0 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.etag.checksum.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.store.EtagChecksum getFileChecksum(org.apache.hadoop.fs.Path,long)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.store.EtagChecksum getFileChecksum(org.apache.hadoop.fs.Path,long)>
		 -> $z0 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.etag.checksum.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.store.EtagChecksum getFileChecksum(org.apache.hadoop.fs.Path,long)>
		 -> if $z0 == 0 goto return null
The sink if i0 < 0 goto $z1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.proxy.port", -1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.proxy.port", -1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> if i0 < 0 goto $z1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1)
The sink if r9 == null goto i1 = i1 + 1 in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> if r9 == null goto i1 = i1 + 1
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z2, "Unsupported output format %s", r11) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r9 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.output.format", "csv") in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.output.format", "csv")
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r11 = virtualinvoke $r9.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r10)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z2, "Unsupported output format %s", r11)
The sink if $r2 == null goto $z0 = 0 in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.output-path") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.output-path")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> if $r2 == null goto $z0 = 0
The sink if r7 != null goto $i1 = virtualinvoke r7.<org.apache.hadoop.mapred.gridmix.Statistics$JobStats: int getNoOfMaps()>() in method <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r5 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i0)
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r6 = interfaceinvoke $r3.<java.util.Map: java.lang.Object remove(java.lang.Object)>($r5)
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> r7 = (org.apache.hadoop.mapred.gridmix.Statistics$JobStats) $r6
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> if r7 != null goto $i1 = virtualinvoke r7.<org.apache.hadoop.mapred.gridmix.Statistics$JobStats: int getNoOfMaps()>()
The sink if r0 == null goto return in method <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)> was called with values from the following sources:
- $r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue") in method <org.apache.hadoop.mapred.gridmix.GridmixJob$3: org.apache.hadoop.mapreduce.Job run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$3: org.apache.hadoop.mapreduce.Job run()>
		 -> $r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue")
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$3: org.apache.hadoop.mapreduce.Job run()>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r3, $r7)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r0, r1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> if r0 == null goto return
- $r22 = virtualinvoke $r21.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue") in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> $r22 = virtualinvoke $r21.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue")
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r14, $r22)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r0, r1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> if r0 == null goto return
The sink r10 = virtualinvoke r11.<java.lang.Class: java.lang.String getName()>() in method <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- r11 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("distcp.copy.listing.class", class "Lorg/apache/hadoop/tools/GlobbedCopyListing;", class "Lorg/apache/hadoop/tools/CopyListing;") in method <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("distcp.copy.listing.class", class "Lorg/apache/hadoop/tools/GlobbedCopyListing;", class "Lorg/apache/hadoop/tools/CopyListing;")
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
		 -> r10 = virtualinvoke r11.<java.lang.Class: java.lang.String getName()>()
The sink if $z0 == 0 goto $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.failinject.inconsistency.key.substring", "DELAY_LISTING_ME") in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.failinject.inconsistency.key.substring", "DELAY_LISTING_ME")
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: java.lang.String delayKeySubstring> = $r2
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: java.lang.String delayKeySubstring>
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke $r3.<java.lang.String: boolean equals(java.lang.Object)>("*")
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F)
The sink if r2 == null goto $r5 = specialinvoke r3.<org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>(r4, r0) in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> if r2 == null goto $r5 = specialinvoke r3.<org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>(r4, r0)
The sink if i4 >= 0 goto $r2 = r1.<org.apache.hadoop.mapred.gridmix.JobCreator$2: java.lang.String[] hosts> in method <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)> was called with values from the following sources:
- i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.sleep.fake-locations", 0) in method <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
		 -> i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.sleep.fake-locations", 0)
	 -> <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
		 -> if i4 >= 0 goto $r2 = r1.<org.apache.hadoop.mapred.gridmix.JobCreator$2: java.lang.String[] hosts>
The sink $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\"") in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r18)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\"")
The sink $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4) in method <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r23, r11, r10, 0)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r14 = virtualinvoke r0.<java.io.File: java.lang.String getName()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r15 = $r14
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r15)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/")
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> virtualinvoke r4.<org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>(r5, $r9, r2)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4)
The sink $r3 = staticinvoke <org.apache.hadoop.util.BlockingThreadPoolExecutorService: org.apache.hadoop.util.BlockingThreadPoolExecutorService newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)>(i4, $i2, l1, $r2, "s3a-transfer-shared") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.threads.max", 10) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
		 -> i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.threads.max", 10)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
		 -> $i2 = i4 + i0
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.util.BlockingThreadPoolExecutorService: org.apache.hadoop.util.BlockingThreadPoolExecutorService newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)>(i4, $i2, l1, $r2, "s3a-transfer-shared")
The sink $z0 = virtualinvoke r2.<java.lang.String: boolean equals(java.lang.Object)>("class") in method <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r11 = virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>(r1)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> r1 = virtualinvoke r0.<java.lang.String: java.lang.String substring(int)>($i3)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> r2 = virtualinvoke r1.<java.lang.String: java.lang.String substring(int)>($i5)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> return r2
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> $z0 = virtualinvoke r2.<java.lang.String: boolean equals(java.lang.Object)>("class")
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z1, "Unsupported input format %s", r7) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.input.format", "csv") in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.input.format", "csv")
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke $r5.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r6)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z1, "Unsupported input format %s", r7)
The sink $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/fs/s3a/s3guard/S3Guard$DisabledWarnLevel;", r0) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel valueOf(java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel valueOf(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/fs/s3a/s3guard/S3Guard$DisabledWarnLevel;", r0)
The sink $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r4)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r20 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r19) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r15 = virtualinvoke $r14.<java.lang.Class: java.lang.String getName()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r16 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r15)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a resource usage plugin as it does not extend ")
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r20 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r19)
The sink if r2 != $r29 goto $r31 = r9.<org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.Statistics statistics> in method <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r5 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>($r4)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r2 = (org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy) $r1
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> return $r5
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getJobSubmissionPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> if r2 != $r29 goto $r31 = r9.<org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.Statistics statistics>
The sink if $z3 == 0 goto $i6 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.paging.maximum", 1000) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r40)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if $z3 == 0 goto $i6 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.paging.maximum", 1000)
The sink $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r38) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r38)
The sink virtualinvoke $r19.<java.util.concurrent.ScheduledThreadPoolExecutor: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r21, l5, l5, $r22) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("mapreduce.task.timeout", 120000L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("mapreduce.task.timeout", 120000L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l5 = $l3 / 2L
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r19.<java.util.concurrent.ScheduledThreadPoolExecutor: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r21, l5, l5, $r22)
The sink $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r4) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r4)
- $l1 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r2) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r4)
The sink r4 = virtualinvoke r20.<java.lang.Class: java.lang.reflect.Constructor getDeclaredConstructor(java.lang.Class[])>($r3) in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r19)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>(r1)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>
		 -> r27 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String,boolean,java.lang.ClassLoader)>(r5, 1, $r8)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>
		 -> return r27
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>
		 -> return r2
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r20 = virtualinvoke $r2.<java.lang.Class: java.lang.Class asSubclass(java.lang.Class)>(class "Lorg/apache/hadoop/tools/CopyFilter;")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r20.<java.lang.Class: java.lang.reflect.Constructor getDeclaredConstructor(java.lang.Class[])>($r3)
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setConnectionTimeout(int)>($i2) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $i2 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.connection.establish.timeout", 50000) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $i2 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.connection.establish.timeout", 50000)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setConnectionTimeout(int)>($i2)
The sink $z0 = virtualinvoke $r5.<java.lang.String: boolean equals(java.lang.Object)>(r2) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromString(java.lang.String)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.mode", "server") in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.mode", "server")
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke $r2.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r3)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromString(java.lang.String)>(r4)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromString(java.lang.String)>
		 -> $z0 = virtualinvoke $r5.<java.lang.String: boolean equals(java.lang.Object)>(r2)
The sink if i7 >= i1 goto $i8 = i4 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.min.records_per_chunk", 5) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.min.records_per_chunk", 5)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> if i7 >= i1 goto $i8 = i4
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = (float) i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> i4 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $i6 = i3 * i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f3 = (float) $i6
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f5 = $f4 / $f3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d2 = (double) $f5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d3 = staticinvoke <java.lang.Math: double ceil(double)>($d2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> i7 = (int) $d3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> if i7 >= i1 goto $i8 = i4
The sink $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.DistCpSync: org.apache.hadoop.tools.CopyFilter copyFilter> = $r3
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> r7 = $r4
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> $z1 = virtualinvoke r7.<org.apache.hadoop.tools.DistCpSync: boolean sync()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> $z0 = specialinvoke r0.<org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>
		 -> throw $r50
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> return 0
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>(r2, r7)
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> specialinvoke $r2.<org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>($r4, $r5, r6)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.DistCpSync distCpSync> = r3
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r44 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i16) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.socket.timeout", 60000) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.socket.timeout", 60000)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int socketTimeout> = $i2
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i16 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int socketTimeout>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r44 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i16)
The sink $r11 = virtualinvoke r3.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[2] = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[1] = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r33 = staticinvoke <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>(r52, $r32)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r5 = virtualinvoke r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r11 = virtualinvoke r3.<java.lang.StringBuilder: java.lang.String toString()>()
The sink interfaceinvoke $r11.<org.apache.commons.logging.Log: void debug(java.lang.Object)>($r15) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" from configuration (0 if not present).")
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke $r11.<org.apache.commons.logging.Log: void debug(java.lang.Object)>($r15)
The sink if $z0 == 0 goto return in method <org.apache.hadoop.tools.rumen.state.StatePool: void reload()> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.state.StatePool: void reload()>()
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void reload()>
		 -> $z0 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void reload()>
		 -> if $z0 == 0 goto return
The sink $r6 = staticinvoke <org.apache.commons.lang3.RandomStringUtils: java.lang.String random(int,int,int,boolean,boolean,char[],java.util.Random)>(i2, 0, 0, 1, 0, null, $r4) in method <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,java.lang.Long,int)> was called with values from the following sources:
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.datagenerator.randomtext.wordsize", 10) in method <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: int getRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: int getRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.datagenerator.randomtext.wordsize", 10)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: int getRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$RandomTextDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r3.<org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,int)>(i0, i1)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,int)>
		 -> specialinvoke r0.<org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,java.lang.Long,int)>(i0, $r1, i1)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,java.lang.Long,int)>
		 -> $r6 = staticinvoke <org.apache.commons.lang3.RandomStringUtils: java.lang.String random(int,int,int,boolean,boolean,char[],java.util.Random)>(i2, 0, 0, 1, 0, null, $r4)
The sink $d3 = staticinvoke <java.lang.Math: double ceil(double)>($d2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i5 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f3 = (float) i5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f5 = $f4 / $f3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d2 = (double) $f5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d3 = staticinvoke <java.lang.Math: double ceil(double)>($d2)
The sink specialinvoke $r20.<java.io.OutputStreamWriter: void <init>(java.io.OutputStream,java.lang.String)>($r21, "UTF-8") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r23 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/jobruntime.csv")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r21.<java.io.FileOutputStream: void <init>(java.lang.String)>($r26)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r20.<java.io.OutputStreamWriter: void <init>(java.io.OutputStream,java.lang.String)>($r21, "UTF-8")
The sink if z0 == 0 goto $r26 = <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- z0 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.oss.connection.secure.enabled", 1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> z0 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.oss.connection.secure.enabled", 1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if z0 == 0 goto $r26 = <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: org.slf4j.Logger LOG>
The sink $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r94) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r94 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r94 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r94)
The sink if $z1 == 0 goto $l16 = staticinvoke <java.lang.System: long currentTimeMillis()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $z0 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("createfile.should-delete", 0) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $z0 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("createfile.should-delete", 0)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean shouldDelete> = $z0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $z1 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: boolean shouldDelete>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $z1 == 0 goto $l16 = staticinvoke <java.lang.System: long currentTimeMillis()>()
The sink $z0 = virtualinvoke $r4.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("distcp.exclude-file-regex", "") in method <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("distcp.exclude-file-regex", "")
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.RegexpInConfigurationFilter: java.lang.String excludeFileRegex> = $r3
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = r0.<org.apache.hadoop.tools.RegexpInConfigurationFilter: java.lang.String excludeFileRegex>
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke $r4.<java.lang.String: boolean isEmpty()>()
The sink if $z1 == 0 goto $r4 = virtualinvoke r1.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,boolean)>(r12, 0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: java.io.OutputStream getPossiblyCompressedOutputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: java.io.OutputStream getPossiblyCompressedOutputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto $r4 = virtualinvoke r1.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,boolean)>(r12, 0)
The sink virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d0 = (double) l21
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d1 = 1.0 * $d0
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> d3 = $d2 / $d1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r47 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47)
The sink $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r3, r1) in method <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.azure.delegation.token.provider.type", null, class "Lorg/apache/hadoop/fs/azurebfs/extensions/CustomDelegationTokenManager;") in method <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.azure.delegation.token.provider.type", null, class "Lorg/apache/hadoop/fs/azurebfs/extensions/CustomDelegationTokenManager;")
	 -> <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r3, r1)
The sink $i5 = interfaceinvoke $r47.<java.util.List: int size()>() in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r47 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $i5 = interfaceinvoke $r47.<java.util.List: int size()>()
The sink $r7 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.consistency.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.consistency.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> $r7 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r44) in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()> was called with values from the following sources:
- r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id") in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r44)
The sink if r2 == null goto return in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> if r2 == null goto return
The sink $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
The sink if z15 == 0 goto $r39 = <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute REPLICATION> in method <org.apache.hadoop.tools.util.DistCpUtils: void preserve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.CopyListingFileStatus,java.util.EnumSet,boolean)> was called with values from the following sources:
- z2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> z2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: void preserve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.CopyListingFileStatus,java.util.EnumSet,boolean)>(r39, r38, r22, r8, z2)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: void preserve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.CopyListingFileStatus,java.util.EnumSet,boolean)>
		 -> if z15 == 0 goto $r39 = <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute REPLICATION>
- z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0) in method <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: void preserve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.CopyListingFileStatus,java.util.EnumSet,boolean)>($r106, r104, r99, r19, z1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: void preserve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.CopyListingFileStatus,java.util.EnumSet,boolean)>
		 -> if z15 == 0 goto $r39 = <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute REPLICATION>
The sink $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F) in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F)
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: float rateFactor> = $f0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>(r1, "SerialJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: org.apache.hadoop.mapred.gridmix.SerialJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory: java.lang.Thread createReaderThread()>
		 -> return $r0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread> = $r9
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>(r1, "StressJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: org.apache.hadoop.mapred.gridmix.StressJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory: java.lang.Thread createReaderThread()>
		 -> return $r0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread> = $r9
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>(r1, "ReplayJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: org.apache.hadoop.mapred.gridmix.ReplayJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory: java.lang.Thread createReaderThread()>
		 -> return $r0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread> = $r9
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r17 = r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread>
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r18 = virtualinvoke $r17.<java.lang.Thread: java.lang.String getName()>()
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r19 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r18)
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if r34 != null goto $z4 = 0 in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r34 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.username") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r34 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.username")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if r34 != null goto $z4 = 0
The sink if z5 != 0 goto z6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.feature.ownerandgroup.enableupn", 0) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.enable.client.latency.tracker", 1) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.enable.client.latency.tracker", 1)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if z5 != 0 goto z6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.feature.ownerandgroup.enableupn", 0)
The sink $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49) in method <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r32 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r27 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
The sink if $z0 == 0 goto i1 = i1 + 1 in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve") in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve")
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r2 = r1
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r3 = r2[i1]
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke $r5.<java.lang.String: boolean startsWith(java.lang.String)>(r3)
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto i1 = i1 + 1
The sink if $z0 == 0 goto $r3 = null in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> r61 = virtualinvoke $r9.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>()
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>(r1)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>(r1, $r2)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r5 = staticinvoke <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.fs.permission.AclStatus access$100(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>(r1, $r4, r3)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.fs.permission.AclStatus access$100(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
		 -> $r3 = specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>(r1, r2)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
		 -> $z0 = r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs>
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
		 -> if $z0 == 0 goto $r3 = null
The sink $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r4) in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r4)
The sink $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r103) in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()> was called with values from the following sources:
- r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "") in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r103)
The sink if $z1 != 0 goto $r3 = <org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void close()> was called with values from the following sources:
- $z1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.skip.metrics", 0) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void close()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void close()>
		 -> $z1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.skip.metrics", 0)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void close()>
		 -> if $z1 != 0 goto $r3 = <org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.slf4j.Logger LOG>
The sink specialinvoke r0.<java.io.IOException: void <init>(java.lang.String)>(r1) in method <org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)> was called with values from the following sources:
- $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB> = $i7
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i20 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r60 = virtualinvoke $r59.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i20)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r61 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r106.<org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: void <init>(java.lang.String)>($r61)
	 -> <org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>
		 -> specialinvoke r0.<java.io.IOException: void <init>(java.lang.String)>(r1)
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r4)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r6.<org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>
		 -> specialinvoke r0.<java.io.IOException: void <init>(java.lang.String)>(r1)
- $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB> = $i9
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i19 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i19)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r108.<org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: void <init>(java.lang.String)>($r56)
	 -> <org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>
		 -> specialinvoke r0.<java.io.IOException: void <init>(java.lang.String)>(r1)
- $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB> = $i5
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i21 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke $r64.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i21)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r66 = virtualinvoke $r65.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r104.<org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: void <init>(java.lang.String)>($r66)
	 -> <org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.swift.exceptions.SwiftException: void <init>(java.lang.String)>
		 -> specialinvoke r0.<java.io.IOException: void <init>(java.lang.String)>(r1)
The sink $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads> = $i1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>($r0)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> $r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper cap0> = $r1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: java.util.function.Function relativeToAbsoluteTimestamp> = $r11
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>($r0)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: void <init>(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i2 = r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" threads")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/realtimetrack.json") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r6 = staticinvoke <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r1)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r7 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/realtimetrack.json")
The sink if z5 == 0 goto $z6 = 0 in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z5 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.secure.mode", 0) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z5 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.secure.mode", 0)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if z5 == 0 goto $z6 = 0
The sink specialinvoke $r73.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()> was called with values from the following sources:
- i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> specialinvoke $r73.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3)
The sink if i15 >= $i8 goto $i9 = virtualinvoke r0.<java.lang.String: int length()>() in method <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r5 = r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys>
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r6 = interfaceinvoke $r5.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r7 = interfaceinvoke $r6.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r8 = interfaceinvoke r7.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r10 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r11 = (java.lang.String) $r10
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> virtualinvoke r1.<org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator: java.lang.String validate(java.lang.String)>($r11)
	 -> <org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator: java.lang.String validate(java.lang.String)>
		 -> $z0 = staticinvoke <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>
		 -> $i7 = virtualinvoke r0.<java.lang.String: int length()>()
	 -> <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>
		 -> $i8 = $i7 - 2
	 -> <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>
		 -> if i15 >= $i8 goto $i9 = virtualinvoke r0.<java.lang.String: int length()>()
The sink $r20 = virtualinvoke $r19.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table createTable(com.amazonaws.services.dynamodbv2.model.CreateTableRequest)>(r11) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r15 = (java.lang.String) $r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>(r16)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> r11 = virtualinvoke $r9.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withTags(java.util.Collection)>($r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r20 = virtualinvoke $r19.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table createTable(com.amazonaws.services.dynamodbv2.model.CreateTableRequest)>(r11)
The sink $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z0 == 0 goto $r7 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.Tag newVersionMarkerTag()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $z0 = interfaceinvoke r6.<java.util.Iterator: boolean hasNext()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> if $z0 == 0 goto $r7 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.Tag newVersionMarkerTag()>()
The sink $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads> = $i1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2)
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: long startTimestampMs> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2)
- $d0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: double getDouble(java.lang.String,double)>("auditreplay.rate-factor", 1.0) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $d0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: double getDouble(java.lang.String,double)>("auditreplay.rate-factor", 1.0)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: double rateFactor> = $d0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r11 = staticinvoke <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$lambda_setup_0__225: java.util.function.Function bootstrap$(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper)>(r2)
The sink r4 = staticinvoke <org.apache.hadoop.yarn.util.ResourceCalculatorPlugin: org.apache.hadoop.yarn.util.ResourceCalculatorPlugin getResourceCalculatorPlugin(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r3, r2) in method <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)> was called with values from the following sources:
- r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("mapreduce.tasktracker.resourcecalculatorplugin", null, class "Lorg/apache/hadoop/yarn/util/ResourceCalculatorPlugin;") in method <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("mapreduce.tasktracker.resourcecalculatorplugin", null, class "Lorg/apache/hadoop/yarn/util/ResourceCalculatorPlugin;")
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> r4 = staticinvoke <org.apache.hadoop.yarn.util.ResourceCalculatorPlugin: org.apache.hadoop.yarn.util.ResourceCalculatorPlugin getResourceCalculatorPlugin(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r3, r2)
The sink $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(".") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(".")
The sink if r1 != null goto r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> if r1 != null goto r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
The sink $r14 = virtualinvoke r6.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> $r14 = virtualinvoke r6.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
The sink virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.job.queuename", r0) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)> was called with values from the following sources:
- $r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue") in method <org.apache.hadoop.mapred.gridmix.GridmixJob$3: org.apache.hadoop.mapreduce.Job run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$3: org.apache.hadoop.mapreduce.Job run()>
		 -> $r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue")
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$3: org.apache.hadoop.mapreduce.Job run()>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r3, $r7)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r0, r1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.job.queuename", r0)
- $r22 = virtualinvoke $r21.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue") in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> $r22 = virtualinvoke $r21.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job-submission.default-queue")
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r14, $r22)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void access$100(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>(r0, r1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: void setJobQueue(org.apache.hadoop.mapreduce.Job,java.lang.String)>
		 -> virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.job.queuename", r0)
The sink specialinvoke $r1.<org.apache.hadoop.io.BytesWritable: void <init>(byte[])>($r4) in method <org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gendata.val.bytes", 1048576) in method <org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gendata.val.bytes", 1048576)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r4 = newarray (byte)[$i0]
	 -> <org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r1.<org.apache.hadoop.io.BytesWritable: void <init>(byte[])>($r4)
The sink $r40 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i23) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $i8 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.concurrentRequestCount.out", $i7) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i8 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.concurrentRequestCount.out", $i7)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int concurrentWrites> = $i8
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i23 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int concurrentWrites>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r40 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i23)
The sink if $z0 == 0 goto $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- $z0 = virtualinvoke $r15.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.is.minicluster", 0) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $z0 = virtualinvoke $r15.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.is.minicluster", 0)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> if $z0 == 0 goto $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if i0 > 0 goto return i0 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.split.ratio", 2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.split.ratio", 2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(org.apache.hadoop.conf.Configuration)>
		 -> if i0 > 0 goto return i0
The sink r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke r0.<org.apache.hadoop.mapreduce.RecordReader: void <init>()>() in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1: void <init>(org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat,long,long)> was called with values from the following sources:
- l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l2 = virtualinvoke $r4.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l1, $r3)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> specialinvoke $r5.<org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1: void <init>(org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat,long,long)>(r6, l0, l2)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1: void <init>(org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat,long,long)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1: long val$RINTERVAL> = l1
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1: void <init>(org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat,long,long)>
		 -> specialinvoke r0.<org.apache.hadoop.mapreduce.RecordReader: void <init>()>()
The sink specialinvoke r0.<java.lang.Object: void <init>()>() in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)> was called with values from the following sources:
- i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r6.<org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>($r8, i0)
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> r0.<org.apache.hadoop.yarn.sls.web.SLSWebApp: int port> = i0
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: org.apache.hadoop.yarn.sls.web.SLSWebApp web> = $r6
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
The sink virtualinvoke r71.<com.microsoft.azure.datalake.store.ADLStoreOptions: com.microsoft.azure.datalake.store.ADLStoreOptions setDefaultTimeout(int)>(i0) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.http.timeout", -1) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.http.timeout", -1)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r71.<com.microsoft.azure.datalake.store.ADLStoreOptions: com.microsoft.azure.datalake.store.ADLStoreOptions setDefaultTimeout(int)>(i0)
The sink $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = (float) i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
The sink $z3 = virtualinvoke r10.<java.io.File: boolean isDirectory()>() in method <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $z0 = virtualinvoke r2.<org.apache.hadoop.streaming.StreamJob: boolean isLocalHadoop()>()
	 -> <org.apache.hadoop.streaming.StreamJob: boolean isLocalHadoop()>
		 -> return $z0
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $z3 = virtualinvoke r10.<java.io.File: boolean isDirectory()>()
The sink $z0 = virtualinvoke r3.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])> was called with values from the following sources:
- $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid> = $r8
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void setWorkPath(org.apache.hadoop.fs.Path)>($r10)
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: void setWorkPath(org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>($r25, r6)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void initFileOutputCommitterOptions(org.apache.hadoop.mapreduce.JobContext)>(r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void initFileOutputCommitterOptions(org.apache.hadoop.mapreduce.JobContext)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid>
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.conf.Configuration,java.lang.String)>(r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String)>($r1, r0, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r3[1] = r6
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r7 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>($r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>
		 -> r2 = r1
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>
		 -> r3 = r2[i1]
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>
		 -> $z0 = virtualinvoke r3.<java.lang.String: boolean isEmpty()>()
The sink $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB> = $i9
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i19 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i19)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.String toString()>()
The sink virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("<CPS>") in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("<CPS>")
The sink specialinvoke $r12.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>($r17) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a valid value for ")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.ssl.channel.mode")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> specialinvoke $r12.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>($r17)
The sink if r4 == null goto $r5 = new org.apache.hadoop.mapred.JobConf in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.map.output.compress.codec") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.map.output.compress.codec")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> if r4 == null goto $r5 = new org.apache.hadoop.mapred.JobConf
The sink specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1) in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1)
The sink $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13)
The sink $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l9 = (long) $i8
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords> = $l11
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l13 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> l3 = $l12 / $l13
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f0 = (float) l3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f3 = $f0 * $f2
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i21 = (int) $f3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21)
- $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f2 = staticinvoke <java.lang.Math: float min(float,float)>(1.0F, $f1)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f3 = $f0 * $f2
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i21 = (int) $f3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f3 = $f2 / f4
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l17 = (long) $f3
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r40.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l17, $l8, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l12 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> l3 = $l12 / $l13
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f0 = (float) l3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f3 = $f0 * $f2
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i21 = (int) $f3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $f1 = $f0 / f2
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l6 = (long) $f1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> specialinvoke $r32.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l6, l7, $r24, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l12 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> l3 = $l12 / $l13
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f0 = (float) l3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f3 = $f0 * $f2
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i21 = (int) $f3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21)
The sink if r70 == null goto $r71 = new java.io.IOException in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal") in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal")
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> if r70 == null goto $r71 = new java.io.IOException
The sink $r7 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Waiting for %d DataNodes to register with the NameNode...", $r5) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f2 = $f0 * $f1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> i1 = (int) $f2
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r6 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i1)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r5[0] = $r6
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r7 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Waiting for %d DataNodes to register with the NameNode...", $r5)
The sink if $z2 == 0 goto $r8 = new org.apache.hadoop.fs.azure.WasbRemoteCallHelper in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.enable.kerberos.support", 0) in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.enable.kerberos.support", 0)
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: boolean isKerberosSupportEnabled> = $z0
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: boolean isKerberosSupportEnabled>
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $z2 == 0 goto $r8 = new org.apache.hadoop.fs.azure.WasbRemoteCallHelper
The sink $r38 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r38 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy connectivityFailure> = $r13
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r28.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r29, $r31)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Invoker s3guardInvoker> = $r28
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean enableMultiObjectsDelete> = $z2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean failOnMetadataWriteError> = $z1
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean errorsIncludeSql> = $z1
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r61.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>(l13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = $r61
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r3)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory blockFactory> = $r54
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: boolean magicCommitEnabled> = z0
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean enabled> = $z0
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>(r1, r2, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r3, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r1, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: boolean requireVersion> = z0
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy changeDetectionPolicy> = $r45
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r3 = (org.apache.hadoop.fs.s3a.WriteOperationHelper) $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.WriteOperationHelper operations> = $r3
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r3 = staticinvoke <com.amazonaws.services.s3.model.CannedAccessControlList: com.amazonaws.services.s3.model.CannedAccessControlList valueOf(java.lang.String)>(r1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.acl.default", "") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.acl.default", "")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <com.amazonaws.services.s3.model.CannedAccessControlList: com.amazonaws.services.s3.model.CannedAccessControlList valueOf(java.lang.String)>(r1)
The sink if z1 != 0 goto r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0 in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.namenode.acls.enabled", 1) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.namenode.acls.enabled", 1)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if z1 != 0 goto r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.output.fileoutputformat.compress.codec", r2) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.codec") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.codec")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.output.fileoutputformat.compress.codec", r2)
The sink if $z1 != 0 goto r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4) in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $z1 = virtualinvoke r4.<java.io.File: boolean exists()>()
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> if $z1 != 0 goto r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4)
The sink $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(": ") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" defined in ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(": ")
The sink $r5 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[2] = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[1] = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r5 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
The sink specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3) in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
The sink virtualinvoke r71.<com.microsoft.azure.datalake.store.ADLStoreOptions: void setSSLChannelMode(java.lang.String)>(r42) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r42 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.ssl.channel.mode", "Default") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r42 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.ssl.channel.mode", "Default")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r71.<com.microsoft.azure.datalake.store.ADLStoreOptions: void setSSLChannelMode(java.lang.String)>(r42)
The sink virtualinvoke r21.<com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder: com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder withRoleSessionDurationSeconds(int)>($i2) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.assumed.role.session.duration", "1h", $r15) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.assumed.role.session.duration", "1h", $r15)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: long duration> = $l0
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $l1 = r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: long duration>
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i2 = (int) $l1
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r21.<com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder: com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder withRoleSessionDurationSeconds(int)>($i2)
The sink virtualinvoke r0.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r15, l4) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()> was called with values from the following sources:
- $i1 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.vcores", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> $i1 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.vcores", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> l4 = (long) $i1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> virtualinvoke r0.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r15, l4)
- $i2 = virtualinvoke $r14.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.memory.mb", 10240) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> $i2 = virtualinvoke $r14.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.nm.memory.mb", 10240)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> l4 = (long) $i2
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> virtualinvoke r0.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r15, l4)
The sink if r22 != null goto $r5 = interfaceinvoke r22.<java.util.List: java.util.stream.Stream stream()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> throw $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r30 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>($r3, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r20 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.model.TableDescription describe()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r2 = virtualinvoke r20.<com.amazonaws.services.dynamodbv2.model.TableDescription: java.lang.String getTableArn()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r21 = virtualinvoke $r1.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest withResourceArn(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r4 = interfaceinvoke r3.<com.amazonaws.services.dynamodbv2.AmazonDynamoDB: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult listTagsOfResource(com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest)>(r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r22 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult: java.util.List getTags()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> if r22 != null goto $r5 = interfaceinvoke r22.<java.util.List: java.util.stream.Stream stream()>()
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r33 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> throw $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r30 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>($r3, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r20 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.model.TableDescription describe()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r2 = virtualinvoke r20.<com.amazonaws.services.dynamodbv2.model.TableDescription: java.lang.String getTableArn()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r21 = virtualinvoke $r1.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest withResourceArn(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r4 = interfaceinvoke r3.<com.amazonaws.services.dynamodbv2.AmazonDynamoDB: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult listTagsOfResource(com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest)>(r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r22 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult: java.util.List getTags()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> if r22 != null goto $r5 = interfaceinvoke r22.<java.util.List: java.util.stream.Stream stream()>()
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setSocketTimeout(int)>($i3) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $i3 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.connection.timeout", 200000) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $i3 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.connection.timeout", 200000)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setSocketTimeout(int)>($i3)
The sink $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r10 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r64.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> r11 = $r64
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $r20 = virtualinvoke r11.<org.apache.hadoop.fs.Path: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.Path: java.lang.String toString()>
		 -> $r24 = r2.<org.apache.hadoop.fs.Path: java.net.URI uri>
	 -> <org.apache.hadoop.fs.Path: java.lang.String toString()>
		 -> $r25 = virtualinvoke $r24.<java.net.URI: java.lang.String getScheme()>()
	 -> <org.apache.hadoop.fs.Path: java.lang.String toString()>
		 -> $r26 = virtualinvoke r1.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r25)
	 -> <org.apache.hadoop.fs.Path: java.lang.String toString()>
		 -> $r9 = virtualinvoke r1.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.Path: java.lang.String toString()>
		 -> return $r9
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $r21 = virtualinvoke $r68.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r20)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $r22 = virtualinvoke $r21.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r15)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if r3 != null goto $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r3, r1) in method <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.azure.delegation.token.provider.type", null, class "Lorg/apache/hadoop/fs/azurebfs/extensions/CustomDelegationTokenManager;") in method <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.azure.delegation.token.provider.type", null, class "Lorg/apache/hadoop/fs/azurebfs/extensions/CustomDelegationTokenManager;")
	 -> <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> if r3 != null goto $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r3, r1)
The sink $z0 = virtualinvoke $r5.<java.lang.String: boolean equals(java.lang.Object)>(r2) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromString(java.lang.String)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.source", "etag") in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.source", "etag")
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke $r2.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r3)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromString(java.lang.String)>(r4)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromString(java.lang.String)>
		 -> $z0 = virtualinvoke $r5.<java.lang.String: boolean equals(java.lang.Object)>(r2)
The sink $z0 = virtualinvoke r2.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r2.<java.lang.String: boolean isEmpty()>()
The sink $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r31) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror") in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror")
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r31)
The sink $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i4) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)> was called with values from the following sources:
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i4 = r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i4)
The sink virtualinvoke r1.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r18, l1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getResourceForContainer(java.util.Map)> was called with values from the following sources:
- i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getResourceForContainer(java.util.Map)>
		 -> virtualinvoke r1.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r18, l1)
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getResourceForContainer(java.util.Map)>
		 -> virtualinvoke r1.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r18, l1)
The sink if $r11 == null goto $r15 = "hadoop.tmp.dir" in method <org.apache.hadoop.fs.s3a.S3AFileSystem: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r11 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.buffer.dir") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.buffer.dir")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> if $r11 == null goto $r15 = "hadoop.tmp.dir"
The sink $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("file") in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("file")
The sink virtualinvoke r1.<com.microsoft.azure.datalake.store.ADLFileOutputStream: void setBufferSize(int)>($i0) in method <org.apache.hadoop.fs.adl.AdlFsOutputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileOutputStream,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.feature.client.cache.drop.behind.writes", 4194304) in method <org.apache.hadoop.fs.adl.AdlFsOutputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileOutputStream,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFsOutputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileOutputStream,org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.feature.client.cache.drop.behind.writes", 4194304)
	 -> <org.apache.hadoop.fs.adl.AdlFsOutputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileOutputStream,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r1.<com.microsoft.azure.datalake.store.ADLFileOutputStream: void setBufferSize(int)>($i0)
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>(r1, $r3) in method <org.apache.hadoop.tools.util.DistCpUtils: void publish(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Object)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i5 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r3 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i5)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: void publish(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Object)>($r2, "mapred.num.entries.per.chunk", $r3)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: void publish(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Object)>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String valueOf(java.lang.Object)>(r2)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: void publish(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Object)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>(r1, $r3)
The sink if z3 == 0 goto return r2 in method <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)> was called with values from the following sources:
- z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0) in method <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r99 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)>(r29, $r31, $z2, z10, z1, $l0, $l1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)>
		 -> if z3 == 0 goto return r2
The sink $i1 = virtualinvoke r98.<java.lang.String: int hashCode()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $i1 = virtualinvoke r98.<java.lang.String: int hashCode()>()
The sink $r21 = interfaceinvoke $r37.<java.util.List: java.lang.Object get(int)>($i11) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i5 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f3 = (float) i5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f5 = $f4 / $f3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d2 = (double) $f5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d3 = staticinvoke <java.lang.Math: double ceil(double)>($d2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i6 = (int) $d3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i13 = staticinvoke <java.lang.Math: int min(int,int)>($i7, i6)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i11 = i14 % i13
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r21 = interfaceinvoke $r37.<java.util.List: java.lang.Object get(int)>($i11)
The sink staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r3, $r2) in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>(r14)
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> throw r12
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r15, r0)
	 -> <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>(r0)
	 -> <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>
		 -> interfaceinvoke r0.<org.apache.hadoop.service.Service: void stop()>()
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>($r4)
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> throw $r8
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStop()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> $r7 = r1.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r8, $r7)
	 -> <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>(r0)
	 -> <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>
		 -> interfaceinvoke r0.<org.apache.hadoop.service.Service: void stop()>()
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>(r26)
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> $r6 = virtualinvoke r2.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>()
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> $r1 = r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.ServiceStateModel stateModel>
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> $r2 = virtualinvoke $r1.<org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE getState()>()
	 -> <org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE getState()>
		 -> $r1 = r0.<org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE state>
	 -> <org.apache.hadoop.service.ServiceStateModel: org.apache.hadoop.service.Service$STATE getState()>
		 -> return $r1
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> return $r2
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> r2.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE failureState> = $r6
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> $r6 = virtualinvoke r2.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>()
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE getServiceState()>
		 -> r0 := @this: org.apache.hadoop.service.AbstractService
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> r2 := @this: org.apache.hadoop.service.AbstractService
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $r21 := @caughtexception
	 -> <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>
		 -> r0 := @parameter0: org.apache.hadoop.service.Service
	 -> <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>
		 -> r3 := @parameter0: org.slf4j.Logger
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> $r7 = r1.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> $r5 := @caughtexception
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>($r4)
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> return r3
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStop()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStop()>()
	 -> <org.apache.hadoop.service.AbstractService: void serviceStop()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> $r2 = r1.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r3, $r2)
The sink virtualinvoke r5.<java.util.ArrayList: boolean add(java.lang.Object)>($r13) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r13 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l22)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r5.<java.util.ArrayList: boolean add(java.lang.Object)>($r13)
The sink interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37) in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
The sink virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r45) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d0 = (double) l21
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d1 = 1.0 * $d0
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> d3 = $d2 / $d1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r47 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r45)
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r15) in method <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r23, r11, r10, 0)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r14 = virtualinvoke r0.<java.io.File: java.lang.String getName()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r15 = $r14
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r15)
The sink $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)> was called with values from the following sources:
- $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean enableMultiObjectsDelete> = $z2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean allowAuthoritativeMetadataStore> = $z5
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r22 := @caughtexception
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r3)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory blockFactory> = $r54
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean failOnMetadataWriteError> = $z1
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean errorsIncludeSql> = $z1
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r61.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>(l13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = $r61
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r3)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory blockFactory> = $r54
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: boolean magicCommitEnabled> = z0
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean enabled> = $z0
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>(r1, r2, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r3, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r1, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: boolean requireVersion> = z0
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy changeDetectionPolicy> = $r45
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r3 = (org.apache.hadoop.fs.s3a.WriteOperationHelper) $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.WriteOperationHelper operations> = $r3
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: org.apache.hadoop.fs.s3a.S3AFileSystem cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$lambda_getAuthoritativePaths_6__151: java.util.function.Function bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r12 = interfaceinvoke r11.<java.util.function.Function: java.lang.Object apply(java.lang.Object)>(r7)
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0) in method <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)> was called with values from the following sources:
- f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F) in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F)
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = f0 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>(r2, $f2)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0)
The sink $i2 = virtualinvoke r1.<java.util.ArrayList: int size()>() in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $i2 = virtualinvoke r1.<java.util.ArrayList: int size()>()
The sink $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
The sink $r11 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>($r10) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)> was called with values from the following sources:
- $r10 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r10 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r11 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>($r10)
The sink virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("<CPS>") in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("<CPS>")
The sink specialinvoke $r17.<java.io.FileOutputStream: void <init>(java.lang.String)>($r9) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r6 = staticinvoke <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r1)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r7 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/realtimetrack.json")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> specialinvoke $r17.<java.io.FileOutputStream: void <init>(java.lang.String)>($r9)
The sink $r5 = virtualinvoke $r4.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1) in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.authorization.caching.maxentries", 512) in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.authorization.caching.maxentries", 512)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = (long) $i0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke $r4.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
The sink $r43 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r43 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink $z1 = virtualinvoke $r8.<java.io.File: boolean isDirectory()>() in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> specialinvoke $r8.<java.io.File: void <init>(java.lang.String)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $z1 = virtualinvoke $r8.<java.io.File: boolean isDirectory()>()
The sink r9 = virtualinvoke r6.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,boolean,int,short,long)>(r7, z0, $i2, 1, $l4) in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: void writeDataset(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[],int,int,boolean)> was called with values from the following sources:
- $i2 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("io.file.buffer.size", 4096) in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: void writeDataset(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[],int,int,boolean)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: void writeDataset(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[],int,int,boolean)>
		 -> $i2 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("io.file.buffer.size", 4096)
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: void writeDataset(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[],int,int,boolean)>
		 -> r9 = virtualinvoke r6.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,boolean,int,short,long)>(r7, z0, $i2, 1, $l4)
The sink $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4) in method <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
The sink if z3 == 0 goto r5 = staticinvoke <com.google.common.collect.Maps: java.util.HashMap newHashMap()>() in method <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)> was called with values from the following sources:
- z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0) in method <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r99 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)>(r29, $r31, $z2, z10, z1, $l0, $l1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)>
		 -> if z3 == 0 goto r5 = staticinvoke <com.google.common.collect.Maps: java.util.HashMap newHashMap()>()
The sink $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
The sink virtualinvoke $r21.<com.aliyun.oss.OSSClient: void setBucketAcl(java.lang.String,com.aliyun.oss.model.CannedAccessControlList)>($r20, r19) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r19 = staticinvoke <com.aliyun.oss.model.CannedAccessControlList: com.aliyun.oss.model.CannedAccessControlList valueOf(java.lang.String)>(r40)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke $r21.<com.aliyun.oss.OSSClient: void setBucketAcl(java.lang.String,com.aliyun.oss.model.CannedAccessControlList)>($r20, r19)
The sink interfaceinvoke r10.<java.util.List: boolean add(java.lang.Object)>($r46) in method <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)> was called with values from the following sources:
- i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> specialinvoke $r46.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>($r21, l18, r41, 10, "reduce")
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>(r1, l0, r2, i1, r3, $r4)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r1, l0, r2, i1, r3, r4, -1L, 0L)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> interfaceinvoke r10.<java.util.List: boolean add(java.lang.Object)>($r46)
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> specialinvoke $r46.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>($r21, l18, r41, 10, "reduce")
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>(r1, l0, r2, i1, r3, $r4)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r1, l0, r2, i1, r3, r4, -1L, 0L)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> interfaceinvoke r10.<java.util.List: boolean add(java.lang.Object)>($r46)
The sink $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>()
The sink specialinvoke $r29.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r31) in method <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r31 = virtualinvoke $r30.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r31)
The sink $z0 = interfaceinvoke r6.<java.util.Iterator: boolean hasNext()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $z0 = interfaceinvoke r6.<java.util.Iterator: boolean hasNext()>()
The sink $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void start()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l7) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l7 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l7)
The sink $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" defined in ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.metadatastore.impl")
The sink if r1 == null goto return null in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
		 -> if r1 == null goto return null
The sink $r6 = interfaceinvoke $r4.<com.google.common.cache.Cache: java.lang.Object get(java.lang.Object,java.util.concurrent.Callable)>(r1, $r5) in method <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)> was called with values from the following sources:
- $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid> = $r8
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r9 = r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid>
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r10 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path buildWorkPath(org.apache.hadoop.mapreduce.JobContext,java.lang.String)>($r24, $r9)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path buildWorkPath(org.apache.hadoop.mapreduce.JobContext,java.lang.String)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>($r2, r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>($r1, r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>(r2, r13, r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.lang.String cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r6 = interfaceinvoke $r4.<com.google.common.cache.Cache: java.lang.Object get(java.lang.Object,java.util.concurrent.Callable)>(r1, $r5)
The sink r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4) in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4)
The sink if $z1 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> r1 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("array")
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> if $z1 == 0 goto (branch)
The sink $r7 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6) in method <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r5 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>($r4)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r2 = (org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy) $r1
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> return $r5
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getJobSubmissionPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r6 = virtualinvoke r2.<org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: java.lang.String name()>()
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r7 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
The sink if i5 <= 2 goto (branch) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if i5 <= 2 goto (branch)
The sink if $z2 == 0 goto $z8 = 0 in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.output.fileoutputformat.compress", 0) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.output.fileoutputformat.compress", 0)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $z2 == 0 goto $z8 = 0
The sink $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r72)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/")
The sink $r53 = virtualinvoke $r49.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($r52) in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()> was called with values from the following sources:
- r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id") in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r44)
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r49 = virtualinvoke $r48.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\' is being simulated as \'")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r53 = virtualinvoke $r49.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($r52)
The sink specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r5, r3) in method <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])> was called with values from the following sources:
- $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid> = $r8
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void setWorkPath(org.apache.hadoop.fs.Path)>($r10)
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: void setWorkPath(org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>($r25, r6)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void initFileOutputCommitterOptions(org.apache.hadoop.mapreduce.JobContext)>(r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void initFileOutputCommitterOptions(org.apache.hadoop.mapreduce.JobContext)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid>
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.conf.Configuration,java.lang.String)>(r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String)>($r1, r0, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r3[1] = r6
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getMultipartUploadCommitsDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r7 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>($r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>
		 -> r2 = r1
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>
		 -> r3 = r2[i1]
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path path(org.apache.hadoop.fs.Path,java.lang.String[])>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r5, r3)
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> throw $r2
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
The sink r3 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList(java.lang.Iterable)>(r1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r3 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList(java.lang.Iterable)>(r1)
The sink virtualinvoke r30.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void start()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $z0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("auditreplay.create-blocks", 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $z0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("auditreplay.create-blocks", 1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: boolean createBlocks> = $z0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r30 = $r28
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke r30.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void start()>()
- $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: long startTimestampMs> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r30 = $r28
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke r30.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void start()>()
- $r10 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r10 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r11 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>($r10)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: java.net.URI namenodeUri> = $r11
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r30 = $r28
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke r30.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void start()>()
The sink $r2 = virtualinvoke r0.<java.util.ArrayList: java.lang.Object get(int)>(i3) in method <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r13 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l22)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r5.<java.util.ArrayList: boolean add(java.lang.Object)>($r13)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r34 = specialinvoke r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>(r5)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> $r2 = virtualinvoke r0.<java.util.ArrayList: java.lang.Object get(int)>(i3)
The sink $r48 = virtualinvoke r5.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>(r1, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $r48 = virtualinvoke r5.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $r48 = virtualinvoke r5.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
The sink specialinvoke $r1.<org.apache.hadoop.io.BytesWritable: void <init>(byte[])>($r4) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gendata.val.bytes", 1048576) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gendata.val.bytes", 1048576)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r4 = newarray (byte)[$i0]
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r1.<org.apache.hadoop.io.BytesWritable: void <init>(byte[])>($r4)
The sink if $c2 != $c3 goto i4 = i4 + 1 in method <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $c1 = virtualinvoke r1.<java.lang.String: char charAt(int)>(i2)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $r2 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>($c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> $c3 = staticinvoke <java.lang.Character: char toUpperCase(char)>(c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> if $c2 != $c3 goto i4 = i4 + 1
The sink if i0 != -1 goto $r2 = new java.util.ArrayList in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.num-mappers", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.num-mappers", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> if i0 != -1 goto $r2 = new java.util.ArrayList
The sink $r72 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l4) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- l4 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l3, $r70) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l3, $r70)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r72 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l4)
The sink $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
The sink staticinvoke <com.amazonaws.regions.Regions: com.amazonaws.regions.Regions fromName(java.lang.String)>(r15) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> staticinvoke <com.amazonaws.regions.Regions: com.amazonaws.regions.Regions fromName(java.lang.String)>(r15)
The sink $z0 = interfaceinvoke r1.<java.util.Collection: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = interfaceinvoke r1.<java.util.Collection: boolean isEmpty()>()
The sink l11 = virtualinvoke $r7.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l10, $r6) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- l10 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l10 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l11 = virtualinvoke $r7.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l10, $r6)
The sink $r15 = virtualinvoke $r14.<java.lang.Class: java.lang.String getName()>() in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r15 = virtualinvoke $r14.<java.lang.Class: java.lang.String getName()>()
The sink if r2 == null goto $z0 = 0 in method <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[2] = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[1] = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r29)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r33 = virtualinvoke $r32.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" failed")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r34 = virtualinvoke $r33.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r35 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateDynamoDBException(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException)>(r29, $r34, r62)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateDynamoDBException(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException)>
		 -> specialinvoke $r19.<org.apache.hadoop.fs.s3a.AWSServiceIOException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>(r3, r0)
	 -> <org.apache.hadoop.fs.s3a.AWSServiceIOException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>
		 -> if r2 == null goto $z0 = 0
The sink $r11 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>($l0) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l6 = $l5 + l22
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart> = $l6
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l0 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart>
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r11 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>($l0)
The sink if $b8 >= 0 goto $r5 = <org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter: org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter R_ALL> in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l4 = (long) i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> l6 = $l5 / $l4
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $b8 = $l7 cmp l6
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> if $b8 >= 0 goto $r5 = <org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter: org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter R_ALL>
The sink staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object,java.lang.String,java.lang.Object)>(r3, "No child entry in item %s", r0) in method <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":mod_time", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r13 = virtualinvoke $r12.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_dir", 1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r0, $r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3> = $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r4 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r5 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r2, $r3, $r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.ItemCollection scan(java.lang.String,java.lang.String,java.util.Map,java.util.Map)>(r1, r2, null, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r10 = (com.amazonaws.services.dynamodbv2.document.ItemCollection) $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> return $r10
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r14 = virtualinvoke r13.<com.amazonaws.services.dynamodbv2.document.ItemCollection: com.amazonaws.services.dynamodbv2.document.internal.IteratorSupport iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r19 = interfaceinvoke r14.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r20 = (com.amazonaws.services.dynamodbv2.document.Item) $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r22 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>(r20, $r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> r3 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: java.lang.String getString(java.lang.String)>("child")
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object,java.lang.String,java.lang.Object)>(r3, "No child entry in item %s", r0)
The sink r20 = virtualinvoke $r2.<java.lang.Class: java.lang.Class asSubclass(java.lang.Class)>(class "Lorg/apache/hadoop/tools/CopyFilter;") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r19)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>(r1)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>
		 -> r27 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String,boolean,java.lang.ClassLoader)>(r5, 1, $r8)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>
		 -> return r27
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>
		 -> return r2
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r20 = virtualinvoke $r2.<java.lang.Class: java.lang.Class asSubclass(java.lang.Class)>(class "Lorg/apache/hadoop/tools/CopyFilter;")
The sink specialinvoke r0.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: void <init>()>() in method <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r8 = r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$1: org.apache.hadoop.yarn.sls.SLSRunner val$se> = r2
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: void <init>()>()
The sink if $z12 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z12 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("magic")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z12 == 0 goto (branch)
The sink $z2 = virtualinvoke $r7.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.identity.transformer.service.principal.id", "") in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.identity.transformer.service.principal.id", "")
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: java.lang.String servicePrincipalId> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: java.lang.String servicePrincipalId>
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke $r7.<java.lang.String: boolean isEmpty()>()
The sink if z6 == 0 goto $r74 = <com.microsoft.azure.datalake.store.UserGroupRepresentation: com.microsoft.azure.datalake.store.UserGroupRepresentation OID> in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.feature.ownerandgroup.enableupn", 0) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.feature.ownerandgroup.enableupn", 0)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if z6 == 0 goto $r74 = <com.microsoft.azure.datalake.store.UserGroupRepresentation: com.microsoft.azure.datalake.store.UserGroupRepresentation OID>
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyPort(int)>(i4) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- i4 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.proxy.port", -1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> i4 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.proxy.port", -1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyPort(int)>(i4)
The sink if $z0 == 0 goto $r3 = <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto $r3 = <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: org.slf4j.Logger LOG>
The sink $z0 = virtualinvoke $r3.<java.lang.String: boolean equals(java.lang.Object)>("*") in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.failinject.inconsistency.key.substring", "DELAY_LISTING_ME") in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.failinject.inconsistency.key.substring", "DELAY_LISTING_ME")
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: java.lang.String delayKeySubstring> = $r2
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: java.lang.String delayKeySubstring>
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke $r3.<java.lang.String: boolean equals(java.lang.Object)>("*")
The sink $r33 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r28.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r29, $r31)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Invoker s3guardInvoker> = $r28
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r33 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r28.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r29, $r31)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Invoker s3guardInvoker> = $r28
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r33 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
The sink if $z1 != 0 goto return "lib/" in method <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r11 = virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>(r1)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> r1 = virtualinvoke r0.<java.lang.String: java.lang.String substring(int)>($i3)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> r2 = virtualinvoke r1.<java.lang.String: java.lang.String substring(int)>($i5)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> return r2
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> $z1 = virtualinvoke r2.<java.lang.String: boolean equals(java.lang.Object)>("jar")
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> if $z1 != 0 goto return "lib/"
The sink virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("./log4j.properties") in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r13 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("<CPS>")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("./log4j.properties")
The sink specialinvoke $r3.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5) in method <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path tempDirForStaging(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.tmp.path", r2) in method <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path tempDirForStaging(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path tempDirForStaging(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.tmp.path", r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path tempDirForStaging(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
The sink if $r5 == null goto $r6 = new java.io.IOException in method <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.delegation.token.service.urls") in method <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.delegation.token.service.urls")
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: java.lang.String[] dtServiceUrls> = $r4
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = r0.<org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: java.lang.String[] dtServiceUrls>
	 -> <org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> if $r5 == null goto $r6 = new java.io.IOException
The sink if $b1 <= 0 goto return in method <org.apache.hadoop.tools.util.ThrottledInputStream: void throttle()> was called with values from the following sources:
- f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F) in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F)
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = f0 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>(r2, $f2)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> r0.<org.apache.hadoop.tools.util.ThrottledInputStream: float maxBytesPerSec> = f0
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: long copyBytes(org.apache.hadoop.tools.CopyListingFileStatus,long,java.io.OutputStream,int,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i15 = staticinvoke <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: int readBytes(org.apache.hadoop.tools.util.ThrottledInputStream,byte[],int)>(r13, r2, i18)
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: int readBytes(org.apache.hadoop.tools.util.ThrottledInputStream,byte[],int)>
		 -> $i1 = virtualinvoke r0.<org.apache.hadoop.tools.util.ThrottledInputStream: int read(byte[],int,int)>(r1, 0, i0)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: int read(byte[],int,int)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.util.ThrottledInputStream: void throttle()>()
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void throttle()>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.tools.util.ThrottledInputStream: long getBytesPerSec()>()
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: long getBytesPerSec()>
		 -> return $l7
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void throttle()>
		 -> $f0 = r0.<org.apache.hadoop.tools.util.ThrottledInputStream: float maxBytesPerSec>
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void throttle()>
		 -> $b1 = $f1 cmpl $f0
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void throttle()>
		 -> if $b1 <= 0 goto return
The sink if $i29 != 0 goto $r16 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: java.util.concurrent.ExecutorService readAheadExecutorService> in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)> was called with values from the following sources:
- $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> = $l1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>(0L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l36 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize>
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l26 = l42 + l36
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l43 = $l26 - 1L
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>(l42, l43)
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> $l2 = l0 - l1
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> $i3 = (int) $l2
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> $i4 = $i3 + 1
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> $r4 = newarray (byte)[$i4]
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] buffer> = $r4
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> return
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> r11 = $r10
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $r12 = virtualinvoke r11.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] getBuffer()>()
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] getBuffer()>
		 -> $r1 = r0.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] buffer>
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] getBuffer()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $i29 = lengthof $r12
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> if $i29 != 0 goto $r16 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: java.util.concurrent.ExecutorService readAheadExecutorService>
The sink if $z1 == 0 goto $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> if $z1 == 0 goto $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
- r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> if $z1 == 0 goto $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
The sink if $i0 != -1 goto return r9 in method <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r15 = staticinvoke <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>(r1, r2, null)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> $i0 = virtualinvoke r8.<java.lang.String: int indexOf(int)>(46)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> if $i0 != -1 goto return r9
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>($r1, $r3) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getTempJobAttemptPath(int,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getTempTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getTempJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getTempJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r0[0] = $r1
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r2 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("app-attempt-%04d", $r0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getTempJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>($r1, $r3)
The sink if $b4 != 0 goto $z5 = 0 in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $b4 = l8 cmp 0L
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> if $b4 != 0 goto $z5 = 0
The sink r11 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r15 = staticinvoke <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>(r1, r2, null)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r8)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> r11 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $r1 != null goto $r2 = <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: org.apache.hadoop.fs.LocalDirAllocator directoryAllocator> in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.oss.buffer.dir") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.oss.buffer.dir")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> if $r1 != null goto $r2 = <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: org.apache.hadoop.fs.LocalDirAllocator directoryAllocator>
The sink if null != $r3 goto $i3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("hdfs.image.writer.ugi.single.gid", 1) in method <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hdfs.image.writer.ugi.single.user") in method <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hdfs.image.writer.ugi.single.user")
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String user> = $r2
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String user>
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> if null != $r3 goto $i3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("hdfs.image.writer.ugi.single.gid", 1)
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3) in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.fs.Path getTempFile(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.fs.Path getTempFile(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.fs.Path getTempFile(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.fs.Path getTempFile(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3)
The sink $r2 = virtualinvoke $r1.<java.io.File: java.nio.file.Path toPath()>() in method <org.apache.hadoop.tools.RegexCopyFilter: void initialize()> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.CopyFilter copyFilter> = $r9
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r10 = r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.CopyFilter copyFilter>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> virtualinvoke $r10.<org.apache.hadoop.tools.CopyFilter: void initialize()>()
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void initialize()>
		 -> $r1 = r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile>
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void initialize()>
		 -> $r2 = virtualinvoke $r1.<java.io.File: java.nio.file.Path toPath()>()
The sink $r15 = virtualinvoke r9.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r9, $r8, r2, $r6)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r13, $r12, r2, $r10)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r17, $r16, r2, $r14)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r21, $r20, r2, $r18)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r24, $r23, r2, $r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r27 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r30 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r37 = specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r10 = virtualinvoke r9.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r15 = virtualinvoke r9.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $z0 == 0 goto $r2 = staticinvoke <org.apache.hadoop.mapred.gridmix.GenerateData: org.apache.hadoop.mapred.gridmix.GenerateData$DataStatistics publishPlainDataStatistics(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>(r0, r1) in method <org.apache.hadoop.mapred.gridmix.GenerateData: org.apache.hadoop.mapred.gridmix.GenerateData$DataStatistics publishDataStatistics(org.apache.hadoop.fs.Path,long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData: org.apache.hadoop.mapred.gridmix.GenerateData$DataStatistics publishDataStatistics(org.apache.hadoop.fs.Path,long,org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto $r2 = staticinvoke <org.apache.hadoop.mapred.gridmix.GenerateData: org.apache.hadoop.mapred.gridmix.GenerateData$DataStatistics publishPlainDataStatistics(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)>(r0, r1)
The sink r4 = virtualinvoke $r2.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r3) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.source", "etag") in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.source", "etag")
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke $r2.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r3)
The sink $z3 = virtualinvoke $r8.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.identity.transformer.domain.name", "") in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.identity.transformer.domain.name", "")
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: java.lang.String domainName> = $r6
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: java.lang.String domainName>
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z3 = virtualinvoke $r8.<java.lang.String: boolean isEmpty()>()
The sink $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z5 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z5)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>()
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z4 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z4)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" Persist:")
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z5)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if z0 == 0 goto r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0 in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if z0 == 0 goto r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
The sink $z0 = interfaceinvoke r0.<java.util.List: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>("batch threshold reached")
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $r5 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $z0 = interfaceinvoke r0.<java.util.List: boolean isEmpty()>()
The sink if $i6 > 0 goto $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB> = $i5
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i6 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $i6 > 0 goto $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
The sink $i2 = virtualinvoke r3.<java.lang.String: int length()>() in method <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r23, r11, r10, 0)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r14 = virtualinvoke r0.<java.io.File: java.lang.String getName()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r15 = $r14
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> virtualinvoke r4.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r5, r15, r2, $i3)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $i2 = virtualinvoke r3.<java.lang.String: int length()>()
The sink $r13 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink if i1 >= i0 goto return 1 in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve") in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve")
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r2 = r1
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> i0 = lengthof r2
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> if i1 >= i0 goto return 1
The sink $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r5) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r5)
- r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r5)
The sink virtualinvoke $r58.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("tmpfiles", r104) in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()> was called with values from the following sources:
- r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "") in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r103)
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(",")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r57 = virtualinvoke $r56.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r95)
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r104 = virtualinvoke $r57.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> virtualinvoke $r58.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("tmpfiles", r104)
The sink $r7 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r4, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r4, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r34 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r42, $r41, r2, $r39)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> $r136 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> $r34 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $b21 <= 0 goto $l25 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long lastByteStart> in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)> was called with values from the following sources:
- $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> = $l1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>(0L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l36 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize>
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l17 = l36 * $l16
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l20 = $l18 + $l17
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $b21 = $l20 cmp $l19
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> if $b21 <= 0 goto $l25 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long lastByteStart>
The sink interfaceinvoke r3.<java.util.List: boolean addAll(java.util.Collection)>(r2) in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("hadoop.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("hadoop.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r3.<java.util.List: boolean addAll(java.util.Collection)>(r2)
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r3 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList(java.lang.Iterable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r3.<java.util.List: boolean addAll(java.util.Collection)>(r2)
The sink $r1 = staticinvoke <org.apache.hadoop.yarn.api.records.Resource: org.apache.hadoop.yarn.api.records.Resource newInstance(long,int)>($l0, $i1) in method <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.am.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $i1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.am.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = staticinvoke <org.apache.hadoop.yarn.api.records.Resource: org.apache.hadoop.yarn.api.records.Resource newInstance(long,int)>($l0, $i1)
- $l0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("yarn.sls.am.container.memory", 1024L) in method <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("yarn.sls.am.container.memory", 1024L)
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = staticinvoke <org.apache.hadoop.yarn.api.records.Resource: org.apache.hadoop.yarn.api.records.Resource newInstance(long,int)>($l0, $i1)
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("region", $r8) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: int run(java.lang.String[],java.io.PrintStream)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>(r4, r53)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r1 = interfaceinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: java.util.Map getDiagnostics()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> $r8 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("region", $r8)
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: int run(java.lang.String[],java.io.PrintStream)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>(r4, r53)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r1 = interfaceinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: java.util.Map getDiagnostics()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> $r8 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("region", $r8)
The sink $c1 = virtualinvoke r1.<java.lang.String: char charAt(int)>(i2) in method <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $c1 = virtualinvoke r1.<java.lang.String: char charAt(int)>(i2)
The sink if r1 == null goto return r0 in method <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> if r1 == null goto return r0
The sink staticinvoke <com.google.common.base.Preconditions: void checkState(boolean,java.lang.Object)>($z0, "Delegation tokens are not enabled") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.delegation.token.binding", "") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.delegation.token.binding", "")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>($r1)
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkState(boolean,java.lang.Object)>($z0, "Delegation tokens are not enabled")
The sink l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
The sink staticinvoke <java.lang.Thread: void sleep(long)>(l0) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)> was called with values from the following sources:
- l0 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.background.sleep", 25L, $r6) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> l0 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.background.sleep", 25L, $r6)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> staticinvoke <java.lang.Thread: void sleep(long)>(l0)
The sink if z3 == 0 goto $z5 = interfaceinvoke r10.<java.util.Iterator: boolean hasNext()>() in method <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)> was called with values from the following sources:
- z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0) in method <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> z1 = virtualinvoke $r20.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r99 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)>(r29, $r31, $z2, z10, z1, $l0, $l1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: org.apache.hadoop.tools.CopyListingFileStatus toCopyListingFileStatusHelper(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,long,long)>
		 -> if z3 == 0 goto $z5 = interfaceinvoke r10.<java.util.Iterator: boolean hasNext()>()
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSBadRequestException;", $r35) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSBadRequestException;", $r35)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> throw $r2
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSBadRequestException;", $r35)
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSBadRequestException;", $r35)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> throw $r2
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSBadRequestException;", $r35)
The sink if r2 == null goto r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.type") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.codec") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.codec")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> if r2 == null goto r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.type")
The sink r9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r11) in method <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r15 = staticinvoke <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>(r1, r2, null)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r8)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> r11 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> r9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r11)
The sink $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r3) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r3.<org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>(r1)
	 -> <org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.fs.Path: java.net.URI toUri()>()
	 -> <org.apache.hadoop.fs.Path: java.net.URI toUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r3)
The sink $i0 = virtualinvoke r8.<java.lang.String: int indexOf(int)>(46) in method <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r15 = staticinvoke <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>(r1, r2, null)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> $i0 = virtualinvoke r8.<java.lang.String: int indexOf(int)>(46)
The sink $r22 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i24) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- i4 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.storage.timeout", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> i4 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.storage.timeout", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i24 = i4
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r22 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i24)
The sink $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r2, r0) in method <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.adl.oauth2.access.token.provider", null, class "Lorg/apache/hadoop/fs/adl/oauth2/AzureADTokenProvider;") in method <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.adl.oauth2.access.token.provider", null, class "Lorg/apache/hadoop/fs/adl/oauth2/AzureADTokenProvider;")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r2, r0)
The sink r33 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("]")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r33 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.String toString()>()
The sink virtualinvoke $r46.<org.apache.hadoop.fs.FileSystem: boolean delete(org.apache.hadoop.fs.Path,boolean)>(r41, 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r14 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>(r6)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r16 = staticinvoke <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>($r14, $r15)
	 -> <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke $r4.<org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>(r0, r3)
	 -> <org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>(r1, r2)
	 -> <org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)>(r1, r2, 0L)
	 -> <org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)>
		 -> $r3 = virtualinvoke r1.<java.net.URI: java.lang.String getScheme()>()
	 -> <org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)>
		 -> $r10 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toLowerCase(java.lang.String)>($r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toLowerCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toLowerCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)>
		 -> r0.<org.apache.hadoop.fs.FileSystem$Cache$Key: java.lang.String scheme> = $r10
	 -> <org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)>
		 -> return
	 -> <org.apache.hadoop.fs.FileSystem$Cache$Key: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = specialinvoke r4.<org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)>(r1, r2, r3)
	 -> <org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)>
		 -> staticinvoke <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem$Cache$Key)>(r28, r2)
	 -> <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem$Cache$Key)>
		 -> r0.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem$Cache$Key key> = r1
	 -> <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem$Cache$Key)>
		 -> return r1
	 -> <org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)>
		 -> return r28
	 -> <org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> return $r5
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: org.apache.hadoop.fs.FileSystem fs> = $r16
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r46 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: org.apache.hadoop.fs.FileSystem fs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r46.<org.apache.hadoop.fs.FileSystem: boolean delete(org.apache.hadoop.fs.Path,boolean)>(r41, 1)
The sink $z0 = virtualinvoke r0.<java.lang.String: boolean equalsIgnoreCase(java.lang.String)>($r3) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean isStorageEmulatorAccount(java.lang.String)> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.storage.emulator.account.name", "storageemulator") in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean isStorageEmulatorAccount(java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean isStorageEmulatorAccount(java.lang.String)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.storage.emulator.account.name", "storageemulator")
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean isStorageEmulatorAccount(java.lang.String)>
		 -> $z0 = virtualinvoke r0.<java.lang.String: boolean equalsIgnoreCase(java.lang.String)>($r3)
The sink $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
The sink staticinvoke <org.apache.commons.io.FileUtils: void copyURLToFile(java.net.URL,java.io.File,int,int)>(r19, r10, 10000, 60000) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror") in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror")
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r31)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r17 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r16)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> specialinvoke $r12.<java.net.URL: void <init>(java.lang.String)>($r18)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> r19 = $r12
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.commons.io.FileUtils: void copyURLToFile(java.net.URL,java.io.File,int,int)>(r19, r10, 10000, 60000)
The sink if $z0 != 0 goto $r6 = r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.azure.NativeFileSystemStore store> in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.skip.metrics", 0) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.skip.metrics", 0)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $z0 != 0 goto $r6 = r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.azure.NativeFileSystemStore store>
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2)
The sink $z0 = interfaceinvoke r5.<java.util.Iterator: boolean hasNext()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> throw $r30
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> throw $r40
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>
		 -> throw $r10
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r2 = virtualinvoke $r1.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.RMContext getRMContext()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r3 = interfaceinvoke $r2.<org.apache.hadoop.yarn.server.resourcemanager.RMContext: java.util.concurrent.ConcurrentMap getRMNodes()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r4 = interfaceinvoke $r3.<java.util.concurrent.ConcurrentMap: java.util.Collection values()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> r5 = interfaceinvoke $r4.<java.util.Collection: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $z0 = interfaceinvoke r5.<java.util.Iterator: boolean hasNext()>()
The sink $i16 = virtualinvoke r70.<java.lang.String: int length()>() in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal") in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal")
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $i16 = virtualinvoke r70.<java.lang.String: int length()>()
The sink if r0 == null goto r4 = "#()" in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: java.lang.String stateAsString(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState)> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> l0 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r4 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r7 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>($r6, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> $r3 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore store> = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> virtualinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.BulkOperationState: void close()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void close()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: java.lang.String stateAsString(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: java.lang.String stateAsString(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState)>
		 -> if r0 == null goto r4 = "#()"
- $l1 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r2) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.io.retry.RetryPolicy batchWriteRetryPolicy> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> l0 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r4 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r7 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>($r6, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> $r3 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore store> = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> virtualinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.BulkOperationState: void close()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void close()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: java.lang.String stateAsString(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: java.lang.String stateAsString(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState)>
		 -> if r0 == null goto r4 = "#()"
The sink specialinvoke $r17.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()> was called with values from the following sources:
- i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> specialinvoke $r17.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3)
The sink $r18 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r23, r2) in method <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r17 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("rumen.datatypes.jobproperties.parsers", $r16) in method <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> $r17 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("rumen.datatypes.jobproperties.parsers", $r16)
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r21 = (java.lang.Class[]) $r17
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r22 = r21
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r23 = r22[i1]
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> $r18 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r23, r2)
The sink if $z4 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z4 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("staging")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if $z4 == 0 goto (branch)
The sink if z3 == 0 goto $r83 = "is not" in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if z3 == 0 goto $r83 = "is not"
The sink $r30 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r9, $r8, r2, $r6)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r13, $r12, r2, $r10)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r17, $r16, r2, $r14)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r21, $r20, r2, $r18)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r24, $r23, r2, $r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r27 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r30 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink $z0 = virtualinvoke r4.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.signing-algorithm", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.signing-algorithm", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $z0 = virtualinvoke r4.<java.lang.String: boolean isEmpty()>()
The sink if $b2 == 0 goto $r16 = new com.microsoft.azure.storage.blob.BlobRequestOptions in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l16 = staticinvoke <java.lang.Math: long max(long,long)>(134217728L, l0)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $l1 = l16 % 512L
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $b2 = $l1 cmp 0L
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> if $b2 == 0 goto $r16 = new com.microsoft.azure.storage.blob.BlobRequestOptions
The sink $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "") in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>()
The sink specialinvoke $r35.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r40) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/file")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l15)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r35.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r40)
The sink $r3 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i5) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i5 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r3 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i5)
The sink specialinvoke $r57.<java.io.EOFException: void <init>(java.lang.String)>(r49) in method <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r32 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> specialinvoke $r57.<java.io.EOFException: void <init>(java.lang.String)>(r49)
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r27 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> specialinvoke $r57.<java.io.EOFException: void <init>(java.lang.String)>(r49)
The sink if $b0 < 0 goto $r0 = new java.lang.RuntimeException in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $b0 = f1 cmpl 0.07F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> if $b0 < 0 goto $r0 = new java.lang.RuntimeException
The sink $r29 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>($f3) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $f1 = virtualinvoke $r19.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.azure.selfthrottling.write.factor", 1.0F) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $f1 = virtualinvoke $r19.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.azure.selfthrottling.write.factor", 1.0F)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: float selfThrottlingWriteFactor> = $f1
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $f3 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: float selfThrottlingWriteFactor>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r29 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>($f3)
The sink $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>() in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>()
The sink $r25 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i20) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $i10 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.backoff.interval", 30000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i10 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.backoff.interval", 30000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxBackoff> = $i10
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i20 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxBackoff>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r25 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i20)
The sink if i0 > 0 goto return i0 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.min.records_per_chunk", 5) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.min.records_per_chunk", 5)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
		 -> if i0 > 0 goto return i0
The sink specialinvoke $r15.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()> was called with values from the following sources:
- i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> specialinvoke $r15.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3)
The sink virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
The sink $i2 = virtualinvoke $r9.<java.lang.Integer: int intValue()>() in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r7 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r8 = interfaceinvoke $r6.<java.util.Map: java.lang.Object get(java.lang.Object)>($r7)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r9 = (java.lang.Integer) $r8
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $i2 = virtualinvoke $r9.<java.lang.Integer: int intValue()>()
The sink $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink specialinvoke r0.<java.lang.Object: void <init>()>() in method <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)> was called with values from the following sources:
- z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> r61 = virtualinvoke $r9.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>()
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>(r1)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>(r1, $r2)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: org.apache.hadoop.hdfs.server.namenode.TreeWalk this$0> = r1
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>
		 -> r0 := @this: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> r0 := @this: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>(r1)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>(r1, $r2)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0) in method <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r7 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r8 = interfaceinvoke $r6.<java.util.Map: java.lang.Object get(java.lang.Object)>($r7)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r9 = (java.lang.Integer) $r8
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $i2 = virtualinvoke $r9.<java.lang.Integer: int intValue()>()
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> return $i2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)>(r0, i0)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
The sink $z2 = virtualinvoke r1.<java.lang.String: boolean contains(java.lang.CharSequence)>("://") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<java.lang.String: boolean contains(java.lang.CharSequence)>("://")
The sink $r41 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i13) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB> = $i5
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i13 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i13)
The sink $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2) in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
The sink $r26 = virtualinvoke r7.<java.util.HashMap: java.util.Set entrySet()>() in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d0 = (double) l21
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d1 = 1.0 * $d0
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> d3 = $d2 / $d1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r47 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r26 = virtualinvoke r7.<java.util.HashMap: java.util.Set entrySet()>()
The sink $r45 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i17) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i3 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.throttle.delay", 0) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i3 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.throttle.delay", 0)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int throttleDelay> = $i3
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i17 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int throttleDelay>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r45 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i17)
The sink r2 = virtualinvoke r1.<java.lang.String: java.lang.String substring(int)>($i5) in method <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r11 = virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>(r1)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> r1 = virtualinvoke r0.<java.lang.String: java.lang.String substring(int)>($i3)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> r2 = virtualinvoke r1.<java.lang.String: java.lang.String substring(int)>($i5)
The sink $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z4 == 0 goto $z5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.atomic.copy", 0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $z4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.delete.missing.source", 0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> $z4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.delete.missing.source", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> if $z4 == 0 goto $z5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.atomic.copy", 0)
The sink if $z1 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("random")
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> if $z1 == 0 goto (branch)
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("random")
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> if $z1 == 0 goto (branch)
The sink $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
The sink specialinvoke $r4.<java.util.concurrent.ThreadPoolExecutor: void <init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory)>(i4, 2147483647, l1, $r6, $r5, $r7) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.threads.max", 10) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
		 -> i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.threads.max", 10)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r4.<java.util.concurrent.ThreadPoolExecutor: void <init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory)>(i4, 2147483647, l1, $r6, $r5, $r7)
The sink if $z4 == 0 goto $z6 = 0 in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.authorization", 0) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.authorization", 0)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $z4 == 0 goto $z6 = 0
The sink r6 = virtualinvoke $r4.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>($r5) in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()> was called with values from the following sources:
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998) in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> return $i0
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r0.<javax.ws.rs.core.UriBuilder: javax.ws.rs.core.UriBuilder port(int)>($i0)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r2.<javax.ws.rs.core.UriBuilder: java.net.URI build(java.lang.Object[])>($r3)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI baseURI> = $r4
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI baseURI>
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> return $r5
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()>
		 -> r6 = virtualinvoke $r4.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>($r5)
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>
		 -> virtualinvoke $r2.<java.util.ArrayList: boolean add(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>
		 -> $r2 = r0.<org.apache.hadoop.http.HttpServer2$Builder: java.util.ArrayList endpoints>
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>
		 -> r0 := @this: org.apache.hadoop.http.HttpServer2$Builder
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder setConf(org.apache.hadoop.conf.Configuration)>($r31)
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder setConf(org.apache.hadoop.conf.Configuration)>
		 -> return r0
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()>
		 -> r6 = virtualinvoke $r4.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>($r5)
The sink $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.proxy.host", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.proxy.host", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>()
The sink specialinvoke $r0.<org.apache.hadoop.fs.permission.PermissionStatus: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>($r2, $r5, r6) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.permission.PermissionStatus createPermissionStatus(org.apache.hadoop.fs.permission.FsPermission)> was called with values from the following sources:
- $r5 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.permissions.supergroup", "supergroup") in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.permission.PermissionStatus createPermissionStatus(org.apache.hadoop.fs.permission.FsPermission)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.permission.PermissionStatus createPermissionStatus(org.apache.hadoop.fs.permission.FsPermission)>
		 -> $r5 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.azure.permissions.supergroup", "supergroup")
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.permission.PermissionStatus createPermissionStatus(org.apache.hadoop.fs.permission.FsPermission)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.permission.PermissionStatus: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)>($r2, $r5, r6)
The sink $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy connectivityFailure> = $r13
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r28.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r29, $r31)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Invoker s3guardInvoker> = $r28
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
- $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean enableMultiObjectsDelete> = $z2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy connectivityFailure> = $r13
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r28.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r29, $r31)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Invoker s3guardInvoker> = $r28
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
- $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean failOnMetadataWriteError> = $z1
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.Listing: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Listing listing> = $r34
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: boolean magicCommitEnabled> = z0
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.SignerManager: org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider delegationTokenProvider> = r3
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.auth.SignerManager signerManager> = $r37
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
The sink $r6 = virtualinvoke r2.<org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: java.lang.String name()>() in method <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r5 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>($r4)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r2 = (org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy) $r1
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> return $r5
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getJobSubmissionPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r6 = virtualinvoke r2.<org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: java.lang.String name()>()
The sink if $b2 >= 0 goto $r2 = new java.text.SimpleDateFormat in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $b2 = $l1 cmp 0L
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $b2 >= 0 goto $r2 = new java.text.SimpleDateFormat
The sink $r10 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r7, r4) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)> was called with values from the following sources:
- r7 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.s3guard.ddb.client.factory.impl", $r6, class "Lorg/apache/hadoop/fs/s3a/s3guard/DynamoDBClientFactory;") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r7 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.s3guard.ddb.client.factory.impl", $r6, class "Lorg/apache/hadoop/fs/s3a/s3guard/DynamoDBClientFactory;")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r10 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r7, r4)
The sink if $i3 != 3 goto i6 = i6 + 1 in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $i3 = lengthof r6
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if $i3 != 3 goto i6 = i6 + 1
The sink $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/oss")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z0 == 0 goto $r5 = newarray (java.lang.Class)[2] in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn>
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>($r3)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto $r5 = newarray (java.lang.Class)[2]
The sink interfaceinvoke r50.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r60, r58) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> return $l0
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r58 = staticinvoke <org.apache.hadoop.yarn.api.records.LocalResource: org.apache.hadoop.yarn.api.records.LocalResource newInstance(org.apache.hadoop.yarn.api.records.URL,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.yarn.api.records.LocalResourceVisibility,long,long)>($r53, $r55, $r56, $l11, $l12)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> interfaceinvoke r50.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r60, r58)
The sink if $z0 != 0 goto r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4) in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $z0 = virtualinvoke r4.<java.io.File: boolean mkdirs()>()
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> if $z0 != 0 goto r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4)
The sink if $z15 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z15 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("staging")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z15 == 0 goto (branch)
The sink $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r6) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r3.<org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>(r1)
	 -> <org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.fs.Path: java.net.URI toUri()>()
	 -> <org.apache.hadoop.fs.Path: java.net.URI toUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r3)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", Unable to move to ")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r6)
The sink if $i8 > 0 goto $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB> = $i7
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i8 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $i8 > 0 goto $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64)
The sink if r2 != null goto $r3 = (org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore) r2 in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r7.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>($r9, $r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>($r10, r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r24.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>(r12, r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> if r2 != null goto $r3 = (org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore) r2
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r20.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>(r12, r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> if r2 != null goto $r3 = (org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore) r2
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/io/EOFException;", $r33) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/io/EOFException;", $r33)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/io/EOFException;", $r33)
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/io/EOFException;", $r33)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/io/EOFException;", $r33)
The sink if z4 != 0 goto $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z4 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.fast.upload", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z4 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.fast.upload", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if z4 != 0 goto $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
The sink $r31 = virtualinvoke $r30.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r31 = virtualinvoke $r30.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27)
The sink if b4 < 0 goto return 0 in method <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r5 = r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys>
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r6 = interfaceinvoke $r5.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r7 = interfaceinvoke $r6.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r8 = interfaceinvoke r7.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r10 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r11 = (java.lang.String) $r10
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> virtualinvoke r1.<org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator: java.lang.String validate(java.lang.String)>($r11)
	 -> <org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator: java.lang.String validate(java.lang.String)>
		 -> $z0 = staticinvoke <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>
		 -> $c3 = virtualinvoke r0.<java.lang.String: char charAt(int)>(i15)
	 -> <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>
		 -> b4 = (byte) $c3
	 -> <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>
		 -> if b4 < 0 goto return 0
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the job output data.") in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f5)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the job output data.")
The sink $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
The sink if $z2 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("file")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if $z2 == 0 goto (branch)
The sink i13 = staticinvoke <java.lang.Math: int min(int,int)>($i7, i6) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i5 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f3 = (float) i5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f5 = $f4 / $f3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d2 = (double) $f5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d3 = staticinvoke <java.lang.Math: double ceil(double)>($d2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i6 = (int) $d3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i13 = staticinvoke <java.lang.Math: int min(int,int)>($i7, i6)
The sink $l5 = virtualinvoke $r13.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>($l3, $r12) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l3 = (long) i2
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l5 = virtualinvoke $r13.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>($l3, $r12)
The sink $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r40) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r40)
The sink $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r22) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r22 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustertype", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r22 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustertype", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r22)
- r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r72)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r22)
The sink if $i7 != 1 goto $r9 = r6[0] in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $i7 = lengthof r6
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if $i7 != 1 goto $r9 = r6[0]
The sink $r13 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d4) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)> was called with values from the following sources:
- $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f7 = $f3 * $f4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d5 = (double) f7
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of missing blocks", "Hadoop:service=NameNode,name=FSNamesystem", "MissingBlocks", $d5, $d4, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r13 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d4)
- $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f2 = $f0 * $f1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> i1 = (int) $f2
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d2 = (double) i1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of live DataNodes", "Hadoop:service=NameNode,name=FSNamesystemState", "NumLiveDataNodes", $d2, $d1, 0, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r13 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d4)
- $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f8 = $f5 * $f6
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d8 = (double) f8
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of under replicated blocks", "Hadoop:service=NameNode,name=FSNamesystemState", "UnderReplicatedBlocks", $d8, $d7, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r13 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d4)
The sink if $z0 == 0 goto i1 = i1 + 1 in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $z0 = virtualinvoke $r19.<java.lang.String: boolean equalsIgnoreCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> if $z0 == 0 goto i1 = i1 + 1
The sink $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r16) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r16 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.policy", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r16 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.policy", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r16)
The sink specialinvoke $r34.<java.util.jar.JarFile: void <init>(java.lang.String)>(r27) in method <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r26 = interfaceinvoke r4.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r5 = interfaceinvoke r26.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r27 = (java.lang.String) $r5
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r34.<java.util.jar.JarFile: void <init>(java.lang.String)>(r27)
The sink $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r31 = virtualinvoke $r30.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.String toString()>()
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.map.output.compress.codec", r4) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.map.output.compress.codec") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.map.output.compress.codec")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.map.output.compress.codec", r4)
The sink $z1 = interfaceinvoke r61.<java.util.Iterator: boolean hasNext()>() in method <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])> was called with values from the following sources:
- z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> r61 = virtualinvoke $r9.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>()
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>(r1)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>(r1, $r2)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: org.apache.hadoop.hdfs.server.namenode.TreeWalk this$0> = r1
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>
		 -> r0 := @this: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> r0 := @this: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r0 := @this: org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> throw $r10
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> return $r3
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> $z1 = interfaceinvoke r61.<java.util.Iterator: boolean hasNext()>()
The sink if $b5 >= 0 goto $l6 = r0.<org.apache.hadoop.fs.azure.PageBlobOutputStream: long configuredPageBlobExtensionSize> in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l3 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.extension.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $l3 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.extension.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.PageBlobOutputStream: long configuredPageBlobExtensionSize> = $l3
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $l4 = r0.<org.apache.hadoop.fs.azure.PageBlobOutputStream: long configuredPageBlobExtensionSize>
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $b5 = $l4 cmp 134217728L
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> if $b5 >= 0 goto $l6 = r0.<org.apache.hadoop.fs.azure.PageBlobOutputStream: long configuredPageBlobExtensionSize>
The sink $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4) in method <org.apache.hadoop.streaming.StreamJob: void msg(java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r2.<org.apache.hadoop.streaming.StreamJob: void msg(java.lang.String)>($r7)
	 -> <org.apache.hadoop.streaming.StreamJob: void msg(java.lang.String)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4)
The sink virtualinvoke r0.<com.amazonaws.ClientConfiguration: void setRequestTimeout(int)>($i7) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- l8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)>("fs.s3a.connection.request.timeout", 0L, $r3, $r2) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> l8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)>("fs.s3a.connection.request.timeout", 0L, $r3, $r2)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $i7 = (int) l8
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> virtualinvoke r0.<com.amazonaws.ClientConfiguration: void setRequestTimeout(int)>($i7)
The sink specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStart()>() in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStart()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStart()>()
The sink $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", Hadoop/") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r9 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", Hadoop/")
The sink $l13 = virtualinvoke $r10.<org.apache.hadoop.yarn.api.records.Resource: long getMemorySize()>() in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $i11 = i17 * i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l12 = (long) $i11
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation: org.apache.hadoop.yarn.api.records.Resource getCapacityAtTime(long)>($l12)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l13 = virtualinvoke $r10.<org.apache.hadoop.yarn.api.records.Resource: long getMemorySize()>()
The sink if $b1 > 0 goto $z0 = 0 in method <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean done()> was called with values from the following sources:
- $l1 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.max.total.scan", 109951162777600L) in method <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
		 -> $l1 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.max.total.scan", 109951162777600L)
	 -> <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
		 -> specialinvoke $r8.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: void <init>(long,long)>($l0, $l1)
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: void <init>(long,long)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: long totalScan> = l1
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: void <init>(long,long)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
		 -> specialinvoke $r3.<org.apache.hadoop.mapred.gridmix.FilePool$InnerDesc: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter)>($r6, $r7, $r8)
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$InnerDesc: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter)>
		 -> $z8 = virtualinvoke r9.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean done()>()
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean done()>
		 -> $l0 = r0.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: long totalScan>
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean done()>
		 -> $b1 = $l0 cmp 0L
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean done()>
		 -> if $b1 > 0 goto $z0 = 0
The sink virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
The sink if $b26 > 0 goto $r9 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.fs.FileStatus current> in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l21 = l21 - l22
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $b26 = l21 cmp 0L
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> if $b26 > 0 goto $r9 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.fs.FileStatus current>
The sink if $z3 == 0 goto $r95 = "(unset)" in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r94 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r94 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r94)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z3 == 0 goto $r95 = "(unset)"
The sink if $z2 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> r1 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $z2 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("disk")
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> if $z2 == 0 goto (branch)
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.track.missing.source") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.track.missing.source")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2)
The sink r34 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>($r10) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r10 = r6[2]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r34 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>($r10)
The sink specialinvoke $r2.<java.util.ArrayList: void <init>(int)>(i0) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.num-mappers", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.num-mappers", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> specialinvoke $r2.<java.util.ArrayList: void <init>(int)>(i0)
The sink if $z0 != 0 goto $z2 = 0 in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> if $z0 != 0 goto $z2 = 0
The sink $r8 = virtualinvoke r37.<java.lang.Class: java.lang.reflect.Constructor getDeclaredConstructor(java.lang.Class[])>($r7) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r37 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r37.<java.lang.Class: java.lang.reflect.Constructor getDeclaredConstructor(java.lang.Class[])>($r7)
The sink if $i10 > 0 goto $r30 = newarray (java.lang.Object)[15] in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB> = $i9
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i10 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $i10 > 0 goto $r30 = newarray (java.lang.Object)[15]
The sink $z0 = virtualinvoke r10.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- r10 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.copy.listing.class", "") in method <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
		 -> r10 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.copy.listing.class", "")
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.tools.CopyListing getCopyListing(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r10.<java.lang.String: boolean isEmpty()>()
The sink if $z1 == 0 goto r9 = virtualinvoke r2.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path)>(r0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: java.io.InputStream getPossiblyDecompressedInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.input-decompression.enable", 0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isInputCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isInputCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.input-decompression.enable", 0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isInputCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: java.io.InputStream getPossiblyDecompressedInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long)>
		 -> if $z1 == 0 goto r9 = virtualinvoke r2.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path)>(r0)
The sink specialinvoke $r18.<com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder: void <init>(java.lang.String,java.lang.String)>($r20, $r19) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r20 = r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn>
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r18.<com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder: void <init>(java.lang.String,java.lang.String)>($r20, $r19)
- $r14 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.session.name", $r13) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.session.name", $r13)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String sessionName> = $r14
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r19 = r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String sessionName>
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r18.<com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder: void <init>(java.lang.String,java.lang.String)>($r20, $r19)
The sink $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)> was called with values from the following sources:
- $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: long startTimestampMs> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $l1 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.distcache.byte.count", -1L) in method <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.distcache.byte.count", -1L)
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l1)
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
The sink r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r26
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r29 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker readOp>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: org.apache.hadoop.fs.s3a.Invoker readOp> = r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r26
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> specialinvoke $r14.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>(r33)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: org.apache.hadoop.conf.Configuration getConf()>()
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $r5 == null goto $r6 = new java.io.IOException in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.cred.service.urls") in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.cred.service.urls")
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: java.lang.String[] commaSeparatedUrls> = $r4
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: java.lang.String[] commaSeparatedUrls>
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $r5 == null goto $r6 = new java.io.IOException
The sink $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror") in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror")
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r31)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r17 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r16)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l1) in method <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.distcache.byte.count", -1L) in method <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.distcache.byte.count", -1L)
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l1)
The sink $r45 = virtualinvoke $r42.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_average_block_download_latency_ms", $r44, 0L) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r38 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r36[0] = $r38
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r39 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("The average latency in milliseconds of uploading a single block. The average latency is calculated over a %d-second rolling window.", $r36)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r40 = virtualinvoke $r37.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_average_block_upload_latency_ms", $r39, 0L)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>
		 -> $r3 = staticinvoke <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys: java.lang.Object add(java.lang.Object,java.lang.Object)>(r1, r2)
	 -> <org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys: java.lang.Object add(java.lang.Object,java.lang.Object)>
		 -> r10 = interfaceinvoke $r12.<java.util.Map: java.lang.Object get(java.lang.Object)>(r4)
	 -> <org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys: java.lang.Object add(java.lang.Object,java.lang.Object)>
		 -> return r10
	 -> <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>
		 -> $r5 = (org.apache.hadoop.metrics2.MetricsInfo) $r4
	 -> <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>($r3, l0)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> specialinvoke $r3.<org.apache.hadoop.metrics2.lib.MutableGaugeLong: void <init>(org.apache.hadoop.metrics2.MetricsInfo,long)>(r1, l0)
	 -> <org.apache.hadoop.metrics2.lib.MutableGaugeLong: void <init>(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> specialinvoke r0.<org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>(r1)
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object,java.lang.Object)>(r1, "metric info")
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> $r3 = (org.apache.hadoop.metrics2.MetricsInfo) $r2
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> r0.<org.apache.hadoop.metrics2.lib.MutableGauge: org.apache.hadoop.metrics2.MetricsInfo info> = $r3
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> return
	 -> <org.apache.hadoop.metrics2.lib.MutableGaugeLong: void <init>(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> return
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> r4 = $r3
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> interfaceinvoke $r5.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r6, r4)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> $r5 = r0.<org.apache.hadoop.metrics2.lib.MetricsRegistry: java.util.Map metricsMap>
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> specialinvoke r0.<org.apache.hadoop.metrics2.lib.MetricsRegistry: void checkMetricName(java.lang.String)>($r2)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: void checkMetricName(java.lang.String)>
		 -> r1 := @this: org.apache.hadoop.metrics2.lib.MetricsRegistry
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> r0 := @this: org.apache.hadoop.metrics2.lib.MetricsRegistry
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>
		 -> r0 := @this: org.apache.hadoop.metrics2.lib.MetricsRegistry
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r37 = r0.<org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: org.apache.hadoop.metrics2.lib.MetricsRegistry registry>
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r42 = r0.<org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: org.apache.hadoop.metrics2.lib.MetricsRegistry registry>
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r45 = virtualinvoke $r42.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_average_block_download_latency_ms", $r44, 0L)
The sink $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
The sink $r44 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("The average latency in milliseconds of downloading a single block. The average latency is calculated over a %d-second rolling window.", $r41) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r43 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r41[0] = $r43
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r44 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("The average latency in milliseconds of downloading a single block. The average latency is calculated over a %d-second rolling window.", $r41)
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i1) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", numMaps:")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i1)
The sink if r2 == null goto $r3 = <org.apache.hadoop.fs.s3a.auth.SignerManager: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if r2 == null goto $r3 = <org.apache.hadoop.fs.s3a.auth.SignerManager: org.slf4j.Logger LOG>
The sink $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f4) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f4)
The sink staticinvoke <com.amazonaws.auth.SignerFactory: void registerSigner(java.lang.String,java.lang.Class)>(r0, r12) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r9 = r6[0]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>($r9, $r8, $r7)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <com.amazonaws.auth.SignerFactory: void registerSigner(java.lang.String,java.lang.Class)>(r0, r12)
The sink specialinvoke $r29.<org.apache.hadoop.mapreduce.lib.input.CombineFileSplit: void <init>(org.apache.hadoop.fs.Path[],long[],long[],java.lang.String[])>($r32, $r33, $r34, r28) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l6 = $l5 + l22
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart> = $l6
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l0 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart>
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r11 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>($l0)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r3.<java.util.ArrayList: boolean add(java.lang.Object)>($r11)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r33 = specialinvoke r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>(r3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> $r2 = virtualinvoke r0.<java.util.ArrayList: java.lang.Object get(int)>(i3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> $r3 = (java.lang.Long) $r2
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> $l2 = virtualinvoke $r3.<java.lang.Long: long longValue()>()
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> r1[i3] = $l2
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> return r1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> specialinvoke $r29.<org.apache.hadoop.mapreduce.lib.input.CombineFileSplit: void <init>(org.apache.hadoop.fs.Path[],long[],long[],java.lang.String[])>($r32, $r33, $r34, r28)
The sink $z5 = virtualinvoke r103.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()> was called with values from the following sources:
- r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "") in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $z5 = virtualinvoke r103.<java.lang.String: boolean isEmpty()>()
The sink $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>(r1, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.CopyFilter copyFilter> = $r9
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r41
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a valid value for ") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a valid value for ")
The sink if $b6 <= 0 goto $l7 = l4 in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15) in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15)
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $b6 = l4 cmp $l5
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $b6 <= 0 goto $l7 = l4
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2) in method <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2)
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod> = $l0
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> r4 = $r2
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r4.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>(r3)
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l2 = virtualinvoke r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>()
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> $l0 = r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod>
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> return $l0
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l3 = $l2 * 24L
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l1 = $l3 * 60L
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l5 = l1 - 5L
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $b6 = l4 cmp $l5
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $b6 <= 0 goto $l7 = l4
The sink if $b7 < 0 goto i8 = i8 + 1 in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)> was called with values from the following sources:
- $i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.fsck.temp.expiry.seconds", 3600) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)>
		 -> $i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.fsck.temp.expiry.seconds", 3600)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)>
		 -> $i2 = $i1 * 1000
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)>
		 -> $l3 = (long) $i2
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)>
		 -> l4 = $l0 - $l3
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)>
		 -> $b7 = $l6 cmp l4
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler)>
		 -> if $b7 < 0 goto i8 = i8 + 1
The sink virtualinvoke r11.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withBillingMode(com.amazonaws.services.dynamodbv2.model.BillingMode)>($r13) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r15 = (java.lang.String) $r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>(r16)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> r11 = virtualinvoke $r9.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withTags(java.util.Collection)>($r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> virtualinvoke r11.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withBillingMode(com.amazonaws.services.dynamodbv2.model.BillingMode)>($r13)
The sink $r66 = virtualinvoke $r65.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB> = $i5
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i21 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke $r64.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i21)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r66 = virtualinvoke $r65.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $b1 > 0 goto $r0 = new java.lang.RuntimeException in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $b1 = f1 cmpg 0.68F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> if $b1 > 0 goto $r0 = new java.lang.RuntimeException
The sink $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i2) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads> = $i1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i2 = r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i2)
The sink staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object,java.lang.Object)>(r0, "Null \'out\' path") in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingJobAttemptsPath(org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid> = $r8
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r9 = r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid>
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r10 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path buildWorkPath(org.apache.hadoop.mapreduce.JobContext,java.lang.String)>($r24, $r9)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path buildWorkPath(org.apache.hadoop.mapreduce.JobContext,java.lang.String)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>($r2, r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>($r1, r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>(r2, r13, r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.lang.String cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r6 = interfaceinvoke $r4.<com.google.common.cache.Cache: java.lang.Object get(java.lang.Object,java.util.concurrent.Callable)>(r1, $r5)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r7 = (org.apache.hadoop.fs.Path) $r6
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>(r0, $r4)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingTaskAttemptsPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>($r7, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingTaskAttemptsPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r2 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r2 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingJobAttemptsPath(org.apache.hadoop.fs.Path)>(r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingJobAttemptsPath(org.apache.hadoop.fs.Path)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object,java.lang.Object)>(r0, "Null \'out\' path")
The sink r5 = interfaceinvoke $r4.<java.util.Collection: java.util.Iterator iterator()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r8 = r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$1: org.apache.hadoop.yarn.sls.SLSRunner val$se> = r2
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm> = $r24
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> throw $r40
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>
		 -> throw $r10
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r2 = virtualinvoke $r1.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.RMContext getRMContext()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r3 = interfaceinvoke $r2.<org.apache.hadoop.yarn.server.resourcemanager.RMContext: java.util.concurrent.ConcurrentMap getRMNodes()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r4 = interfaceinvoke $r3.<java.util.concurrent.ConcurrentMap: java.util.Collection values()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> r5 = interfaceinvoke $r4.<java.util.Collection: java.util.Iterator iterator()>()
The sink $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2) in method <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
The sink $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2) in method <org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l12 = virtualinvoke $r57.<org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l12 = virtualinvoke $r57.<org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
The sink virtualinvoke $r21.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: void start()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r21 = r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> virtualinvoke $r21.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: void start()>()
The sink if $z3 == 0 goto $z6 = 0 in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $z3 == 0 goto $z6 = 0
The sink $r28 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>($f2) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $f0 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.azure.selfthrottling.read.factor", 1.0F) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $f0 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.azure.selfthrottling.read.factor", 1.0F)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: float selfThrottlingReadFactor> = $f0
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $f2 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: float selfThrottlingReadFactor>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r28 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>($f2)
The sink $r57 = virtualinvoke $r56.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r95) in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()> was called with values from the following sources:
- r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "") in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r103)
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(",")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r57 = virtualinvoke $r56.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r95)
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z0, "Not an AncestorState %s", r0) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int markAsAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)> was called with values from the following sources:
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> l0 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r4 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r7 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>($r6, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> $r3 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore store> = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> interfaceinvoke r4.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: int markAsAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r3, r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int markAsAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z0, "Not an AncestorState %s", r0)
The sink virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setApplicationACLs(java.util.Map)>(r38) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $r41 = virtualinvoke $r40.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r41 = virtualinvoke $r40.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> interfaceinvoke r38.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r39, $r41)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setApplicationACLs(java.util.Map)>(r38)
The sink $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.split.ratio", 2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.split.ratio", 2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> return i2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = (float) i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> i4 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> $i8 = i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> return $i8
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
The sink $r4 = interfaceinvoke r1.<java.util.Map: java.lang.Object get(java.lang.Object)>($r3) in method <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r52 = virtualinvoke $r51.<org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> $r4 = interfaceinvoke r1.<java.util.Map: java.lang.Object get(java.lang.Object)>($r3)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r52 = virtualinvoke $r51.<org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> $r4 = interfaceinvoke r1.<java.util.Map: java.lang.Object get(java.lang.Object)>($r3)
The sink r14 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationDefinition: org.apache.hadoop.yarn.api.records.ReservationDefinition newInstance(long,long,org.apache.hadoop.yarn.api.records.ReservationRequests,java.lang.String)>(l1, l2, r13, "LpSolver#toRecurringRDL") in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l15 = (long) i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r12 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequest: org.apache.hadoop.yarn.api.records.ReservationRequest newInstance(org.apache.hadoop.yarn.api.records.Resource,int,int,long)>(r2, $i16, 1, $l15)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>(r12)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r13 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequests: org.apache.hadoop.yarn.api.records.ReservationRequests newInstance(java.util.List,org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter)>(r4, $r5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r14 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationDefinition: org.apache.hadoop.yarn.api.records.ReservationDefinition newInstance(long,long,org.apache.hadoop.yarn.api.records.ReservationRequests,java.lang.String)>(l1, l2, r13, "LpSolver#toRecurringRDL")
The sink virtualinvoke $r14.<java.util.concurrent.TimeUnit: void sleep(long)>(l14) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- l10 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l10 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l11 = virtualinvoke $r7.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l10, $r6)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l14 = staticinvoke <java.lang.Math: long min(long,long)>(l5, l11)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> virtualinvoke $r14.<java.util.concurrent.TimeUnit: void sleep(long)>(l14)
The sink if i2 >= 0 goto $l4 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if i2 >= 0 goto $l4 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
The sink $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r5 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>($r4)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0)
The sink r19 = virtualinvoke r20.<java.lang.Class: java.lang.String getName()>() in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.class")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r19)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>(r1)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>
		 -> r27 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String,boolean,java.lang.ClassLoader)>(r5, 1, $r8)
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByNameOrNull(java.lang.String)>
		 -> return r27
	 -> <org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>
		 -> return r2
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r20 = virtualinvoke $r2.<java.lang.Class: java.lang.Class asSubclass(java.lang.Class)>(class "Lorg/apache/hadoop/tools/CopyFilter;")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r19 = virtualinvoke r20.<java.lang.Class: java.lang.String getName()>()
The sink interfaceinvoke r10.<java.util.List: boolean add(java.lang.Object)>($r44) in method <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)> was called with values from the following sources:
- i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> specialinvoke $r44.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>($r30, l17, r37, 20, "map")
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>(r1, l0, r2, i1, r3, $r4)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r1, l0, r2, i1, r3, r4, -1L, 0L)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> interfaceinvoke r10.<java.util.List: boolean add(java.lang.Object)>($r44)
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> specialinvoke $r44.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>($r30, l17, r37, 20, "map")
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>(r1, l0, r2, i1, r3, $r4)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r1, l0, r2, i1, r3, r4, -1L, 0L)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(org.apache.hadoop.tools.rumen.LoggedJob,long)>
		 -> interfaceinvoke r10.<java.util.List: boolean add(java.lang.Object)>($r44)
The sink if i0 < 0 goto $z0 = 0 in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()> was called with values from the following sources:
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.bucket.probe", 2) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.bucket.probe", 2)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()>
		 -> if i0 < 0 goto $z0 = 0
The sink if $r1 == null goto r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.metadatastore.impl", class "Lorg/apache/hadoop/fs/s3a/s3guard/NullMetadataStore;", class "Lorg/apache/hadoop/fs/s3a/s3guard/MetadataStore;") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
		 -> if $r1 == null goto r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.metadatastore.impl", class "Lorg/apache/hadoop/fs/s3a/s3guard/NullMetadataStore;", class "Lorg/apache/hadoop/fs/s3a/s3guard/MetadataStore;")
The sink specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1) in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1)
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object,java.lang.Object)>($z1, "childPath %s must be a child of %s", r1, $r6) in method <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)> was called with values from the following sources:
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> interfaceinvoke $r9.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r16, null)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r5 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r11 = r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r12 = interfaceinvoke $r11.<com.google.common.cache.Cache: java.lang.Object getIfPresent(java.lang.Object)>(r32)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r33 = (org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry) $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r13 = virtualinvoke r33.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata dirListingMetadata>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> virtualinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: boolean put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: boolean put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>
		 -> r4 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> $r6 = r4.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path path>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object,java.lang.Object)>($z1, "childPath %s must be a child of %s", r1, $r6)
The sink r58 = staticinvoke <org.apache.hadoop.yarn.api.records.LocalResource: org.apache.hadoop.yarn.api.records.LocalResource newInstance(org.apache.hadoop.yarn.api.records.URL,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.yarn.api.records.LocalResourceVisibility,long,long)>($r53, $r55, $r56, $l11, $l12) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> return $l0
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r58 = staticinvoke <org.apache.hadoop.yarn.api.records.LocalResource: org.apache.hadoop.yarn.api.records.LocalResource newInstance(org.apache.hadoop.yarn.api.records.URL,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.yarn.api.records.LocalResourceVisibility,long,long)>($r53, $r55, $r56, $l11, $l12)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> return $l0
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r58 = staticinvoke <org.apache.hadoop.yarn.api.records.LocalResource: org.apache.hadoop.yarn.api.records.LocalResource newInstance(org.apache.hadoop.yarn.api.records.URL,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.yarn.api.records.LocalResourceVisibility,long,long)>($r53, $r55, $r56, $l11, $l12)
The sink $r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the map output data.") in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f4)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the map output data.")
The sink interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
The sink if r39 != null goto $r4 = new java.lang.StringBuilder in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> if r39 != null goto $r4 = new java.lang.StringBuilder
The sink if z0 != 0 goto $r4 = <java.lang.Boolean: java.lang.Boolean TRUE> in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()> was called with values from the following sources:
- z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.s3guard.ddb.table.sse.enabled", 0) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.s3guard.ddb.table.sse.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> if z0 != 0 goto $r4 = <java.lang.Boolean: java.lang.Boolean TRUE>
The sink if $z14 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z14 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("directory")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z14 == 0 goto (branch)
The sink $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0) in method <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5) in method <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r52 = virtualinvoke $r51.<org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> $r4 = interfaceinvoke r1.<java.util.Map: java.lang.Object get(java.lang.Object)>($r3)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> $r5 = (java.lang.String) $r4
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r52 = virtualinvoke $r51.<org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> $r4 = interfaceinvoke r1.<java.util.Map: java.lang.Object get(java.lang.Object)>($r3)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> $r5 = (java.lang.String) $r4
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: org.apache.hadoop.fs.Path getPath(java.util.Map)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
The sink if $z3 != 0 goto $r4 = <java.lang.System: java.io.PrintStream out> in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z3 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $z3 != 0 goto $r4 = <java.lang.System: java.io.PrintStream out>
The sink $r25 = staticinvoke <java.util.Optional: java.util.Optional of(java.lang.Object)>($r24) in method <org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: java.util.Optional maybeInitSTS()> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>($r11)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r9.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>($r10, null)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls> = $r9
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r15 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r16 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)>($r15, r14)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: void <init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)>(r1, r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: void <init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: org.apache.hadoop.io.retry.RetryPolicy defaultPolicy> = r1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: void <init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = $r16
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: java.util.Optional maybeInitSTS()>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r13, $r14)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: java.util.Optional maybeInitSTS()>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: org.apache.hadoop.fs.s3a.Invoker invoker> = $r12
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: java.util.Optional maybeInitSTS()>
		 -> $r23 = r0.<org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: org.apache.hadoop.fs.s3a.Invoker invoker>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: java.util.Optional maybeInitSTS()>
		 -> $r24 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient createClientConnection(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker)>(r22, $r23)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient createClientConnection(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient: void <init>(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.fs.s3a.auth.STSClientFactory$1)>(r1, r2, null)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient: void <init>(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.fs.s3a.auth.STSClientFactory$1)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient: void <init>(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient: void <init>(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient: org.apache.hadoop.fs.s3a.Invoker invoker> = r2
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient: void <init>(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient: void <init>(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.fs.s3a.auth.STSClientFactory$1)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient createClientConnection(com.amazonaws.services.securitytoken.AWSSecurityTokenService,org.apache.hadoop.fs.s3a.Invoker)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding: java.util.Optional maybeInitSTS()>
		 -> $r25 = staticinvoke <java.util.Optional: java.util.Optional of(java.lang.Object)>($r24)
The sink interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28) in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
The sink interfaceinvoke $r31.<java.util.List: boolean add(java.lang.Object)>(r30) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $l0 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: long startTimestampMs> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r30 = $r28
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> interfaceinvoke $r31.<java.util.List: boolean add(java.lang.Object)>(r30)
- $z0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("auditreplay.create-blocks", 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $z0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("auditreplay.create-blocks", 1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: boolean createBlocks> = $z0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r30 = $r28
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> interfaceinvoke $r31.<java.util.List: boolean add(java.lang.Object)>(r30)
- $r10 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r10 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> $r11 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>($r10)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: java.net.URI namenodeUri> = $r11
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread: void <init>(org.apache.hadoop.mapreduce.Mapper$Context,java.util.concurrent.DelayQueue,java.util.concurrent.ConcurrentMap)>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r30 = $r28
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> interfaceinvoke $r31.<java.util.List: boolean add(java.lang.Object)>(r30)
The sink $z1 = virtualinvoke r4.<java.io.File: boolean exists()>() in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $z1 = virtualinvoke r4.<java.io.File: boolean exists()>()
The sink specialinvoke $r20.<java.lang.RuntimeException: void <init>(java.lang.String)>($r24) in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> specialinvoke $r20.<java.lang.RuntimeException: void <init>(java.lang.String)>($r24)
The sink $r7 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l8) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- l8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)>("fs.s3a.connection.request.timeout", 0L, $r3, $r2) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> l8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)>("fs.s3a.connection.request.timeout", 0L, $r3, $r2)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r7 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l8)
The sink $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r23, r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.metadatastore.impl", class "Lorg/apache/hadoop/fs/s3a/s3guard/NullMetadataStore;", class "Lorg/apache/hadoop/fs/s3a/s3guard/MetadataStore;") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.metadatastore.impl", class "Lorg/apache/hadoop/fs/s3a/s3guard/NullMetadataStore;", class "Lorg/apache/hadoop/fs/s3a/s3guard/MetadataStore;")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
		 -> return r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r23, r2)
The sink specialinvoke $r11.<java.lang.RuntimeException: void <init>(java.lang.String)>($r21) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r15 = virtualinvoke $r14.<java.lang.Class: java.lang.String getName()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r16 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r15)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a resource usage plugin as it does not extend ")
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r20 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r19)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> specialinvoke $r11.<java.lang.RuntimeException: void <init>(java.lang.String)>($r21)
The sink specialinvoke $r15.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>($i16, $i15, $i14, $i13) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $i12 = virtualinvoke $r14.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.retries", 30) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i12 = virtualinvoke $r14.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.retries", 30)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxRetries> = $i12
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i13 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxRetries>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> specialinvoke $r15.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>($i16, $i15, $i14, $i13)
- $i10 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.backoff.interval", 30000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i10 = virtualinvoke $r12.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.max.backoff.interval", 30000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxBackoff> = $i10
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i14 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int maxBackoff>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> specialinvoke $r15.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>($i16, $i15, $i14, $i13)
- $i11 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.backoff.interval", 3000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i11 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.backoff.interval", 3000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int deltaBackoff> = $i11
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i15 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int deltaBackoff>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> specialinvoke $r15.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>($i16, $i15, $i14, $i13)
- $i9 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.min.backoff.interval", 3000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i9 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.retry.min.backoff.interval", 3000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int minBackoff> = $i9
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $i16 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: int minBackoff>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> specialinvoke $r15.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>($i16, $i15, $i14, $i13)
The sink $r43 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i15) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.retry.count", 3) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.retry.count", 3)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int retryCount> = $i0
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i15 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int retryCount>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r43 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i15)
The sink $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/") in method <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r23, r11, r10, 0)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r14 = virtualinvoke r0.<java.io.File: java.lang.String getName()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r15 = $r14
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> virtualinvoke r4.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r5, r15, r2, $i3)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/")
The sink $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
The sink $z3 = virtualinvoke r68.<org.apache.hadoop.fs.FileSystem: boolean isFile(org.apache.hadoop.fs.Path)>(r67) in method <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r6 = $r4
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> interfaceinvoke r8.<java.util.List: boolean add(java.lang.Object)>(r6)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r14.<org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>(r8, r11)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List sourcePaths> = r2
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r16 = virtualinvoke $r14.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withOverwrite(boolean)>($z2)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withOverwrite(boolean)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r17 = virtualinvoke $r16.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withSyncFolder(boolean)>($z3)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withSyncFolder(boolean)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $r3 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: java.util.List sourcePaths> = $r3
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> $r2 = r0.<org.apache.hadoop.tools.DistCpOptions: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> $r3 = staticinvoke <java.util.Collections: java.util.List unmodifiableList(java.util.List)>($r2)
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> return $r3
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: java.util.List sourcePaths> = $r2
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r47 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r48 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r49 = interfaceinvoke $r48.<java.util.List: java.lang.Object get(int)>(0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> r67 = (org.apache.hadoop.fs.Path) $r49
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $z3 = virtualinvoke r68.<org.apache.hadoop.fs.FileSystem: boolean isFile(org.apache.hadoop.fs.Path)>(r67)
The sink specialinvoke $r16.<com.aliyun.oss.OSSClient: void <init>(java.lang.String,com.aliyun.oss.common.auth.CredentialsProvider,com.aliyun.oss.ClientConfiguration)>(r38, r39, r4) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke $r16.<com.aliyun.oss.OSSClient: void <init>(java.lang.String,com.aliyun.oss.common.auth.CredentialsProvider,com.aliyun.oss.ClientConfiguration)>(r38, r39, r4)
The sink $l22 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l7) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- i20 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i20 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l6 = (long) i20
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l7 = $l5 / $l6
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l22 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l7)
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z0, "Failed to initialize %s.", r3) in method <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.azure.delegation.token.provider.type", null, class "Lorg/apache/hadoop/fs/azurebfs/extensions/CustomDelegationTokenManager;") in method <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.azure.delegation.token.provider.type", null, class "Lorg/apache/hadoop/fs/azurebfs/extensions/CustomDelegationTokenManager;")
	 -> <org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z0, "Failed to initialize %s.", r3)
The sink $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
The sink r5 = virtualinvoke r4.<java.lang.String: java.lang.String[] split(java.lang.String)>(" ") in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script") in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script")
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String[] split(java.lang.String)>(" ")
The sink if $z5 == 0 goto $z6 = 0 in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $z5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", 0) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $z5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", 0)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $z5 == 0 goto $z6 = 0
The sink $r11 = interfaceinvoke r5.<java.util.Iterator: java.lang.Object next()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r8 = r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$1: org.apache.hadoop.yarn.sls.SLSRunner val$se> = r2
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm> = $r24
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> throw $r40
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>
		 -> throw $r10
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void printSimulationInfo()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r2 = virtualinvoke $r1.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.RMContext getRMContext()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r3 = interfaceinvoke $r2.<org.apache.hadoop.yarn.server.resourcemanager.RMContext: java.util.concurrent.ConcurrentMap getRMNodes()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r4 = interfaceinvoke $r3.<java.util.concurrent.ConcurrentMap: java.util.Collection values()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> r5 = interfaceinvoke $r4.<java.util.Collection: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void waitForNodesRunning()>
		 -> $r11 = interfaceinvoke r5.<java.util.Iterator: java.lang.Object next()>()
The sink $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_getConsistentItem_3__114: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,com.amazonaws.services.dynamodbv2.document.spec.GetItemSpec)>(r5, r4) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.Item getConsistentItem(org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Diff: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r10 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Diff: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Diff: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Diff: void compareRoot(org.apache.hadoop.fs.Path,java.io.PrintStream)>(r16, r11)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Diff: void compareRoot(org.apache.hadoop.fs.Path,java.io.PrintStream)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Diff: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Diff: void compareRoot(org.apache.hadoop.fs.Path,java.io.PrintStream)>
		 -> r11 = interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: org.apache.hadoop.fs.s3a.s3guard.PathMetadata get(org.apache.hadoop.fs.Path)>(r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.PathMetadata get(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata get(org.apache.hadoop.fs.Path)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata get(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata get(org.apache.hadoop.fs.Path,boolean)>(r1, 0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata get(org.apache.hadoop.fs.Path,boolean)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.Path checkPath(org.apache.hadoop.fs.Path)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.Path checkPath(org.apache.hadoop.fs.Path)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata get(org.apache.hadoop.fs.Path,boolean)>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata innerGet(org.apache.hadoop.fs.Path,boolean)>(r1, z0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata innerGet(org.apache.hadoop.fs.Path,boolean)>
		 -> r25 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.Item getConsistentItem(org.apache.hadoop.fs.Path)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.Item getConsistentItem(org.apache.hadoop.fs.Path)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_getConsistentItem_3__114: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,com.amazonaws.services.dynamodbv2.document.spec.GetItemSpec)>(r5, r4)
The sink $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>($r1) in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.delegation.token.binding", "") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.delegation.token.binding", "")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: boolean hasDelegationTokenBinding(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>($r1)
The sink specialinvoke $r8.<java.io.File: void <init>(java.lang.String)>(r39) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> specialinvoke $r8.<java.io.File: void <init>(java.lang.String)>(r39)
The sink r4 = virtualinvoke $r2.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r3) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.mode", "server") in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.mode", "server")
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke $r2.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r3)
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("mapreduce.output.fileoutputformat.compress", $z0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.output.fileoutputformat.compress", 0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.output.fileoutputformat.compress", 0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("mapreduce.output.fileoutputformat.compress", $z0)
The sink virtualinvoke r7.<com.amazonaws.ClientConfiguration: void setUserAgentPrefix(java.lang.String)>(r12) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", ")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r12)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> virtualinvoke r7.<com.amazonaws.ClientConfiguration: void setUserAgentPrefix(java.lang.String)>(r12)
The sink $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("spark.sql.sources.writeJobUUID", $r2) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("spark.sql.sources.writeJobUUID", $r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3)
The sink $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.DistCpSync: org.apache.hadoop.tools.CopyFilter copyFilter> = $r3
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> r7 = $r4
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> $z1 = virtualinvoke r7.<org.apache.hadoop.tools.DistCpSync: boolean sync()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> $z0 = specialinvoke r0.<org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>
		 -> throw $r50
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> return 0
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>(r2, r7)
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> specialinvoke $r2.<org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>($r4, $r5, r6)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.DistCpSync distCpSync> = r3
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $z1 = virtualinvoke $r3.<org.apache.hadoop.fs.Path: boolean equals(java.lang.Object)>($r5) in method <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)> was called with values from the following sources:
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> interfaceinvoke $r9.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r16, null)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r5 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r11 = r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r12 = interfaceinvoke $r11.<com.google.common.cache.Cache: java.lang.Object getIfPresent(java.lang.Object)>(r32)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r33 = (org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry) $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r13 = virtualinvoke r33.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata dirListingMetadata>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> virtualinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: boolean put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: boolean put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>
		 -> r4 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> $r5 = r4.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path path>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> $z1 = virtualinvoke $r3.<org.apache.hadoop.fs.Path: boolean equals(java.lang.Object)>($r5)
The sink specialinvoke $r15.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r2) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir") in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir")
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r2)
The sink $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", numMaps:") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", numMaps:")
The sink specialinvoke $r15.<java.lang.IllegalArgumentException: void <init>(java.lang.String,java.lang.Throwable)>($r19, r20) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> specialinvoke $r15.<java.lang.IllegalArgumentException: void <init>(java.lang.String,java.lang.Throwable)>($r19, r20)
The sink if $i3 > 0 goto $r3 = <org.apache.hadoop.tools.SimpleCopyListing: org.slf4j.Logger LOG> in method <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>(r1, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $i3 = r5.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> if $i3 > 0 goto $r3 = <org.apache.hadoop.tools.SimpleCopyListing: org.slf4j.Logger LOG>
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.tools.CopyListing simpleListing> = $r3
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r8 = r6.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.tools.CopyListing simpleListing>
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke $r8.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r7, r2)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $i3 = r5.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> if $i3 > 0 goto $r3 = <org.apache.hadoop.tools.SimpleCopyListing: org.slf4j.Logger LOG>
The sink virtualinvoke $r8.<java.util.ArrayList: boolean add(java.lang.Object)>($r43) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l9 = (long) $i8
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords> = $l11
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l13 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> l3 = $l12 / $l13
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f0 = (float) l3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f3 = $f0 * $f2
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i21 = (int) $f3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: int keyLen> = $i22
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r8.<java.util.ArrayList: boolean add(java.lang.Object)>($r43)
- $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f2 = staticinvoke <java.lang.Math: float min(float,float)>(1.0F, $f1)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f3 = $f0 * $f2
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i21 = (int) $f3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: int keyLen> = $i22
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r8.<java.util.ArrayList: boolean add(java.lang.Object)>($r43)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f1 = $f0 / f5
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l18 = (long) $f1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r43.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l18, $l2, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r8.<java.util.ArrayList: boolean add(java.lang.Object)>($r43)
The sink $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB> = $i7
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i11 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i11)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[8] = $r39
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30)
- $i1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.timeout", 15000) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.timeout", 15000)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int connectTimeout> = $i1
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i14 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int connectTimeout>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i14)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[11] = $r42
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30)
- $i2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.socket.timeout", 60000) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.socket.timeout", 60000)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int socketTimeout> = $i2
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i16 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int socketTimeout>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r44 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i16)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[13] = $r44
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30)
- $i0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.retry.count", 3) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.retry.count", 3)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int retryCount> = $i0
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i15 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int retryCount>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r43 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i15)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[12] = $r43
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30)
- $i3 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.throttle.delay", 0) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i3 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.throttle.delay", 0)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int throttleDelay> = $i3
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i17 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int throttleDelay>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r45 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i17)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[14] = $r45
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30)
- $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB> = $i9
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i12 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r40 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i12)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[9] = $r40
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30)
- $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB> = $i5
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i13 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i13)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[10] = $r41
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r46 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Service={%s} container={%s} uri={%s} tenant={%s} user={%s} region={%s} publicURL={%b} location aware={%b} partition size={%d KB}, buffer size={%d KB} block size={%d KB} connect timeout={%d}, retry count={%d} socket timeout={%d} throttle delay={%d}", $r30)
The sink if $z0 == 0 goto staticinvoke <com.amazonaws.regions.Regions: com.amazonaws.regions.Regions fromName(java.lang.String)>(r15) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> if $z0 == 0 goto staticinvoke <com.amazonaws.regions.Regions: com.amazonaws.regions.Regions fromName(java.lang.String)>(r15)
The sink $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
The sink l2 = virtualinvoke $r4.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l1, $r3) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l2 = virtualinvoke $r4.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l1, $r3)
The sink $z0 = virtualinvoke $r10.<java.lang.Class: boolean isAssignableFrom(java.lang.Class)>(r9) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $z0 = virtualinvoke $r10.<java.lang.Class: boolean isAssignableFrom(java.lang.Class)>(r9)
The sink if i4 < 0 goto (branch) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- i4 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.proxy.port", -1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> i4 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.proxy.port", -1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if i4 < 0 goto (branch)
The sink $r16 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r15) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r15 = virtualinvoke $r14.<java.lang.Class: java.lang.String getName()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r16 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r15)
The sink $r10 = virtualinvoke $r9.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding: java.lang.String getName()>() in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStart()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStart()>()
	 -> <org.apache.hadoop.service.AbstractService: void serviceStart()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.security.token.Token selectTokenFromFSOwner()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.security.token.Token selectTokenFromFSOwner()>
		 -> return $r6
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToDelegationToken(org.apache.hadoop.security.token.Token)>(r2)
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToDelegationToken(org.apache.hadoop.security.token.Token)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> $r9 = r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> $r10 = virtualinvoke $r9.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding: java.lang.String getName()>()
The sink if r31 != null goto $z1 = virtualinvoke r7.<java.io.File: boolean exists()>() in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror") in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror")
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> if r31 != null goto $z1 = virtualinvoke r7.<java.io.File: boolean exists()>()
The sink $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $z1 == 0 goto return r1 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext: java.lang.String getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto return r1
The sink staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean enableMultiObjectsDelete> = $z2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
- $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean failOnMetadataWriteError> = $z1
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.Listing: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Listing listing> = $r34
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
- $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean errorsIncludeSql> = $z1
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: boolean magicCommitEnabled> = z0
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.SignerManager: org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider delegationTokenProvider> = r3
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.auth.SignerManager signerManager> = $r37
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
- l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r61.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>(l13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = $r61
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r3)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory blockFactory> = $r54
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
- $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean enabled> = $z0
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>(r1, r2, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r3, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r1, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: boolean requireVersion> = z0
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy changeDetectionPolicy> = $r45
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r3 = (org.apache.hadoop.fs.s3a.WriteOperationHelper) $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.WriteOperationHelper operations> = $r3
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.auth.SignerManager
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.Listing
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.WriteOperationHelper: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper> = $r32
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r0)
The sink z0 = virtualinvoke $r6.<java.lang.String: boolean equals(java.lang.Object)>(r5) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> z0 = virtualinvoke $r6.<java.lang.String: boolean equals(java.lang.Object)>(r5)
- r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> z0 = virtualinvoke $r6.<java.lang.String: boolean equals(java.lang.Object)>(r5)
The sink specialinvoke $r64.<java.nio.file.AccessDeniedException: void <init>(java.lang.String,java.lang.String,java.lang.String)>(r2, null, r52) in method <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r32 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> specialinvoke $r64.<java.nio.file.AccessDeniedException: void <init>(java.lang.String,java.lang.String,java.lang.String)>(r2, null, r52)
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r27 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> specialinvoke $r64.<java.nio.file.AccessDeniedException: void <init>(java.lang.String,java.lang.String,java.lang.String)>(r2, null, r52)
The sink if $z0 == 0 goto virtualinvoke r23.<java.util.jar.JarOutputStream: void close()>() in method <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r26 = interfaceinvoke r4.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $z0 = interfaceinvoke r26.<java.util.Iterator: boolean hasNext()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> if $z0 == 0 goto virtualinvoke r23.<java.util.jar.JarOutputStream: void close()>()
The sink $r19 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> throw $r30
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r19 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyDomain(java.lang.String)>($r27) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $r27 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.domain") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r27 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.domain")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyDomain(java.lang.String)>($r27)
The sink $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
The sink $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/file") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/file")
The sink $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
The sink if $r4 == null goto $z1 = 0 in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $r4 == null goto $z1 = 0
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $r4 == null goto $z1 = 0
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Destroy: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $r4 == null goto $z1 = 0
The sink $z5 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$BooleanConfigurationValidatorAnnotation;") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r5 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Field[] getDeclaredFields()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r6 = r5
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r7 = r6[i3]
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $z5 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$BooleanConfigurationValidatorAnnotation;")
The sink specialinvoke $r41.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>(i3, i5, i4, i6) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)> was called with values from the following sources:
- i5 = virtualinvoke $r37.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.backoff.interval", 30000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> i5 = virtualinvoke $r37.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.backoff.interval", 30000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> specialinvoke $r41.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>(i3, i5, i4, i6)
- i6 = virtualinvoke $r38.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.max.retries", 15) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> i6 = virtualinvoke $r38.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.max.retries", 15)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> specialinvoke $r41.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>(i3, i5, i4, i6)
- i4 = virtualinvoke $r36.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.max.backoff.interval", 90000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> i4 = virtualinvoke $r36.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.max.backoff.interval", 90000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> specialinvoke $r41.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>(i3, i5, i4, i6)
- i3 = virtualinvoke $r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.min.backoff.interval", 3000) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> i3 = virtualinvoke $r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.io.copyblob.retry.min.backoff.interval", 3000)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean)>
		 -> specialinvoke $r41.<com.microsoft.azure.storage.RetryExponentialRetry: void <init>(int,int,int,int)>(i3, i5, i4, i6)
The sink interfaceinvoke $r12.<java.util.concurrent.ScheduledExecutorService: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r11, 0L, 1000L, $r13) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r6.<org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>($r8, i0)
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> r0.<org.apache.hadoop.yarn.sls.web.SLSWebApp: int port> = i0
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: org.apache.hadoop.yarn.sls.web.SLSWebApp web> = $r6
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke $r12.<java.util.concurrent.ScheduledExecutorService: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r11, 0L, 1000L, $r13)
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke $r12.<java.util.concurrent.ScheduledExecutorService: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r11, 0L, 1000L, $r13)
The sink $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3) in method <org.apache.hadoop.streaming.JarBuilder: void addNamedStream(java.util.jar.JarOutputStream,java.lang.String,java.io.InputStream)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>(r23, r11, r10)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> $r6 = virtualinvoke r1.<java.io.File: java.lang.String getName()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> $r7 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> virtualinvoke r9.<org.apache.hadoop.streaming.JarBuilder: void addNamedStream(java.util.jar.JarOutputStream,java.lang.String,java.io.InputStream)>(r10, r8, r2)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addNamedStream(java.util.jar.JarOutputStream,java.lang.String,java.io.InputStream)>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
The sink specialinvoke $r5.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r6) in method <org.apache.hadoop.hdfs.server.namenode.ImageWriter$Options: void setConf(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("dfs.namenode.name.dir", r4) in method <org.apache.hadoop.hdfs.server.namenode.ImageWriter$Options: void setConf(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.ImageWriter$Options: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("dfs.namenode.name.dir", r4)
	 -> <org.apache.hadoop.hdfs.server.namenode.ImageWriter$Options: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r5.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r6)
The sink $l5 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: long getLong(java.lang.String)>("mod_time") in method <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":last_updated", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r5 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r5.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_deleted", 1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r0, $r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3> = $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r4 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r5 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r2, $r3, $r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.ItemCollection scan(java.lang.String,java.lang.String,java.util.Map,java.util.Map)>(r1, r2, null, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r10 = (com.amazonaws.services.dynamodbv2.document.ItemCollection) $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> return $r10
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r14 = virtualinvoke r13.<com.amazonaws.services.dynamodbv2.document.ItemCollection: com.amazonaws.services.dynamodbv2.document.internal.IteratorSupport iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r19 = interfaceinvoke r14.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r20 = (com.amazonaws.services.dynamodbv2.document.Item) $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r22 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>(r20, $r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> $l5 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: long getLong(java.lang.String)>("mod_time")
The sink $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r12)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r22 = virtualinvoke $r21.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" tmpDir=")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11)
The sink $r5 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i0) in method <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r5 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i0)
The sink $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(char)>(34)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>()
The sink r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r17 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r17 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
The sink r72 = virtualinvoke r44.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.security.token.Token[] addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)>(r70, r68) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal") in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r70 = virtualinvoke $r69.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.principal")
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r72 = virtualinvoke r44.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.security.token.Token[] addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)>(r70, r68)
The sink r7 = virtualinvoke $r5.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r6) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.input.format", "csv") in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.input.format", "csv")
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke $r5.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r6)
The sink $r7 = virtualinvoke r23.<java.lang.Class: java.lang.String getSimpleName()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.metadatastore.impl", class "Lorg/apache/hadoop/fs/s3a/s3guard/NullMetadataStore;", class "Lorg/apache/hadoop/fs/s3a/s3guard/MetadataStore;") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.metadatastore.impl", class "Lorg/apache/hadoop/fs/s3a/s3guard/NullMetadataStore;", class "Lorg/apache/hadoop/fs/s3a/s3guard/MetadataStore;")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.lang.Class getMetadataStoreClass(org.apache.hadoop.conf.Configuration)>
		 -> return r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r7 = virtualinvoke r23.<java.lang.Class: java.lang.String getSimpleName()>()
The sink $i0 = virtualinvoke r1.<java.lang.String: int hashCode()>() in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> r1 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $i0 = virtualinvoke r1.<java.lang.String: int hashCode()>()
The sink virtualinvoke r4.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>(r18) in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name") in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
	on Path: 
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name")
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> virtualinvoke r4.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>(r18)
The sink specialinvoke $r2.<org.apache.hadoop.fs.FSDataInputStream: void <init>(java.io.InputStream)>($r3) in method <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,int)> was called with values from the following sources:
- $z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.feature.experiment.positional.read.enable", 1) in method <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("adl.feature.experiment.positional.read.enable", 1)
	 -> <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.adl.AdlFsInputStream: boolean enablePositionalReadExperiment> = $z0
	 -> <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,int)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.FSDataInputStream: void <init>(java.io.InputStream)>($r3)
The sink r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r11 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getKey()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r12 = (java.lang.String) $r11
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r13 = virtualinvoke $r10.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withKey(java.lang.String)>($r12)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15)
The sink specialinvoke $r2.<java.text.SimpleDateFormat: void <init>(java.lang.String)>($r3) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-date.format", "yyyy-MM-dd HH:mm:ss,SSS") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-date.format", "yyyy-MM-dd HH:mm:ss,SSS")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r2.<java.text.SimpleDateFormat: void <init>(java.lang.String)>($r3)
The sink r9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r8) in method <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r15 = staticinvoke <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>(r1, r2, null)
	 -> <org.apache.hadoop.streaming.StreamUtil: java.lang.Class goodClassOrNull(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
		 -> r9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r8)
The sink $r18 = virtualinvoke $r17.<java.lang.Thread: java.lang.String getName()>() in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F) in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F)
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: float rateFactor> = $f0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>(r1, "StressJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: org.apache.hadoop.mapred.gridmix.StressJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory: java.lang.Thread createReaderThread()>
		 -> return $r0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread> = $r9
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r17 = r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread>
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r18 = virtualinvoke $r17.<java.lang.Thread: java.lang.String getName()>()
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke $r58.<java.io.IOException: void <init>(java.lang.String)>($r62) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r61 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r62 = virtualinvoke $r61.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r58.<java.io.IOException: void <init>(java.lang.String)>($r62)
The sink $r2 = virtualinvoke $r1.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler getResourceScheduler()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> throw $r30
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> throw $r40
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> specialinvoke r2.<org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)>(r5)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)>
		 -> $r2 = virtualinvoke $r1.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler getResourceScheduler()>()
The sink if $i3 <= 0 goto $r12 = null in method <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])> was called with values from the following sources:
- i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.sleep.fake-locations", 0) in method <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
		 -> i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.sleep.fake-locations", 0)
	 -> <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
		 -> specialinvoke $r3.<org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>(r0, l0, r4, r5, r6, i1, i4, $r7)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> r0.<org.apache.hadoop.mapred.gridmix.SleepJob: int fakeLocations> = i2
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> $i3 = r0.<org.apache.hadoop.mapred.gridmix.SleepJob: int fakeLocations>
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> if $i3 <= 0 goto $r12 = null
The sink if $b2 < 0 goto return 0 in method <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean accept(org.apache.hadoop.fs.FileStatus)> was called with values from the following sources:
- $l0 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.min.file.size", 134217728L) in method <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
		 -> $l0 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.min.file.size", 134217728L)
	 -> <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
		 -> specialinvoke $r8.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: void <init>(long,long)>($l0, $l1)
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: void <init>(long,long)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: long minFileSize> = l0
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: void <init>(long,long)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.FilePool: void refresh()>
		 -> specialinvoke $r3.<org.apache.hadoop.mapred.gridmix.FilePool$InnerDesc: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter)>($r6, $r7, $r8)
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$InnerDesc: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter)>
		 -> $z5 = virtualinvoke r9.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean accept(org.apache.hadoop.fs.FileStatus)>(r33)
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean accept(org.apache.hadoop.fs.FileStatus)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean done()>()
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean done()>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean accept(org.apache.hadoop.fs.FileStatus)>
		 -> $l1 = r0.<org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: long minFileSize>
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean accept(org.apache.hadoop.fs.FileStatus)>
		 -> $b2 = $l0 cmp $l1
	 -> <org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter: boolean accept(org.apache.hadoop.fs.FileStatus)>
		 -> if $b2 < 0 goto return 0
The sink $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f4)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the map output data.")
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r11 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getKey()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r11 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getKey()>()
The sink $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the reduce output data.") in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f2)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the reduce output data.")
The sink $z1 = virtualinvoke $r8.<java.lang.String: boolean equals(java.lang.Object)>(r7) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.input.format", "csv") in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.input.format", "csv")
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke $r5.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r6)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke $r8.<java.lang.String: boolean equals(java.lang.Object)>(r7)
The sink virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setLocalResources(java.util.Map)>(r50) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> return $l0
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r58 = staticinvoke <org.apache.hadoop.yarn.api.records.LocalResource: org.apache.hadoop.yarn.api.records.LocalResource newInstance(org.apache.hadoop.yarn.api.records.URL,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.yarn.api.records.LocalResourceVisibility,long,long)>($r53, $r55, $r56, $l11, $l12)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> interfaceinvoke r50.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r60, r58)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setLocalResources(java.util.Map)>(r50)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l11 = virtualinvoke $r54.<org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getLength(java.util.Map)>
		 -> return $l0
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r58 = staticinvoke <org.apache.hadoop.yarn.api.records.LocalResource: org.apache.hadoop.yarn.api.records.LocalResource newInstance(org.apache.hadoop.yarn.api.records.URL,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.yarn.api.records.LocalResourceVisibility,long,long)>($r53, $r55, $r56, $l11, $l12)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> interfaceinvoke r50.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r60, r58)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setLocalResources(java.util.Map)>(r50)
The sink if $i3 <= i2 goto return in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $i3 = i0 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> if $i3 <= i2 goto return
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.tolerable", 400) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksTolerable(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksTolerable(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.tolerable", 400)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksTolerable(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> if $i3 <= i2 goto return
The sink $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r5 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>($r4)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r2 = (org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy) $r1
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> return $r5
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getJobSubmissionPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r6 = virtualinvoke r2.<org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: java.lang.String name()>()
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r7 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if z0 == 0 goto return in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initMultipartUploads(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multipart.purge", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initMultipartUploads(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initMultipartUploads(org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multipart.purge", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initMultipartUploads(org.apache.hadoop.conf.Configuration)>
		 -> if z0 == 0 goto return
The sink if z0 == 0 goto $r3 = <com.amazonaws.Protocol: com.amazonaws.Protocol HTTP> in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> if z0 == 0 goto $r3 = <com.amazonaws.Protocol: com.amazonaws.Protocol HTTP>
The sink $r43 = virtualinvoke r7.<java.util.HashMap: java.lang.Object get(java.lang.Object)>(r42) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d0 = (double) l21
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d1 = 1.0 * $d0
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> d3 = $d2 / $d1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r47 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r43 = virtualinvoke r7.<java.util.HashMap: java.lang.Object get(java.lang.Object)>(r42)
The sink if i3 <= i0 goto $f1 = (float) i0 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)>
		 -> if i3 <= i0 goto $f1 = (float) i0
The sink if $z2 == 0 goto $r9 = <org.apache.hadoop.fs.s3a.impl.NetworkBinding: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<java.lang.String: boolean contains(java.lang.CharSequence)>("://")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> if $z2 == 0 goto $r9 = <org.apache.hadoop.fs.s3a.impl.NetworkBinding: org.slf4j.Logger LOG>
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>()
The sink virtualinvoke r1.<com.microsoft.azure.storage.blob.BlobRequestOptions: void setStoreBlobContentMD5(java.lang.Boolean)>($r4) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()> was called with values from the following sources:
- $z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.store.blob.md5", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()>
		 -> $z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.store.blob.md5", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()>
		 -> $r4 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()>
		 -> virtualinvoke r1.<com.microsoft.azure.storage.blob.BlobRequestOptions: void setStoreBlobContentMD5(java.lang.Boolean)>($r4)
The sink $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(",") in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()> was called with values from the following sources:
- r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "") in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r103)
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(",")
The sink if $z1 == 0 goto virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> specialinvoke $r8.<java.io.File: void <init>(java.lang.String)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $z1 = virtualinvoke $r8.<java.io.File: boolean isDirectory()>()
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> if $z1 == 0 goto virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
The sink $r16 = staticinvoke <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>($r14, $r15) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("nn_uri")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r14 = staticinvoke <java.net.URI: java.net.URI create(java.lang.String)>(r6)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r16 = staticinvoke <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>($r14, $r15)
The sink if i0 < 0 goto $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>() in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.ttl", 60000) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.ttl", 60000)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> if i0 < 0 goto $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
The sink if $r6 != class "Lorg/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler;" goto $r7 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()> was called with values from the following sources:
- r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class") in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class")
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r6 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> if $r6 != class "Lorg/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler;" goto $r7 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z5) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z5 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z5)
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z4 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z4)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" Persist:")
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z5)
The sink specialinvoke $r1.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void configureJob(org.apache.hadoop.mapreduce.Job)> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.output-path") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void configureJob(org.apache.hadoop.mapreduce.Job)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void configureJob(org.apache.hadoop.mapreduce.Job)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.output-path")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void configureJob(org.apache.hadoop.mapreduce.Job)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3)
The sink if i0 <= 0 goto return in method <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.distcache.file.count", -1) in method <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.distcache.file.count", -1)
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void writeDistCacheData(org.apache.hadoop.conf.Configuration)>
		 -> if i0 <= 0 goto return
The sink $r7 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r26, r1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)> was called with values from the following sources:
- r26 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.s3.client.factory.impl", $r6, class "Lorg/apache/hadoop/fs/s3a/S3ClientFactory;") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> r26 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.s3.client.factory.impl", $r6, class "Lorg/apache/hadoop/fs/s3a/S3ClientFactory;")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> $r7 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r26, r1)
The sink interfaceinvoke $r7.<java.util.List: boolean add(java.lang.Object)>(r6) in method <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("distcp.exclude-file-regex", "") in method <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("distcp.exclude-file-regex", "")
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.RegexpInConfigurationFilter: java.lang.String excludeFileRegex> = $r3
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = r0.<org.apache.hadoop.tools.RegexpInConfigurationFilter: java.lang.String excludeFileRegex>
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r6 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>($r5)
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke $r7.<java.util.List: boolean add(java.lang.Object)>(r6)
The sink if $z0 == 0 goto return in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)> was called with values from the following sources:
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: int run(java.lang.String[],java.io.PrintStream)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>(r4, r53)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r1 = interfaceinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: java.util.Map getDiagnostics()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> $r8 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("region", $r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> $r3 = interfaceinvoke r1.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r4 = interfaceinvoke $r3.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> $z0 = interfaceinvoke r4.<java.util.Iterator: boolean hasNext()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> if $z0 == 0 goto return
The sink if null != $r12 goto virtualinvoke r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void resetUGInfo()>() in method <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hdfs.image.writer.ugi.single.user") in method <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hdfs.image.writer.ugi.single.user")
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String user> = $r2
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String user>
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String group> = $r6
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String group>
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> if null != $r12 goto virtualinvoke r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void resetUGInfo()>()
- $r11 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hdfs.image.writer.ugi.single.group") in method <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hdfs.image.writer.ugi.single.group")
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String group> = $r11
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: java.lang.String group>
	 -> <org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> if null != $r12 goto virtualinvoke r0.<org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver: void resetUGInfo()>()
The sink $c3 = staticinvoke <java.lang.Character: char toUpperCase(char)>(c1) in method <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $c1 = virtualinvoke r1.<java.lang.String: char charAt(int)>(i2)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $r2 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>($c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> $c3 = staticinvoke <java.lang.Character: char toUpperCase(char)>(c1)
The sink $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)> was called with values from the following sources:
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
The sink if $z0 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("partitioned")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto (branch)
The sink if $z3 == 0 goto $r23 = new org.apache.hadoop.fs.Path in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> $z3 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> if $z3 == 0 goto $r23 = new org.apache.hadoop.fs.Path
The sink $r27 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r9, $r8, r2, $r6)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r13, $r12, r2, $r10)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r17, $r16, r2, $r14)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r21, $r20, r2, $r18)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r24, $r23, r2, $r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r27 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z2, "STS signing region set set to %s but no STS endpoint specified", r7) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z2, "STS signing region set set to %s but no STS endpoint specified", r7)
- r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z2, "STS signing region set set to %s but no STS endpoint specified", r7)
The sink specialinvoke $r62.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r2) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r62.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r2)
The sink virtualinvoke $r16.<java.util.concurrent.ThreadPoolExecutor: java.util.concurrent.Future submit(java.util.concurrent.Callable)>($r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)> was called with values from the following sources:
- r6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.sql", null) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)>
		 -> r6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.sql", null)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)>
		 -> $r17 = staticinvoke <org.apache.hadoop.fs.s3a.S3AFileSystem$lambda_openFileWithOptions_25__38: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.concurrent.CompletableFuture,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,java.util.Optional)>(r0, r15, r2, r6, r4, r14)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem$lambda_openFileWithOptions_25__38: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.concurrent.CompletableFuture,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,java.util.Optional)>
		 -> specialinvoke $r6.<org.apache.hadoop.fs.s3a.S3AFileSystem$lambda_openFileWithOptions_25__38: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.concurrent.CompletableFuture,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,java.util.Optional)>($r0, $r1, $r2, $r3, $r4, $r5)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem$lambda_openFileWithOptions_25__38: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.concurrent.CompletableFuture,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,java.util.Optional)>
		 -> $r0.<org.apache.hadoop.fs.s3a.S3AFileSystem$lambda_openFileWithOptions_25__38: java.lang.String cap3> = $r4
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem$lambda_openFileWithOptions_25__38: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.concurrent.CompletableFuture,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,java.util.Optional)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem$lambda_openFileWithOptions_25__38: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.concurrent.CompletableFuture,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,java.util.Optional)>
		 -> return $r6
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)>
		 -> virtualinvoke $r16.<java.util.concurrent.ThreadPoolExecutor: java.util.concurrent.Future submit(java.util.concurrent.Callable)>($r17)
The sink $c12 = virtualinvoke r0.<java.lang.String: char charAt(int)>($i11) in method <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r5 = r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys>
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r6 = interfaceinvoke $r5.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r7 = interfaceinvoke $r6.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r8 = interfaceinvoke r7.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r10 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r11 = (java.lang.String) $r10
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> virtualinvoke r1.<org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator: java.lang.String validate(java.lang.String)>($r11)
	 -> <org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator: java.lang.String validate(java.lang.String)>
		 -> $z0 = staticinvoke <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.azurebfs.utils.Base64: boolean validateIsBase64String(java.lang.String)>
		 -> $c12 = virtualinvoke r0.<java.lang.String: char charAt(int)>($i11)
The sink if $z2 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("normal")
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> if $z2 == 0 goto (branch)
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("normal")
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> if $z2 == 0 goto (branch)
The sink $r57 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l8) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r57 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l8)
The sink interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>(r12) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l15 = (long) i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r12 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequest: org.apache.hadoop.yarn.api.records.ReservationRequest newInstance(org.apache.hadoop.yarn.api.records.Resource,int,int,long)>(r2, $i16, 1, $l15)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>(r12)
The sink virtualinvoke r5.<java.io.File: void deleteOnExit()>() in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> virtualinvoke r5.<java.io.File: void deleteOnExit()>()
The sink if $z2 == 0 goto $r12 = "will not" in method <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.saskey.usecontainersaskeyforallaccess", 1) in method <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.saskey.usecontainersaskeyforallaccess", 1)
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: boolean useContainerSasKeyForAllAccess> = $z1
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = r0.<org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: boolean useContainerSasKeyForAllAccess>
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> if $z2 == 0 goto $r12 = "will not"
The sink $i18 = virtualinvoke r52.<java.util.ArrayList: int size()>() in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d0 = (double) l21
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d1 = 1.0 * $d0
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> d3 = $d2 / $d1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r47 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r26 = virtualinvoke r7.<java.util.HashMap: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> specialinvoke $r25.<java.util.ArrayList: void <init>(java.util.Collection)>($r26)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> r52 = $r25
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $i18 = virtualinvoke r52.<java.util.ArrayList: int size()>()
The sink $i0 = virtualinvoke r4.<java.lang.String: int hashCode()>() in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r4.<java.lang.String: int hashCode()>()
The sink r1 = virtualinvoke r0.<java.lang.String: java.lang.String substring(int)>($i3) in method <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r11 = virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>(r1)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> i0 = virtualinvoke r0.<java.lang.String: int lastIndexOf(int)>(47)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> $i3 = i0 + 1
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> r1 = virtualinvoke r0.<java.lang.String: java.lang.String substring(int)>($i3)
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanup(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.meta.folder") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanup(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanup(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.meta.folder")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanup(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2)
The sink specialinvoke r2.<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>(r0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.copy.overwrite", 0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.copy.overwrite", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> r2.<org.apache.hadoop.tools.mapred.CopyCommitter: boolean overwrite> = $z1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> specialinvoke r2.<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>(r0)
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.target.path.exists", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.target.path.exists", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> r2.<org.apache.hadoop.tools.mapred.CopyCommitter: boolean targetPathExists> = $z2
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> specialinvoke r2.<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>(r0)
- $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.sync.folders", 0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.sync.folders", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> r2.<org.apache.hadoop.tools.mapred.CopyCommitter: boolean syncFolder> = $z0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> specialinvoke r2.<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>(r0)
The sink $z0 = interfaceinvoke r26.<java.util.Iterator: boolean hasNext()>() in method <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r26 = interfaceinvoke r4.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $z0 = interfaceinvoke r26.<java.util.Iterator: boolean hasNext()>()
The sink $r49 = virtualinvoke $r48.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\' is being simulated as \'") in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()> was called with values from the following sources:
- r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id") in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r44)
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r49 = virtualinvoke $r48.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\' is being simulated as \'")
The sink virtualinvoke $r33.<org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void setDaemon(boolean)>(1) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L) in method <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: long sleepTime> = $l0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher> = $r44
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r33 = r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher>
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r33.<org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void setDaemon(boolean)>(1)
The sink $z7 = virtualinvoke r6.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> $z7 = virtualinvoke r6.<java.lang.String: boolean isEmpty()>()
The sink if $i0 <= 0 goto r1 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> if $i0 <= 0 goto r1 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList()>()
The sink virtualinvoke r11.<com.fasterxml.jackson.core.JsonGenerator: void writeObject(java.lang.Object)>(r13) in method <org.apache.hadoop.tools.rumen.state.StatePool: void write(java.io.DataOutput)> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run(java.lang.String[])>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: int run()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> $r1 = r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> virtualinvoke $r1.<org.apache.hadoop.tools.rumen.state.StatePool: void persist()>()
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void persist()>
		 -> $z1 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean isUpdated()>()
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: boolean isUpdated()>
		 -> return $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void persist()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.state.StatePool: void write(java.io.DataOutput)>(r17)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void write(java.io.DataOutput)>
		 -> virtualinvoke r11.<com.fasterxml.jackson.core.JsonGenerator: void writeObject(java.lang.Object)>(r13)
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run(java.lang.String[])>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: int run()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> $r1 = r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> virtualinvoke $r1.<org.apache.hadoop.tools.rumen.state.StatePool: void persist()>()
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void persist()>
		 -> $z1 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean isUpdated()>()
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: boolean isUpdated()>
		 -> return $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void persist()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.state.StatePool: void write(java.io.DataOutput)>(r17)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void write(java.io.DataOutput)>
		 -> virtualinvoke r11.<com.fasterxml.jackson.core.JsonGenerator: void writeObject(java.lang.Object)>(r13)
The sink virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyPort(int)>(i0) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.proxy.port", -1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.proxy.port", -1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyPort(int)>(i0)
The sink if z20 != 0 goto $z10 = virtualinvoke r5.<org.apache.hadoop.fs.shell.CommandFormat: boolean getOpt(java.lang.String)>("nonauth") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- z20 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> z20 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if z20 != 0 goto $z10 = virtualinvoke r5.<org.apache.hadoop.fs.shell.CommandFormat: boolean getOpt(java.lang.String)>("nonauth")
The sink specialinvoke $r110.<java.net.URI: void <init>(java.lang.String)>(r5) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB> = $i7
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i11 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i11)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[8] = $r39
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30[2] = r5
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r110.<java.net.URI: void <init>(java.lang.String)>(r5)
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir") in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir")
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r8 = virtualinvoke r6.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>() in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()> was called with values from the following sources:
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998) in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> return $i0
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r0.<javax.ws.rs.core.UriBuilder: javax.ws.rs.core.UriBuilder port(int)>($i0)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r2.<javax.ws.rs.core.UriBuilder: java.net.URI build(java.lang.Object[])>($r3)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI baseURI> = $r4
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI baseURI>
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> return $r5
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()>
		 -> r6 = virtualinvoke $r4.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>($r5)
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>
		 -> virtualinvoke $r2.<java.util.ArrayList: boolean add(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>
		 -> $r2 = r0.<org.apache.hadoop.http.HttpServer2$Builder: java.util.ArrayList endpoints>
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>
		 -> return r0
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()>
		 -> $r8 = virtualinvoke r6.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>()
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>
		 -> $r46 = r0.<org.apache.hadoop.http.HttpServer2$Builder: java.util.ArrayList endpoints>
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>
		 -> $r47 = virtualinvoke $r46.<java.util.ArrayList: java.lang.Object get(int)>(0)
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>
		 -> $r48 = (java.net.URI) $r47
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>
		 -> $r49 = virtualinvoke $r48.<java.net.URI: java.lang.String getHost()>()
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>
		 -> r0.<org.apache.hadoop.http.HttpServer2$Builder: java.lang.String hostName> = $r49
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>
		 -> r0 := @this: org.apache.hadoop.http.HttpServer2$Builder
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()>
		 -> r6 = virtualinvoke $r4.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>($r5)
	 -> <org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2$Builder addEndpoint(java.net.URI)>
		 -> return r0
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: void startResourceEstimatorApp()>
		 -> $r8 = virtualinvoke r6.<org.apache.hadoop.http.HttpServer2$Builder: org.apache.hadoop.http.HttpServer2 build()>()
The sink if $z0 == 0 goto $r1 = newarray (java.util.concurrent.CompletableFuture)[0] in method <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>("batch threshold reached")
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $r5 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $z0 = interfaceinvoke r0.<java.util.List: boolean isEmpty()>()
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> if $z0 == 0 goto $r1 = newarray (java.util.concurrent.CompletableFuture)[0]
The sink $r6 = staticinvoke <java.util.Arrays: java.lang.Object[] copyOf(java.lang.Object[],int)>(r5, $i1) in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script") in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script")
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String[] split(java.lang.String)>(" ")
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $i0 = lengthof r5
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $i1 = $i0 + 1
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r6 = staticinvoke <java.util.Arrays: java.lang.Object[] copyOf(java.lang.Object[],int)>(r5, $i1)
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)> was called with values from the following sources:
- f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F) in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F)
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = f0 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>(r2, $f2)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is invalid")
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
The sink virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyDomain(java.lang.String)>($r8) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.proxy.domain") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.proxy.domain")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyDomain(java.lang.String)>($r8)
The sink if i0 <= 0 goto return null in method <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.committer.threads", 8) in method <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
		 -> i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.committer.threads", 8)
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
		 -> if i0 <= 0 goto return null
The sink $i0 = staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r8, r3, r5) in method <org.apache.hadoop.tools.dynamometer.Client: void main(java.lang.String[])> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $r65 = virtualinvoke r2.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> r0 := @this: org.apache.hadoop.conf.Configured
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $r64 = virtualinvoke r2.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> r0 := @this: org.apache.hadoop.conf.Configured
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $r50 = virtualinvoke r2.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> r0 := @this: org.apache.hadoop.conf.Configured
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $r48 = virtualinvoke r2.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> r0 := @this: org.apache.hadoop.conf.Configured
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $r9 = virtualinvoke r2.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> r0 := @this: org.apache.hadoop.conf.Configured
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2 := @this: org.apache.hadoop.tools.dynamometer.Client
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> r1 := @this: org.apache.hadoop.tools.dynamometer.Client
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.conf.Configured
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> r7 := @parameter0: org.apache.hadoop.conf.Configuration
	 -> <org.apache.hadoop.tools.dynamometer.Client: void main(java.lang.String[])>
		 -> $i0 = staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r8, r3, r5)
The sink $r8 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l1) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.consistency.retry.interval", "2s", $r3) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.consistency.retry.interval", "2s", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> $r8 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l1)
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSS3IOException;", $r45) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>($r11)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r9.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>($r10, null)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls> = $r9
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r45 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSS3IOException;", $r45)
The sink $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>(r1, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.CopyFilter copyFilter> = $r9
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l3 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l3)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z4 != 0 goto $r10 = new org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)> was called with values from the following sources:
- $z1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.secure.mode", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> $z1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.secure.mode", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> r1.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean useSecureMode> = $z1
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> $z4 = r1.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean useSecureMode>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> if $z4 != 0 goto $r10 = new org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl
The sink $r2 = virtualinvoke $r0.<javax.ws.rs.core.UriBuilder: javax.ws.rs.core.UriBuilder port(int)>($i0) in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998) in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> return $i0
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r0.<javax.ws.rs.core.UriBuilder: javax.ws.rs.core.UriBuilder port(int)>($i0)
The sink virtualinvoke r9.<com.codahale.metrics.CsvReporter: void start(long,java.util.concurrent.TimeUnit)>($l1, $r26) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void initMetricsCSVOutput()> was called with values from the following sources:
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.record.interval.ms", 1000) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void initMetricsCSVOutput()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void initMetricsCSVOutput()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.record.interval.ms", 1000)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void initMetricsCSVOutput()>
		 -> $l1 = (long) i0
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void initMetricsCSVOutput()>
		 -> virtualinvoke r9.<com.codahale.metrics.CsvReporter: void start(long,java.util.concurrent.TimeUnit)>($l1, $r26)
The sink if r2 != null goto $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r2, r0) in method <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.adl.oauth2.access.token.provider", null, class "Lorg/apache/hadoop/fs/adl/oauth2/AzureADTokenProvider;") in method <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.adl.oauth2.access.token.provider", null, class "Lorg/apache/hadoop/fs/adl/oauth2/AzureADTokenProvider;")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: org.apache.hadoop.fs.adl.oauth2.AzureADTokenProvider getCustomAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> if r2 != null goto $r4 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r2, r0)
The sink virtualinvoke r1.<org.apache.hadoop.conf.Configuration: void setInt(java.lang.String,int)>("gridmix.datagenerator.randomtext.wordsize", i0) in method <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r7 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r8 = interfaceinvoke $r6.<java.util.Map: java.lang.Object get(java.lang.Object)>($r7)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r9 = (java.lang.Integer) $r8
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $i2 = virtualinvoke $r9.<java.lang.Integer: int intValue()>()
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> return $i2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)>(r0, i0)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)>
		 -> virtualinvoke r1.<org.apache.hadoop.conf.Configuration: void setInt(java.lang.String,int)>("gridmix.datagenerator.randomtext.wordsize", i0)
The sink $r8 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>(r7) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>(r7)
The sink specialinvoke $r4.<java.lang.RuntimeException: void <init>(java.lang.String,java.lang.Throwable)>($r7, r13) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r9 = r6[0]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>($r9, $r8, $r7)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r5[1] = r0
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r7 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("Signer class [%s] not found for signer [%s]", $r5)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r4.<java.lang.RuntimeException: void <init>(java.lang.String,java.lang.Throwable)>($r7, r13)
The sink $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicTaskAttemptsPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink $z3 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> $z3 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("")
The sink if $i0 <= 0 goto return r4 in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.s3a.authoritative.path", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.s3a.authoritative.path", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $i0 = lengthof r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> if $i0 <= 0 goto return r4
The sink $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean enableMultiObjectsDelete> = $z2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
- $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean failOnMetadataWriteError> = $z1
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
- $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean errorsIncludeSql> = $z1
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: boolean magicCommitEnabled> = z0
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.SignerManager: org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider delegationTokenProvider> = r3
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.auth.SignerManager signerManager> = $r37
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
- l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r61.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>(l13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = $r61
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
- $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean enabled> = $z0
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>(r1, r2, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r3, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r1, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: boolean requireVersion> = z0
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy changeDetectionPolicy> = $r45
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r3 = (org.apache.hadoop.fs.s3a.WriteOperationHelper) $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.WriteOperationHelper operations> = $r3
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.auth.SignerManager
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.Listing
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.WriteOperationHelper: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper> = $r32
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r0, $r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.FileSystem: java.lang.String getScheme()>()
The sink $r39 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i11) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB> = $i7
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i11 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i11)
The sink $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
The sink $z12 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("magic") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z12 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("magic")
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.output.fileoutputformat.compress.type", r3) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.type") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.type")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.output.fileoutputformat.compress.type", r3)
The sink virtualinvoke $r3.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>(r0, r1, r2) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem: void registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $i3 = i0 * 1000
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l4 = (long) $i3
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r47.<org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: void <init>(long)>($l4)
	 -> <org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: long windowSizeMs> = l0
	 -> <org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: org.apache.hadoop.fs.azure.metrics.RollingWindowAverage currentBlockDownloadLatency> = $r47
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation instrumentation> = $r4
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation instrumentation>
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem: void registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>($r40, r47, $r39)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem: void registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>
		 -> virtualinvoke $r3.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>(r0, r1, r2)
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>
		 -> r1 = staticinvoke <org.apache.hadoop.metrics2.lib.MetricsAnnotations: org.apache.hadoop.metrics2.lib.MetricsSourceBuilder newSourceBuilder(java.lang.Object)>(r0)
	 -> <org.apache.hadoop.metrics2.lib.MetricsAnnotations: org.apache.hadoop.metrics2.lib.MetricsSourceBuilder newSourceBuilder(java.lang.Object)>
		 -> specialinvoke $r0.<org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: void <init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)>(r1, $r2)
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: void <init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object,java.lang.Object)>(r1, "source")
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: void <init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)>
		 -> r0.<org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: java.lang.Object source> = $r2
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: void <init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)>
		 -> $r7 = specialinvoke r0.<org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: org.apache.hadoop.metrics2.lib.MetricsRegistry initRegistry(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: org.apache.hadoop.metrics2.lib.MetricsRegistry initRegistry(java.lang.Object)>
		 -> return r24
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: void <init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)>
		 -> return
	 -> <org.apache.hadoop.metrics2.lib.MetricsAnnotations: org.apache.hadoop.metrics2.lib.MetricsSourceBuilder newSourceBuilder(java.lang.Object)>
		 -> return $r0
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: org.apache.hadoop.metrics2.MetricsSource build()>()
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: org.apache.hadoop.metrics2.MetricsSource build()>
		 -> $r4 = r0.<org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: java.lang.Object source>
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: org.apache.hadoop.metrics2.MetricsSource build()>
		 -> $r5 = (org.apache.hadoop.metrics2.MetricsSource) $r4
	 -> <org.apache.hadoop.metrics2.lib.MetricsSourceBuilder: org.apache.hadoop.metrics2.MetricsSource build()>
		 -> return $r5
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>
		 -> specialinvoke $r18.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1: void <init>(org.apache.hadoop.metrics2.impl.MetricsSystemImpl,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>(r8, r9, r7, r2)
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1: void <init>(org.apache.hadoop.metrics2.impl.MetricsSystemImpl,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>
		 -> r0.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1: org.apache.hadoop.metrics2.MetricsSource val$s> = r4
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1: void <init>(org.apache.hadoop.metrics2.impl.MetricsSystemImpl,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>
		 -> specialinvoke r0.<org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback: void <init>()>()
	 -> <org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback: void <init>()>
		 -> return
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1: void <init>(org.apache.hadoop.metrics2.impl.MetricsSystemImpl,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>
		 -> return
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>
		 -> specialinvoke r8.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)>(r9, $r18)
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> $r4 = specialinvoke r0.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)>(r2)
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> $r1 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> $r2 = virtualinvoke $r1.<java.lang.Class: java.lang.ClassLoader getClassLoader()>()
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> $r6 = staticinvoke <java.lang.reflect.Proxy: java.lang.Object newProxyInstance(java.lang.ClassLoader,java.lang.Class[],java.lang.reflect.InvocationHandler)>($r2, $r3, $r4)
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> return $r6
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> $r5 = (org.apache.hadoop.metrics2.MetricsSystem$Callback) $r4
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> interfaceinvoke $r3.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(r1, $r5)
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> $r3 = r0.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.util.Map namedCallbacks>
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)>
		 -> r0 := @this: org.apache.hadoop.metrics2.impl.MetricsSystemImpl
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>
		 -> specialinvoke $r18.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1: void <init>(org.apache.hadoop.metrics2.impl.MetricsSystemImpl,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>(r8, r9, r7, r2)
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1: void <init>(org.apache.hadoop.metrics2.impl.MetricsSystemImpl,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>
		 -> r0 := @this: org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1
	 -> <org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>
		 -> r8 := @this: org.apache.hadoop.metrics2.impl.MetricsSystemImpl
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem: void registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>
		 -> virtualinvoke $r3.<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>(r0, r1, r2)
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyUsername(java.lang.String)>(r34) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r34 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.username") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r34 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.username")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyUsername(java.lang.String)>(r34)
The sink $r22 = virtualinvoke $r21.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" tmpDir=") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r12)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r22 = virtualinvoke $r21.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" tmpDir=")
The sink $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f2) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f2)
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setMaxErrorRetry(int)>($i1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $i1 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.attempts.maximum", 10) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $i1 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.attempts.maximum", 10)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setMaxErrorRetry(int)>($i1)
The sink $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27)
The sink if $b2 > 0 goto l8 = l0 in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> $b2 = l0 cmp 0L
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $b2 > 0 goto l8 = l0
The sink interfaceinvoke $r17.<java.util.Queue: boolean add(java.lang.Object)>(r11) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)> was called with values from the following sources:
- $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> = $l1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>(0L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l36 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize>
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l26 = l42 + l36
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l43 = $l26 - 1L
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>(l42, l43)
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: long byteEnd> = l0
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> return
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> r11 = $r10
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $r12 = virtualinvoke r11.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] getBuffer()>()
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] getBuffer()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> virtualinvoke r11.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void setStatus(org.apache.hadoop.fs.aliyun.oss.ReadBuffer$STATUS)>($r18)
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void setStatus(org.apache.hadoop.fs.aliyun.oss.ReadBuffer$STATUS)>
		 -> return
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> interfaceinvoke $r17.<java.util.Queue: boolean add(java.lang.Object)>(r11)
The sink specialinvoke $r0.<java.io.FileInputStream: void <init>(java.io.File)>(r1) in method <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>(r23, r11, r10)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> specialinvoke $r0.<java.io.FileInputStream: void <init>(java.io.File)>(r1)
The sink specialinvoke $r67.<java.util.Date: void <init>(long)>($l2) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r62 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> r14 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0, r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r4 = virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("getVersionMarkerItem", "../VERSION", 1, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r1 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r3 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r2 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.Item getItem(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r5 = (com.amazonaws.services.dynamodbv2.document.Item) $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> return r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r63 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: java.lang.Long extractCreationTimeFromMarker(com.amazonaws.services.dynamodbv2.document.Item)>(r62)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: java.lang.Long extractCreationTimeFromMarker(com.amazonaws.services.dynamodbv2.document.Item)>
		 -> $l0 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: long getLong(java.lang.String)>("table_created")
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: java.lang.Long extractCreationTimeFromMarker(com.amazonaws.services.dynamodbv2.document.Item)>
		 -> $r1 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>($l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: java.lang.Long extractCreationTimeFromMarker(com.amazonaws.services.dynamodbv2.document.Item)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $l2 = virtualinvoke r63.<java.lang.Long: long longValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke $r67.<java.util.Date: void <init>(long)>($l2)
The sink if null != r44 goto $d4 = virtualinvoke r44.<java.lang.Double: double doubleValue()>() in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d0 = (double) l21
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d1 = 1.0 * $d0
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> d3 = $d2 / $d1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r47 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r43 = virtualinvoke r7.<java.util.HashMap: java.lang.Object get(java.lang.Object)>(r42)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> r44 = (java.lang.Double) $r43
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> if null != r44 goto $d4 = virtualinvoke r44.<java.lang.Double: double doubleValue()>()
The sink virtualinvoke $r5.<java.text.DateFormat: void setTimeZone(java.util.TimeZone)>($r6) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-date.time-zone", "UTC") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-date.time-zone", "UTC")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = staticinvoke <java.util.TimeZone: java.util.TimeZone getTimeZone(java.lang.String)>(r4)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r5.<java.text.DateFormat: void setTimeZone(java.util.TimeZone)>($r6)
The sink virtualinvoke r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: void setConf(org.apache.hadoop.conf.Configuration)>(r0) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $i1 = i0 * 1000
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l2 = (long) $i1
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r46.<org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: void <init>(long)>($l2)
	 -> <org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: long windowSizeMs> = l0
	 -> <org.apache.hadoop.fs.azure.metrics.RollingWindowAverage: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: org.apache.hadoop.fs.azure.metrics.RollingWindowAverage currentBlockUploadLatency> = $r46
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation instrumentation> = $r4
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: void setConf(org.apache.hadoop.conf.Configuration)>(r0)
The sink if $b2 > 0 goto return in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)> was called with values from the following sources:
- l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15) in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15)
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l7 = l4
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l8 = $l7
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>(l8, "SASKEY")
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes> = l0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> $l1 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> $b2 = $l1 cmp 0L
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> if $b2 > 0 goto return
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2) in method <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2)
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod> = $l0
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>()
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> $l0 = r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod>
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> return $l0
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>($l0, "SASKEY")
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes> = l0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> $l1 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> $b2 = $l1 cmp 0L
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> if $b2 > 0 goto return
- $l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.authorization.cacheentry.expiry.period", 5L, $r13) in method <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.authorization.cacheentry.expiry.period", 5L, $r13)
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>($l1, "AUTHORIZATION")
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes> = l0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> $l1 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> $b2 = $l1 cmp 0L
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> if $b2 > 0 goto return
The sink if $z1 != 0 goto $r8 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.S3AFileSystem owner> in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> if $z1 != 0 goto $r8 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.S3AFileSystem owner>
The sink if r1 == null goto $z0 = 0 in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: boolean hasServiceURI(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("test.fs.swift.name") in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: boolean hasServiceURI(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: boolean hasServiceURI(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("test.fs.swift.name")
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: boolean hasServiceURI(org.apache.hadoop.conf.Configuration)>
		 -> if r1 == null goto $z0 = 0
The sink $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("magic") in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("magic")
The sink $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z4) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z4 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z4)
The sink $z0 = virtualinvoke r7.<java.lang.String: boolean contains(java.lang.CharSequence)>("(?<timestamp>") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r7.<java.lang.String: boolean contains(java.lang.CharSequence)>("(?<timestamp>")
The sink specialinvoke $r21.<java.io.FileOutputStream: void <init>(java.lang.String)>($r26) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r23 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/jobruntime.csv")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r21.<java.io.FileOutputStream: void <init>(java.lang.String)>($r26)
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", $z1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", 0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", 0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", $z1)
The sink if $i0 > 0 goto $r7 = staticinvoke <org.apache.hadoop.io.retry.RetryUtils: org.apache.hadoop.io.retry.RetryPolicy getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)>(r2, "fs.azure.authorizer.http.retry.policy.enabled", 1, "fs.azure.authorizer.http.retry.policy.spec", "10,3,100,2") in method <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.authorization.remote.service.urls") in method <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.authorization.remote.service.urls")
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: java.lang.String[] commaSeparatedUrls> = $r3
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = r1.<org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: java.lang.String[] commaSeparatedUrls>
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = lengthof $r6
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> if $i0 > 0 goto $r7 = staticinvoke <org.apache.hadoop.io.retry.RetryUtils: org.apache.hadoop.io.retry.RetryPolicy getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)>(r2, "fs.azure.authorizer.http.retry.policy.enabled", 1, "fs.azure.authorizer.http.retry.policy.spec", "10,3,100,2")
The sink interfaceinvoke $r16.<java.util.concurrent.ExecutorService: void execute(java.lang.Runnable)>($r13) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)> was called with values from the following sources:
- $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> = $l1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>(0L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l36 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize>
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l24 = l36 * $l23
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l42 = $l25 + $l24
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>(l42, l43)
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: long byteStart> = l1
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: void <init>(long,long)>
		 -> return
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> r11 = $r10
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $r12 = virtualinvoke r11.<org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] getBuffer()>()
	 -> <org.apache.hadoop.fs.aliyun.oss.ReadBuffer: byte[] getBuffer()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> specialinvoke $r13.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileReaderTask: void <init>(java.lang.String,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,org.apache.hadoop.fs.aliyun.oss.ReadBuffer)>($r15, $r14, r11)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileReaderTask: void <init>(java.lang.String,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,org.apache.hadoop.fs.aliyun.oss.ReadBuffer)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileReaderTask: org.apache.hadoop.fs.aliyun.oss.ReadBuffer readBuffer> = r3
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileReaderTask: void <init>(java.lang.String,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,org.apache.hadoop.fs.aliyun.oss.ReadBuffer)>
		 -> return
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> interfaceinvoke $r16.<java.util.concurrent.ExecutorService: void execute(java.lang.Runnable)>($r13)
The sink if r19 == null goto $r20 = <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.sls.scheduler.TaskRunner runner> in method <org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r8 = r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner$1: org.apache.hadoop.yarn.sls.SLSRunner val$se> = r2
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm> = $r24
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> specialinvoke $r24.<org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>(r1, r8)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner$1: void <init>(org.apache.hadoop.yarn.sls.SLSRunner,org.apache.hadoop.yarn.sls.SLSRunner)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> throw $r40
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> specialinvoke r2.<org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)>(r5)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> $r21 = r2.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> $l5 = virtualinvoke $r21.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: long getStartTime()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> r52 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationId: org.apache.hadoop.yarn.api.records.ReservationId newInstance(long,long)>($l5, $l7)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> specialinvoke r2.<org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)>($r18, r4, r5, r7, l19, l20, r9, r52, $l4, $r19, null, $r20)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)>
		 -> if r19 == null goto $r20 = <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.sls.scheduler.TaskRunner runner>
The sink interfaceinvoke r8.<java.util.List: void forEach(java.util.function.Consumer)>($r9) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3DirContentToMs(org.apache.hadoop.fs.s3a.S3AFileStatus,java.util.List)> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r24.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>(r12, r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> $r3 = (org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore) r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore metadataStore> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r41 = $r24
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r42 = virtualinvoke r41.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3ToMs(org.apache.hadoop.fs.Path)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3ToMs(org.apache.hadoop.fs.Path)>
		 -> specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void compareAuthoritativeDirectoryFlag(java.util.List,org.apache.hadoop.fs.Path,java.util.List)>(r11, r53, r54)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: void compareAuthoritativeDirectoryFlag(java.util.List,org.apache.hadoop.fs.Path,java.util.List)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3ToMs(org.apache.hadoop.fs.Path)>
		 -> r45 = virtualinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3DirContentToMs(org.apache.hadoop.fs.s3a.S3AFileStatus,java.util.List)>(r52, r44)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3DirContentToMs(org.apache.hadoop.fs.s3a.S3AFileStatus,java.util.List)>
		 -> r7 = virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$ComparePair compareFileStatusToPathMetadata(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>(r0, r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$ComparePair compareFileStatusToPathMetadata(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>
		 -> $r10 = specialinvoke r9.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: org.apache.hadoop.fs.Path path(java.lang.String)>("/")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: org.apache.hadoop.fs.Path path(java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$ComparePair compareFileStatusToPathMetadata(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>
		 -> return r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3DirContentToMs(org.apache.hadoop.fs.s3a.S3AFileStatus,java.util.List)>
		 -> $r9 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$lambda_compareS3DirContentToMs_5__134: java.util.function.Consumer bootstrap$(org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck,java.util.List)>(r2, r6)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$lambda_compareS3DirContentToMs_5__134: java.util.function.Consumer bootstrap$(org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck,java.util.List)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$lambda_compareS3DirContentToMs_5__134: void <init>(org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck,java.util.List)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$lambda_compareS3DirContentToMs_5__134: void <init>(org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck,java.util.List)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$lambda_compareS3DirContentToMs_5__134: org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$lambda_compareS3DirContentToMs_5__134: void <init>(org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck,java.util.List)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck$lambda_compareS3DirContentToMs_5__134: java.util.function.Consumer bootstrap$(org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck,java.util.List)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardFsck: java.util.List compareS3DirContentToMs(org.apache.hadoop.fs.s3a.S3AFileStatus,java.util.List)>
		 -> interfaceinvoke r8.<java.util.List: void forEach(java.util.function.Consumer)>($r9)
The sink if $z8 == 0 goto specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path)>(r43, r30, r14) in method <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $z8 = r4.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> if $z8 == 0 goto specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path)>(r43, r30, r14)
The sink if $z2 == 0 goto r39 = staticinvoke <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>(r15, r5) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r38 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.endpoint", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r38)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if $z2 == 0 goto r39 = staticinvoke <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>(r15, r5)
The sink if i0 <= 40 goto (branch) in method <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> if i0 <= 40 goto (branch)
The sink $r15 = virtualinvoke $r14.<com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder: com.amazonaws.client.builder.AwsClientBuilder withRegion(java.lang.String)>(r7) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: com.amazonaws.services.dynamodbv2.AmazonDynamoDB createDynamoDBClient(java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)> was called with values from the following sources:
- r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> return r15
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: com.amazonaws.services.dynamodbv2.AmazonDynamoDB createDynamoDBClient(java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r15 = virtualinvoke $r14.<com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder: com.amazonaws.client.builder.AwsClientBuilder withRegion(java.lang.String)>(r7)
The sink interfaceinvoke r2.<com.amazonaws.services.s3.AmazonS3: void setEndpoint(java.lang.String)>(r1) in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "") in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r2.<com.amazonaws.services.s3.AmazonS3: void setEndpoint(java.lang.String)>(r1)
The sink $i0 = virtualinvoke r2.<java.lang.String: int indexOf(java.lang.String)>("LineRecordReader") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $i0 = virtualinvoke r2.<java.lang.String: int indexOf(java.lang.String)>("LineRecordReader")
The sink $l19 = staticinvoke <java.lang.Math: long min(long,long)>(2147483647L, $l18) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l9 = (long) $i8
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords> = $l11
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l13 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> l3 = $l12 / $l13
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l18 = l3 + 1L
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l19 = staticinvoke <java.lang.Math: long min(long,long)>(2147483647L, $l18)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f3 = $f2 / f4
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l17 = (long) $f3
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r40.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l17, $l8, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l12 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> l3 = $l12 / $l13
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l18 = l3 + 1L
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l19 = staticinvoke <java.lang.Math: long min(long,long)>(2147483647L, $l18)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f1 = $f0 / f5
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l18 = (long) $f1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r43.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l18, $l2, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l12 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> l3 = $l12 / $l13
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l18 = l3 + 1L
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l19 = staticinvoke <java.lang.Math: long min(long,long)>(2147483647L, $l18)
The sink virtualinvoke $r2.<java.util.concurrent.atomic.AtomicLong: void set(long)>($l1) in method <org.apache.hadoop.hdfs.server.namenode.FixedBlockResolver: void setConf(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("hdfs.image.writer.resolver.fixed.block.start", 1073741824L) in method <org.apache.hadoop.hdfs.server.namenode.FixedBlockResolver: void setConf(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FixedBlockResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("hdfs.image.writer.resolver.fixed.block.start", 1073741824L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FixedBlockResolver: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r2.<java.util.concurrent.atomic.AtomicLong: void set(long)>($l1)
The sink $z2 = virtualinvoke $r12.<java.lang.String: boolean equals(java.lang.Object)>(r11) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r9 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.output.format", "csv") in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.output.format", "csv")
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r11 = virtualinvoke $r9.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r10)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void buildRequest(com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke $r12.<java.lang.String: boolean equals(java.lang.Object)>(r11)
The sink $i0 = virtualinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: int ordinal()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel valueOf(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/fs/s3a/s3guard/S3Guard$DisabledWarnLevel;", r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel valueOf(java.lang.String)>
		 -> $r2 = (org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel) $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel valueOf(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $i0 = virtualinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$DisabledWarnLevel: int ordinal()>()
The sink $z0 = virtualinvoke r4.<java.io.File: boolean mkdirs()>() in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $z0 = virtualinvoke r4.<java.io.File: boolean mkdirs()>()
The sink $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2) in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name") in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
	on Path: 
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name")
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
The sink $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>($r3) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn>
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>($r3)
The sink r104 = virtualinvoke $r57.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()> was called with values from the following sources:
- r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "") in method <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r103 = virtualinvoke $r53.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("tmpfiles", "")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r103)
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(",")
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> $r57 = virtualinvoke $r56.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r95)
	 -> <org.apache.hadoop.streaming.StreamJob: void parseArgv()>
		 -> r104 = virtualinvoke $r57.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r11 = staticinvoke <org.apache.hadoop.net.NetUtils: java.lang.String normalizeHostName(java.lang.String)>(r6) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r6 = r1
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = staticinvoke <org.apache.hadoop.net.NetUtils: java.lang.String normalizeHostName(java.lang.String)>(r6)
The sink $r11 = virtualinvoke r6.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.tools.CopyListing simpleListing> = $r3
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r11 = virtualinvoke r6.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $z2 == 0 goto $r8 = new org.apache.hadoop.fs.azure.WasbRemoteCallHelper in method <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.enable.kerberos.support", 0) in method <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.enable.kerberos.support", 0)
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: boolean isKerberosSupportEnabled> = $z0
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = r1.<org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: boolean isKerberosSupportEnabled>
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> if $z2 == 0 goto $r8 = new org.apache.hadoop.fs.azure.WasbRemoteCallHelper
The sink $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>() in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.mode", "server") in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.mode", "server")
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>()
The sink if $z0 != 0 goto $z1 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean isUpdated()>() in method <org.apache.hadoop.tools.rumen.state.StatePool: void persist()> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run(java.lang.String[])>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: int run()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> $r1 = r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> virtualinvoke $r1.<org.apache.hadoop.tools.rumen.state.StatePool: void persist()>()
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void persist()>
		 -> $z0 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void persist()>
		 -> if $z0 != 0 goto $z1 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean isUpdated()>()
The sink $r3 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>(r1, r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r61[0] = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>(r13, "\tWarning: committer \'%s\' is unknown", $r61)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>(r1, r2)
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: int run(java.lang.String[],java.io.PrintStream)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Init: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>(r4, r53)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r1 = interfaceinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: java.util.Map getDiagnostics()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> $r8 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("region", $r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.util.Map getDiagnostics()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> $r3 = interfaceinvoke r1.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r4 = interfaceinvoke $r3.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> $r5 = interfaceinvoke r4.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> r6 = (java.util.Map$Entry) $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> $r9 = interfaceinvoke r6.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> $r7[1] = $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void printStoreDiagnostics(java.io.PrintStream,org.apache.hadoop.fs.s3a.s3guard.MetadataStore)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>(r2, "\t%s=%s", $r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>(r1, r2)
- r94 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r94 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r95 = r94
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r20[1] = $r95
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>(r13, "\tEndpoint: %s=%s", $r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>(r1, r2)
- l4 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l3, $r70) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l3, $r70)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r72 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r71[1] = $r72
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>(r13, "\tMetadata time to live: %s=%s milliseconds", $r71)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void println(java.io.PrintStream,java.lang.String,java.lang.Object[])>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>(r1, r2)
The sink if $z0 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> r1 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("bytebuffer")
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> if $z0 == 0 goto (branch)
The sink $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
- r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z2 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r7)
The sink $r50 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r50 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r50 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $b1 >= 0 goto $z1 = 0 in method <org.apache.hadoop.mapred.gridmix.RandomAlgorithms$Selector: void <init>(int,double,java.util.Random)> was called with values from the following sources:
- i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.sleep.fake-locations", 0) in method <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
		 -> i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.sleep.fake-locations", 0)
	 -> <org.apache.hadoop.mapred.gridmix.JobCreator$2: org.apache.hadoop.mapred.gridmix.GridmixJob createGridmixJob(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int)>
		 -> specialinvoke $r3.<org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>(r0, l0, r4, r5, r6, i1, i4, $r7)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> r0.<org.apache.hadoop.mapred.gridmix.SleepJob: int fakeLocations> = i2
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> $i6 = r0.<org.apache.hadoop.mapred.gridmix.SleepJob: int fakeLocations>
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> $f1 = (float) $i6
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob: void <init>(org.apache.hadoop.conf.Configuration,long,org.apache.hadoop.tools.rumen.JobStory,org.apache.hadoop.fs.Path,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.String[])>
		 -> specialinvoke $r12.<org.apache.hadoop.mapred.gridmix.RandomAlgorithms$Selector: void <init>(int,double,java.util.Random)>($i8, $d0, $r10)
	 -> <org.apache.hadoop.mapred.gridmix.RandomAlgorithms$Selector: void <init>(int,double,java.util.Random)>
		 -> $b1 = d0 cmpg 0.1
	 -> <org.apache.hadoop.mapred.gridmix.RandomAlgorithms$Selector: void <init>(int,double,java.util.Random)>
		 -> if $b1 >= 0 goto $z1 = 0
The sink $i9 = interfaceinvoke r10.<java.util.List: int size()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)> was called with values from the following sources:
- i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getResourceForContainer(java.util.Map)>
		 -> return r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> specialinvoke $r39.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r33, l3, r10, i7, r34, r35, l10, l12)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>($r39)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> return r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(java.util.Map)>
		 -> specialinvoke r6.<org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)>(r18, r17, r5, r19, l0, l1, $r9, $r10, r16)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)>(r1, r2, r3, r4, l0, l1, r5, null, -1L, r6, r7, null)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)>
		 -> $i9 = interfaceinvoke r10.<java.util.List: int size()>()
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> return $r3
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getResourceForContainer(java.util.Map)>
		 -> return r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> specialinvoke $r39.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>(r33, l3, r10, i7, r34, r35, l10, l12)
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: org.apache.hadoop.yarn.api.records.Resource resource> = r2
	 -> <org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator: void <init>(org.apache.hadoop.yarn.api.records.Resource,long,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,long,long)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>($r39)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: java.util.List getTaskContainers(java.util.Map)>
		 -> return r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void createAMForJob(java.util.Map)>
		 -> specialinvoke r6.<org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)>(r18, r17, r5, r19, l0, l1, $r9, $r10, r16)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)>(r1, r2, r3, r4, l0, l1, r5, null, -1L, r6, r7, null)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void runNewAM(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,long,java.util.List,org.apache.hadoop.yarn.api.records.ReservationId,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map)>
		 -> $i9 = interfaceinvoke r10.<java.util.List: int size()>()
The sink $i0 = virtualinvoke r4.<java.lang.String: int hashCode()>() in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $i0 = virtualinvoke r4.<java.lang.String: int hashCode()>()
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $i0 = virtualinvoke r4.<java.lang.String: int hashCode()>()
The sink if $b1 >= 0 goto i2 = virtualinvoke r5.<org.apache.hadoop.mapred.ClusterStatus: int getTaskTrackers()>() in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> $b1 = l0 cmp 0L
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> if $b1 >= 0 goto i2 = virtualinvoke r5.<org.apache.hadoop.mapred.ClusterStatus: int getTaskTrackers()>()
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r11
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r41
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.DistCpSync: org.apache.hadoop.tools.CopyFilter copyFilter> = $r3
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> r7 = $r4
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> $z1 = virtualinvoke r7.<org.apache.hadoop.tools.DistCpSync: boolean sync()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> $z0 = specialinvoke r0.<org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>
		 -> throw $r50
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> return 0
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>(r2, r7)
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> specialinvoke $r2.<org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>($r4, $r5, r6)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.DistCpSync distCpSync> = r3
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r31 = staticinvoke <java.util.Arrays: java.util.List asList(java.lang.Object[])>($r30) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r30 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.azure.chmod.allowed.userlist", $r29) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r30 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.azure.chmod.allowed.userlist", $r29)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r31 = staticinvoke <java.util.Arrays: java.util.List asList(java.lang.Object[])>($r30)
The sink if $z6 == 0 goto $r22 = <java.lang.System: java.io.PrintStream out> in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z6 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $z6 == 0 goto $r22 = <java.lang.System: java.io.PrintStream out>
The sink $r12 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r11) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r9 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", Hadoop/")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r12 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r11)
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0)
The sink specialinvoke r0.<java.lang.Object: void <init>()>() in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)> was called with values from the following sources:
- i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r6.<org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>($r8, i0)
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> r0.<org.apache.hadoop.yarn.sls.web.SLSWebApp: int port> = i0
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: org.apache.hadoop.yarn.sls.web.SLSWebApp web> = $r6
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> specialinvoke r0.<java.lang.Object: void <init>()>()
The sink if $b7 <= 0 goto l36 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)> was called with values from the following sources:
- $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> = $l1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>(0L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l4 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize>
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l6 = l0 + $l4
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $b7 = $l6 cmp $l5
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> if $b7 <= 0 goto l36 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize>
The sink specialinvoke $r18.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r19) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r18.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r19)
The sink if $z2 == 0 goto return r14 in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()> was called with values from the following sources:
- $z2 = virtualinvoke $r24.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.task.jvm-options.enable", 1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> $z2 = virtualinvoke $r24.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.task.jvm-options.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> if $z2 == 0 goto return r14
The sink interfaceinvoke $r31.<com.google.common.cache.Cache: void put(java.lang.Object,java.lang.Object)>(r5, r29) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)> was called with values from the following sources:
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> interfaceinvoke $r9.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r16, null)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r5 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r31 = r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> interfaceinvoke $r31.<com.google.common.cache.Cache: void put(java.lang.Object,java.lang.Object)>(r5, r29)
The sink virtualinvoke $r9.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding: void init(org.apache.hadoop.conf.Configuration)>(r1) in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r9.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding: void init(org.apache.hadoop.conf.Configuration)>(r1)
The sink specialinvoke $r82.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void deleteMissing(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void deleteMissing(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void deleteMissing(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void deleteMissing(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r82.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3)
The sink if i1 >= i0 goto $r4 = r3.<org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: java.util.List emulationPlugins> in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> i0 = lengthof r26
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> if i1 >= i0 goto $r4 = r3.<org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: java.util.List emulationPlugins>
The sink staticinvoke <com.microsoft.azure.storage.OperationContext: void setLoggingEnabledByDefault(boolean)>($z6) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $z6 = virtualinvoke $r38.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.storage.client.logging", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z6 = virtualinvoke $r38.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.storage.client.logging", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> staticinvoke <com.microsoft.azure.storage.OperationContext: void setLoggingEnabledByDefault(boolean)>($z6)
The sink if $i0 != 0 goto r4 = r2 in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $i0 = lengthof r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if $i0 != 0 goto r4 = r2
The sink if $z0 == 0 goto return in method <org.apache.hadoop.yarn.sls.SLSRunner: void enableDNSCaching(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.sls.dns.caching.enabled", 0) in method <org.apache.hadoop.yarn.sls.SLSRunner: void enableDNSCaching(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void enableDNSCaching(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.sls.dns.caching.enabled", 0)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void enableDNSCaching(org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto return
The sink $r2 = virtualinvoke $r0.<java.util.concurrent.ConcurrentHashMap: java.lang.Object remove(java.lang.Object)>($r1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(int)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r1 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(int)>($i0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(int)>
		 -> $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: java.util.List pullDescription(int)>
		 -> $r2 = virtualinvoke $r0.<java.util.concurrent.ConcurrentHashMap: java.lang.Object remove(java.lang.Object)>($r1)
The sink if z20 == 0 goto $z23 = virtualinvoke r5.<org.apache.hadoop.fs.shell.CommandFormat: boolean getOpt(java.lang.String)>("magic") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- z20 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> z20 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if z20 == 0 goto $z23 = virtualinvoke r5.<org.apache.hadoop.fs.shell.CommandFormat: boolean getOpt(java.lang.String)>("magic")
The sink r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" defined in ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(": ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>()
The sink l16 = staticinvoke <java.lang.Math: long max(long,long)>(134217728L, l0) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l16 = staticinvoke <java.lang.Math: long max(long,long)>(134217728L, l0)
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r3) in method <org.apache.hadoop.mapred.gridmix.Gridmix: int start(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,long,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.output.directory", "gridmix") in method <org.apache.hadoop.mapred.gridmix.Gridmix: int start(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,long,org.apache.hadoop.mapred.gridmix.UserResolver)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: int start(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,long,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.output.directory", "gridmix")
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: int start(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,long,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r3)
The sink specialinvoke r0.<java.lang.Thread: void <init>(java.lang.String)>(r2) in method <org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)> was called with values from the following sources:
- $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F) in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F)
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: float rateFactor> = $f0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>(r1, "SerialJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: org.apache.hadoop.mapred.gridmix.SerialJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>
		 -> specialinvoke r0.<java.lang.Thread: void <init>(java.lang.String)>(r2)
The sink $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("sequential") in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("sequential")
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("sequential")
The sink $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2) in method <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
The sink if $z1 == 0 goto return in method <org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: void setConf(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.sls.metrics.switch", 1) in method <org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("yarn.sls.metrics.switch", 1)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: boolean metricsON> = $z0
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = r0.<org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: boolean metricsON>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto return
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyHost(java.lang.String)>(r14) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r14 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.proxy.host", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r14 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.proxy.host", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyHost(java.lang.String)>(r14)
The sink $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1) in method <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.UniformSizeInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1)
The sink if $i2 == 1 goto $i7 = lengthof r6 in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $i2 = lengthof r6
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if $i2 == 1 goto $i7 = lengthof r6
The sink specialinvoke $r9.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>($r13) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> specialinvoke $r9.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>($r13)
The sink $r6 = staticinvoke <java.util.TimeZone: java.util.TimeZone getTimeZone(java.lang.String)>(r4) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-date.time-zone", "UTC") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-date.time-zone", "UTC")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = staticinvoke <java.util.TimeZone: java.util.TimeZone getTimeZone(java.lang.String)>(r4)
The sink $r57 = virtualinvoke $r56.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l9 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l10 = $l9 - l8
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l10)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r56 = virtualinvoke $r55.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ms")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r57 = virtualinvoke $r56.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3) in method <org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("sls.synthetic.trace_file") in method <org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("sls.synthetic.trace_file")
	 -> <org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r3)
The sink virtualinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: void setConf(org.apache.hadoop.conf.Configuration)>(r2) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i2 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.paging.maximum", 1000) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i2 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.paging.maximum", 1000)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: int maxKeys> = $i2
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem: void setConf(org.apache.hadoop.conf.Configuration)>(r2)
The sink $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" from configuration (0 if not present).")
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z5 != 0 goto $z6 = 0 in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.identity.transformer.enable.short.name", 0) in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.identity.transformer.enable.short.name", 0)
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: boolean enableShortName> = $z1
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z5 = r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: boolean enableShortName>
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> if $z5 != 0 goto $z6 = 0
The sink $z0 = virtualinvoke $r5.<java.lang.String: boolean startsWith(java.lang.String)>(r3) in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve") in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve")
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r2 = r1
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r3 = r2[i1]
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke $r5.<java.lang.String: boolean startsWith(java.lang.String)>(r3)
The sink if $i10 != 0 goto $i11 = i14 % i13 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> i5 = (int) $d1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i9 = i13 * i5
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i10 = i14 % $i9
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> if $i10 != 0 goto $i11 = i14 % i13
The sink $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r12) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r12)
The sink $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l2) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l2 = virtualinvoke $r4.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l1, $r3)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l2)
The sink $r9 = staticinvoke <com.google.common.collect.Sets: java.util.HashSet newHashSet(java.lang.Object[])>($r7) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r7[0] = $r8
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = staticinvoke <com.google.common.collect.Sets: java.util.HashSet newHashSet(java.lang.Object[])>($r7)
The sink $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0)
The sink if z0 == 0 goto return in method <org.apache.hadoop.tools.rumen.JsonObjectMapperWriter: void <init>(java.io.OutputStream,boolean)> was called with values from the following sources:
- $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.output.pretty.print", 1) in method <org.apache.hadoop.tools.rumen.DefaultOutputter: void init(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.DefaultOutputter: void init(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.output.pretty.print", 1)
	 -> <org.apache.hadoop.tools.rumen.DefaultOutputter: void init(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.tools.rumen.JsonObjectMapperWriter: void <init>(java.io.OutputStream,boolean)>($r13, $z0)
	 -> <org.apache.hadoop.tools.rumen.JsonObjectMapperWriter: void <init>(java.io.OutputStream,boolean)>
		 -> if z0 == 0 goto return
The sink if i1 >= i0 goto $r6 = r5.<org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties jobProperties> in method <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r17 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("rumen.datatypes.jobproperties.parsers", $r16) in method <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> $r17 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("rumen.datatypes.jobproperties.parsers", $r16)
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r21 = (java.lang.Class[]) $r17
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r22 = r21
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> i0 = lengthof r22
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> if i1 >= i0 goto $r6 = r5.<org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties jobProperties>
The sink specialinvoke $r46.<org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval: void <init>(long,long)>($l9, $l12) in method <org.apache.hadoop.resourceestimator.translator.impl.NativeSingleLineParser: void parseLine(java.lang.String,java.util.Map,java.util.Map)> was called with values from the following sources:
- i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.translator.impl.NativeSingleLineParser: void parseLine(java.lang.String,java.util.Map,java.util.Map)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.translator.impl.NativeSingleLineParser: void parseLine(java.lang.String,java.util.Map,java.util.Map)>
		 -> i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.translator.impl.NativeSingleLineParser: void parseLine(java.lang.String,java.util.Map,java.util.Map)>
		 -> $l11 = (long) i0
	 -> <org.apache.hadoop.resourceestimator.translator.impl.NativeSingleLineParser: void parseLine(java.lang.String,java.util.Map,java.util.Map)>
		 -> $l12 = $l10 + $l11
	 -> <org.apache.hadoop.resourceestimator.translator.impl.NativeSingleLineParser: void parseLine(java.lang.String,java.util.Map,java.util.Map)>
		 -> specialinvoke $r46.<org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval: void <init>(long,long)>($l9, $l12)
The sink r13 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequests: org.apache.hadoop.yarn.api.records.ReservationRequests newInstance(java.util.List,org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter)>(r4, $r5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l15 = (long) i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r12 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequest: org.apache.hadoop.yarn.api.records.ReservationRequest newInstance(org.apache.hadoop.yarn.api.records.Resource,int,int,long)>(r2, $i16, 1, $l15)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>(r12)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r13 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequests: org.apache.hadoop.yarn.api.records.ReservationRequests newInstance(java.util.List,org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter)>(r4, $r5)
The sink if $z1 == 0 goto (branch) in method <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])> was called with values from the following sources:
- z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> r61 = virtualinvoke $r9.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>()
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: org.apache.hadoop.hdfs.server.namenode.FSTreeWalk this$0> = r1
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r0 := @this: org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> throw $r10
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> return $r3
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> $z1 = interfaceinvoke r61.<java.util.Iterator: boolean hasNext()>()
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> if $z1 == 0 goto (branch)
The sink if $b5 != 0 goto $z5 = 0 in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $b5 = l9 cmp 0L
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> if $b5 != 0 goto $z5 = 0
The sink $r54 = virtualinvoke $r53.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\'") in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()> was called with values from the following sources:
- r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id") in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r44)
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r49 = virtualinvoke $r48.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\' is being simulated as \'")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r53 = virtualinvoke $r49.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($r52)
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r54 = virtualinvoke $r53.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\'")
The sink specialinvoke $r13.<org.apache.hadoop.mapreduce.lib.input.FileSplit: void <init>(org.apache.hadoop.fs.Path,long,long,java.lang.String[])>($r16, 0L, $l5, null) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List createSplits(org.apache.hadoop.mapreduce.JobContext,java.util.List)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.min.records_per_chunk", 5) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.min.records_per_chunk", 5)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration)>
		 -> return i0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List createSplits(org.apache.hadoop.mapreduce.JobContext,java.util.List)>
		 -> $l5 = (long) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List createSplits(org.apache.hadoop.mapreduce.JobContext,java.util.List)>
		 -> specialinvoke $r13.<org.apache.hadoop.mapreduce.lib.input.FileSplit: void <init>(org.apache.hadoop.fs.Path,long,long,java.lang.String[])>($r16, 0L, $l5, null)
The sink $r15 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s = %.2f; %s threshold of %.2f; done waiting after %d ms.", $r10) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)> was called with values from the following sources:
- $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f7 = $f3 * $f4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d5 = (double) f7
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of missing blocks", "Hadoop:service=NameNode,name=FSNamesystem", "MissingBlocks", $d5, $d4, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r13 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r10[3] = $r13
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r15 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s = %.2f; %s threshold of %.2f; done waiting after %d ms.", $r10)
- $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f2 = $f0 * $f1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> i1 = (int) $f2
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d2 = (double) i1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of live DataNodes", "Hadoop:service=NameNode,name=FSNamesystemState", "NumLiveDataNodes", $d2, $d1, 0, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r13 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r10[3] = $r13
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r15 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s = %.2f; %s threshold of %.2f; done waiting after %d ms.", $r10)
- $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f8 = $f5 * $f6
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d8 = (double) f8
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of under replicated blocks", "Hadoop:service=NameNode,name=FSNamesystemState", "UnderReplicatedBlocks", $d8, $d7, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r13 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r10[3] = $r13
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $r15 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s = %.2f; %s threshold of %.2f; done waiting after %d ms.", $r10)
The sink $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a valid value for ")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.ssl.channel.mode")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.String toString()>()
The sink i0 = virtualinvoke r0.<java.lang.String: int lastIndexOf(int)>(47) in method <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r11 = virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String getBasePathInJarOut(java.lang.String)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>(r1)
	 -> <org.apache.hadoop.streaming.JarBuilder: java.lang.String fileExtension(java.lang.String)>
		 -> i0 = virtualinvoke r0.<java.lang.String: int lastIndexOf(int)>(47)
The sink if z0 == 0 goto return in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProtocolSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> if z0 == 0 goto return
The sink if i0 > 0 goto return i0 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.ideal", 100)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksIdeal(org.apache.hadoop.conf.Configuration)>
		 -> if i0 > 0 goto return i0
The sink $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean enableMultiObjectsDelete> = $z2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean allowAuthoritativeMetadataStore> = $z5
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean failOnMetadataWriteError> = $z1
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.Listing: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Listing listing> = $r34
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean errorsIncludeSql> = $z1
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r61.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>(l13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = $r61
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: boolean magicCommitEnabled> = z0
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.SignerManager: org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider delegationTokenProvider> = r3
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.auth.SignerManager signerManager> = $r37
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean enabled> = $z0
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1) in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.change.detection.version.required", 1)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>(r1, r2, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r3, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>(r1, z0)
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: boolean requireVersion> = z0
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy: void <init>(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy createPolicy(org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode,org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source,boolean)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy getPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy changeDetectionPolicy> = $r45
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r51 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r50.<org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>($r51)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $r3 = (org.apache.hadoop.fs.s3a.WriteOperationHelper) $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.WriteOperationHelper operations> = $r3
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.auth.SignerManager
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.Listing
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.WriteOperationHelper: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper> = $r32
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
The sink specialinvoke $r23.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("distcp.listing.file.path", "")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: org.apache.hadoop.fs.Path getListingFilePath(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r23.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>(r1)
The sink $z0 = virtualinvoke r2.<java.io.File: boolean isDirectory()>() in method <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r23, r11, r10, 0)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r1 = virtualinvoke r0.<java.io.File: java.io.File[] listFiles()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r2 = r1[i4]
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $z0 = virtualinvoke r2.<java.io.File: boolean isDirectory()>()
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,long,long)>($z5, "S3Guard table read capacity %d and and write capacity %d are inconsistent", l8, l9) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,long,long)>($z5, "S3Guard table read capacity %d and and write capacity %d are inconsistent", l8, l9)
- l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,long,long)>($z5, "S3Guard table read capacity %d and and write capacity %d are inconsistent", l8, l9)
The sink $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r6 = staticinvoke <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r1)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r7 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/realtimetrack.json")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke $r2.<java.lang.AssertionError: void <init>(java.lang.Object)>($r7) in method <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)> was called with values from the following sources:
- f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F) in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F)
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = f0 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>(r2, $f2)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is invalid")
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> specialinvoke $r2.<java.lang.AssertionError: void <init>(java.lang.Object)>($r7)
The sink $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>() in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
The sink $i3 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("stream.recordreader.lookahead", $i2) in method <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)> was called with values from the following sources:
- $i0 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("stream.recordreader.maxrec", 50000) in method <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> $i0 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("stream.recordreader.maxrec", 50000)
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> r0.<org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: int maxRecSize_> = $i0
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> $i1 = r0.<org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: int maxRecSize_>
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> $i2 = 2 * $i1
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> $i3 = virtualinvoke $r9.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("stream.recordreader.lookahead", $i2)
The sink $r27 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z3) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $z1 = virtualinvoke $r17.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.selfthrottling.enable", 1) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z1 = virtualinvoke $r17.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.selfthrottling.enable", 1)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean selfThrottlingEnabled> = $z1
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z3 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean selfThrottlingEnabled>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $r27 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z3)
The sink $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l2 = virtualinvoke $r4.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l1, $r3)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l2)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r26
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r29 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker readOp>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: org.apache.hadoop.fs.s3a.Invoker readOp> = r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r26
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> specialinvoke $r14.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>(r33)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r10 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11)
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r10 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11)
The sink $r33 = virtualinvoke $r32.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" failed") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[2] = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[1] = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r29)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r33 = virtualinvoke $r32.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" failed")
The sink if $z0 == 0 goto specialinvoke r0.<org.apache.hadoop.mapred.gridmix.GenerateData$1: void configureRandomBytesDataGenerator()>() in method <org.apache.hadoop.mapred.gridmix.GenerateData$1: org.apache.hadoop.mapreduce.Job run()> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$1: org.apache.hadoop.mapreduce.Job run()>
		 -> if $z0 == 0 goto specialinvoke r0.<org.apache.hadoop.mapred.gridmix.GenerateData$1: void configureRandomBytesDataGenerator()>()
The sink virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r12) in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $i2 = virtualinvoke r0.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: int validateInt(java.lang.reflect.Field)>(r7)
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: int validateInt(java.lang.reflect.Field)>
		 -> r5 = virtualinvoke r3.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.lang.String get(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.lang.String get(java.lang.String)>
		 -> $r3 = virtualinvoke r0.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.lang.String accountConf(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.lang.String accountConf(java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.lang.String get(java.lang.String)>
		 -> return $r6
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: int validateInt(java.lang.reflect.Field)>
		 -> return $i3
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r12)
The sink if $z1 == 0 goto r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r16 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.policy", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r16 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.policy", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r16)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
The sink $r65 = virtualinvoke $r64.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i21) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i5 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.blocksize", 32768)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB> = $i5
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i21 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int blocksizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r65 = virtualinvoke $r64.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i21)
The sink if $z0 == 0 goto $r2 = <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void abortPendingUploadsInCleanup(boolean)> was called with values from the following sources:
- $z0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.staging.abort.pending.uploads", 1) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void abortPendingUploadsInCleanup(boolean)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void abortPendingUploadsInCleanup(boolean)>
		 -> $z0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.staging.abort.pending.uploads", 1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void abortPendingUploadsInCleanup(boolean)>
		 -> if $z0 == 0 goto $r2 = <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.slf4j.Logger LOG>
The sink $r58 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l9) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r58 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l9)
The sink $z0 = virtualinvoke r5.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $z0 = virtualinvoke r5.<java.lang.String: boolean isEmpty()>()
The sink staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r8, $r7) in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>(r14)
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> throw r12
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r15, r0)
	 -> <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>(r0)
	 -> <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>
		 -> interfaceinvoke r0.<org.apache.hadoop.service.Service: void stop()>()
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>($r4)
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> return r3
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStop()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> $r7 = r1.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding>
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r8, $r7)
The sink if $b8 == 0 goto return in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l3 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.extension.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $l3 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.extension.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.PageBlobOutputStream: long configuredPageBlobExtensionSize> = $l3
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $l6 = r0.<org.apache.hadoop.fs.azure.PageBlobOutputStream: long configuredPageBlobExtensionSize>
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $l7 = $l6 % 512L
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $b8 = $l7 cmp 0L
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> if $b8 == 0 goto return
The sink $r33 = staticinvoke <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider$operationRetried__96: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider)>(r0) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r33 = staticinvoke <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider$operationRetried__96: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider)>(r0)
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.assumed.role.session.duration", "1h", $r15) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.assumed.role.session.duration", "1h", $r15)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: long duration> = $l0
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r33 = staticinvoke <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider$operationRetried__96: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider)>(r0)
- $r14 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.session.name", $r13) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.session.name", $r13)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String sessionName> = $r14
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r33 = staticinvoke <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider$operationRetried__96: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider)>(r0)
The sink $r5 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()> was called with values from the following sources:
- r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class") in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class")
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r5 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
The sink $i0 = interfaceinvoke $r1.<java.util.List: int size()>() in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>("batch threshold reached")
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $i0 = interfaceinvoke $r1.<java.util.List: int size()>()
The sink if $i5 == 3 goto $i7 = lengthof r6 in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $i5 = lengthof r6
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if $i5 == 3 goto $i7 = lengthof r6
The sink if z0 == 0 goto return r1 in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 applyS3ClientOptions(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.path.style.access", 0) in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 applyS3ClientOptions(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 applyS3ClientOptions(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.path.style.access", 0)
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 applyS3ClientOptions(com.amazonaws.services.s3.AmazonS3,org.apache.hadoop.conf.Configuration)>
		 -> if z0 == 0 goto return r1
The sink virtualinvoke r3.<com.amazonaws.ClientConfiguration: void setUseThrottleRetries(boolean)>($z0) in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 createS3Client(java.net.URI,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String)> was called with values from the following sources:
- $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.experimental.aws.s3.throttling", 1) in method <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 createS3Client(java.net.URI,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 createS3Client(java.net.URI,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String)>
		 -> $z0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.experimental.aws.s3.throttling", 1)
	 -> <org.apache.hadoop.fs.s3a.DefaultS3ClientFactory: com.amazonaws.services.s3.AmazonS3 createS3Client(java.net.URI,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String)>
		 -> virtualinvoke r3.<com.amazonaws.ClientConfiguration: void setUseThrottleRetries(boolean)>($z0)
The sink $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f5)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" for the job output data.")
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $z3 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("directory") in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z3 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("directory")
The sink virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r18) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r18)
The sink if z6 == 0 goto $r9 = r2.<org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.mapreduce.TaskAttemptContext taskAttemptContext> in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- z6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> z6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.preserve.rawxattrs", 0)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> if z6 == 0 goto $r9 = r2.<org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.mapreduce.TaskAttemptContext taskAttemptContext>
The sink if r1 != null goto $r2 = new java.net.URI in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("test.fs.swift.name") in method <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("test.fs.swift.name")
	 -> <org.apache.hadoop.fs.swift.util.SwiftTestUtils: java.net.URI getServiceURI(org.apache.hadoop.conf.Configuration)>
		 -> if r1 != null goto $r2 = new java.net.URI
The sink virtualinvoke $r3.<com.microsoft.azure.datalake.store.ADLFileInputStream: void setBufferSize(int)>($i0) in method <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.feature.client.cache.readahead", 4194304) in method <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.feature.client.cache.readahead", 4194304)
	 -> <org.apache.hadoop.fs.adl.AdlFsInputStream: void <init>(com.microsoft.azure.datalake.store.ADLFileInputStream,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r3.<com.microsoft.azure.datalake.store.ADLFileInputStream: void setBufferSize(int)>($i0)
The sink if $z0 != 0 goto r1.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: java.net.URI sessionUri> = r2 in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)> was called with values from the following sources:
- $z0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.skip.metrics", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> $z0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.skip.metrics", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> if $z0 != 0 goto r1.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: java.net.URI sessionUri> = r2
The sink $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r15 = virtualinvoke $r14.<java.lang.Class: java.lang.String getName()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r16 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r15)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a resource usage plugin as it does not extend ")
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r20 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r19)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke $r9.<com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration: void <init>(java.lang.String,java.lang.String)>(r5, r7) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> specialinvoke $r9.<com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration: void <init>(java.lang.String,java.lang.String)>(r5, r7)
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> specialinvoke $r9.<com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration: void <init>(java.lang.String,java.lang.String)>(r5, r7)
- r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> specialinvoke $r9.<com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration: void <init>(java.lang.String,java.lang.String)>(r5, r7)
- r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> specialinvoke $r9.<com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration: void <init>(java.lang.String,java.lang.String)>(r5, r7)
The sink r26 = interfaceinvoke r4.<java.util.List: java.util.Iterator iterator()>() in method <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r26 = interfaceinvoke r4.<java.util.List: java.util.Iterator iterator()>()
The sink if r2 != $r14 goto $r15 = staticinvoke <java.lang.Runtime: java.lang.Runtime getRuntime()>() in method <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r5 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>($r4)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r2 = (org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy) $r1
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> return $r5
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getJobSubmissionPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: void startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> if r2 != $r14 goto $r15 = staticinvoke <java.lang.Runtime: java.lang.Runtime getRuntime()>()
The sink $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" Persist:") in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z4 = r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload>
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z4)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" Persist:")
The sink $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>() in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
The sink if $r5 == null goto r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.track.missing.source") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.track.missing.source")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> if $r5 == null goto r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
The sink $r42 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i14) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.timeout", 15000) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.connect.timeout", 15000)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int connectTimeout> = $i1
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i14 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int connectTimeout>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i14)
The sink specialinvoke $r25.<java.util.ArrayList: void <init>(java.util.Collection)>($r26) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d0 = (double) l21
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $d1 = 1.0 * $d0
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> d3 = $d2 / $d1
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r47 = staticinvoke <java.lang.Double: java.lang.Double valueOf(double)>(d3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r7.<java.util.HashMap: java.lang.Object put(java.lang.Object,java.lang.Object)>(r42, $r47)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r26 = virtualinvoke r7.<java.util.HashMap: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> specialinvoke $r25.<java.util.ArrayList: void <init>(java.util.Collection)>($r26)
The sink $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>() in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.source", "etag") in method <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.change.detection.source", "etag")
	 -> <org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source: org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source fromConfiguration(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r1.<java.lang.String: java.lang.String trim()>()
The sink if $b18 >= 0 goto return in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l4 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l6 = $l4 + $l5
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long endTimeStampMs> = $l6
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l17 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long endTimeStampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $b18 = $l16 cmp $l17
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $b18 >= 0 goto return
- i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l3 = (long) i2
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l5 = virtualinvoke $r13.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>($l3, $r12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l6 = $l4 + $l5
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long endTimeStampMs> = $l6
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l17 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long endTimeStampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $b18 = $l16 cmp $l17
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> if $b18 >= 0 goto return
The sink if $z9 == 0 goto $z11 = 0 in method <org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator: void init(java.lang.String,org.apache.hadoop.mapred.gridmix.JobCreator,boolean)> was called with values from the following sources:
- $z9 = virtualinvoke $r28.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.distributed-cache-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator: void init(java.lang.String,org.apache.hadoop.mapred.gridmix.JobCreator,boolean)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator: void init(java.lang.String,org.apache.hadoop.mapred.gridmix.JobCreator,boolean)>
		 -> $z9 = virtualinvoke $r28.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.distributed-cache-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator: void init(java.lang.String,org.apache.hadoop.mapred.gridmix.JobCreator,boolean)>
		 -> if $z9 == 0 goto $z11 = 0
The sink virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>() in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
The sink $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" from configuration (0 if not present).") in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L) in method <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> l0 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.azure.page.blob.size", 0L)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0)
	 -> <org.apache.hadoop.fs.azure.PageBlobOutputStream: void <init>(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext,org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" from configuration (0 if not present).")
The sink $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
The sink if i0 <= 0 goto $r41 = <org.apache.hadoop.fs.adl.AdlFileSystem: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.http.timeout", -1) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("adl.http.timeout", -1)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if i0 <= 0 goto $r41 = <org.apache.hadoop.fs.adl.AdlFileSystem: org.slf4j.Logger LOG>
The sink if r6 == null goto $z4 = 0 in method <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)> was called with values from the following sources:
- r6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.sql", null) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)>
		 -> r6 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.select.sql", null)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: java.util.concurrent.CompletableFuture openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)>
		 -> if r6 == null goto $z4 = 0
The sink virtualinvoke r2.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r14, l1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(java.util.Map)> was called with values from the following sources:
- $i1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.am.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $i1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.am.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = staticinvoke <org.apache.hadoop.yarn.api.records.Resource: org.apache.hadoop.yarn.api.records.Resource newInstance(long,int)>($l0, $i1)
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(java.util.Map)>
		 -> virtualinvoke r2.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r14, l1)
- $l0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("yarn.sls.am.container.memory", 1024L) in method <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("yarn.sls.am.container.memory", 1024L)
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = staticinvoke <org.apache.hadoop.yarn.api.records.Resource: org.apache.hadoop.yarn.api.records.Resource newInstance(long,int)>($l0, $i1)
	 -> <org.apache.hadoop.yarn.sls.conf.SLSConfiguration: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getAMContainerResource(java.util.Map)>
		 -> virtualinvoke r2.<org.apache.hadoop.yarn.api.records.Resource: void setResourceValue(java.lang.String,long)>($r14, l1)
The sink $r40 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i12) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i9 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.requestsize", 64)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB> = $i9
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i12 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int bufferSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r40 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>($i12)
The sink $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()> was called with values from the following sources:
- i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.vcores", 1)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024) in method <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.container.memory.mb", 1024)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getDefaultContainerResource()>
		 -> $r3 = staticinvoke <org.apache.hadoop.yarn.util.resource.Resources: org.apache.hadoop.yarn.api.records.Resource createResource(int,int)>(i0, i1)
The sink if i4 >= 2 goto i0 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: int intOption(org.apache.hadoop.conf.Configuration,java.lang.String,int,int)>(r0, "fs.s3a.max.total.tasks", 32, 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.threads.max", 10) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
		 -> i4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.threads.max", 10)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initThreadPools(org.apache.hadoop.conf.Configuration)>
		 -> if i4 >= 2 goto i0 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: int intOption(org.apache.hadoop.conf.Configuration,java.lang.String,int,int)>(r0, "fs.s3a.max.total.tasks", 32, 1)
The sink if $z4 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z4 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("partitioned")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z4 == 0 goto (branch)
The sink $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l7 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l7)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if r0 != null goto $z0 = r0 instanceof org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int markAsAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)> was called with values from the following sources:
- $l1 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r2) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.io.retry.RetryPolicy batchWriteRetryPolicy> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.FileStatus getStatus()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> l0 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r4 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> r7 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>($r6, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> $r3 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>(r1, r2, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore store> = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.BulkOperationState initiateBulkWrite(org.apache.hadoop.fs.s3a.s3guard.BulkOperationState$OperationType,org.apache.hadoop.fs.Path)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: long importDir()>
		 -> interfaceinvoke r4.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: int markAsAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r3, r7)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int markAsAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> if r0 != null goto $z0 = r0 instanceof org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$AncestorState
The sink $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.acl.default", "") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.acl.default", "")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initCannedAcls(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>()
The sink $r12 = virtualinvoke $r11.<org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: java.lang.String name()>() in method <org.apache.hadoop.mapred.gridmix.ExecutionSummarizer: void finalize(org.apache.hadoop.mapred.gridmix.JobFactory,java.lang.String,long,org.apache.hadoop.mapred.gridmix.UserResolver,org.apache.hadoop.mapred.gridmix.GenerateData$DataStatistics,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2) in method <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("gridmix.job-submission.policy", $r2)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r4 = staticinvoke <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>(r3)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> $r2 = virtualinvoke r0.<java.lang.String: java.lang.String toUpperCase(java.util.Locale)>($r1)
	 -> <org.apache.hadoop.util.StringUtils: java.lang.String toUpperCase(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> $r5 = staticinvoke <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>($r4)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r1 = staticinvoke <java.lang.Enum: java.lang.Enum valueOf(java.lang.Class,java.lang.String)>(class "Lorg/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy;", r0)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> $r2 = (org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy) $r1
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy valueOf(java.lang.String)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getPolicy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy)>
		 -> return $r5
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy getJobSubmissionPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r2
	 -> <org.apache.hadoop.mapred.gridmix.ExecutionSummarizer: void finalize(org.apache.hadoop.mapred.gridmix.JobFactory,java.lang.String,long,org.apache.hadoop.mapred.gridmix.UserResolver,org.apache.hadoop.mapred.gridmix.GenerateData$DataStatistics,org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke $r11.<org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy: java.lang.String name()>()
The sink if $z8 == 0 goto return in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.identity.transformer.enable.short.name", 0) in method <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.identity.transformer.enable.short.name", 0)
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: boolean enableShortName> = $z1
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $z8 = r0.<org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: boolean enableShortName>
	 -> <org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> if $z8 == 0 goto return
The sink virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterAccess(long,java.util.concurrent.TimeUnit)>($l2, $r9) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.ttl", 60000) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.ttl", 60000)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l2 = (long) i0
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterAccess(long,java.util.concurrent.TimeUnit)>($l2, $r9)
The sink specialinvoke $r0.<com.microsoft.azure.datalake.store.oauth2.MsiTokenProvider: void <init>(int)>($i0) in method <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getMsiBasedTokenProvider(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.adl.oauth2.msi.port", -1) in method <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getMsiBasedTokenProvider(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getMsiBasedTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.adl.oauth2.msi.port", -1)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getMsiBasedTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r0.<com.microsoft.azure.datalake.store.oauth2.MsiTokenProvider: void <init>(int)>($i0)
The sink $r4 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()> was called with values from the following sources:
- $z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.store.blob.md5", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()>
		 -> $z0 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.store.blob.md5", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: com.microsoft.azure.storage.blob.BlobRequestOptions getUploadOptions()>
		 -> $r4 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z0)
The sink r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", ")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r12)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
The sink interfaceinvoke $r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(r1, $r3) in method <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: void markDeleted(org.apache.hadoop.fs.Path,long)> was called with values from the following sources:
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> interfaceinvoke $r9.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r16, null)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r5 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r11 = r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r12 = interfaceinvoke $r11.<com.google.common.cache.Cache: java.lang.Object getIfPresent(java.lang.Object)>(r32)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r33 = (org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry) $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r14 = virtualinvoke r33.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata dirListingMetadata>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> virtualinvoke $r14.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: void markDeleted(org.apache.hadoop.fs.Path,long)>(r5, $l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: void markDeleted(org.apache.hadoop.fs.Path,long)>
		 -> $r2 = r0.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: java.util.Map listMap>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: void markDeleted(org.apache.hadoop.fs.Path,long)>
		 -> interfaceinvoke $r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(r1, $r3)
The sink $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r21 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r12)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r22 = virtualinvoke $r21.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" tmpDir=")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("partitioned") in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("partitioned")
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/net/SocketTimeoutException;", $r47) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>($r11)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r9.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>($r10, null)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls> = $r9
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r47 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/net/SocketTimeoutException;", $r47)
The sink if $z1 == 0 goto $z2 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$LongConfigurationValidatorAnnotation;") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r5 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Field[] getDeclaredFields()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r6 = r5
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r7 = r6[i3]
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $z1 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$IntegerConfigurationValidatorAnnotation;")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> if $z1 == 0 goto $z2 = virtualinvoke r7.<java.lang.reflect.Field: boolean isAnnotationPresent(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$LongConfigurationValidatorAnnotation;")
The sink $r17 = virtualinvoke $r16.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_bytes_read_last_second", "Total number of bytes read from Azure Storage during the last second.", 0L) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r43 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r41[0] = $r43
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r44 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("The average latency in milliseconds of downloading a single block. The average latency is calculated over a %d-second rolling window.", $r41)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r45 = virtualinvoke $r42.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_average_block_download_latency_ms", $r44, 0L)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>
		 -> $r3 = staticinvoke <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys: java.lang.Object add(java.lang.Object,java.lang.Object)>(r1, r2)
	 -> <org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys: java.lang.Object add(java.lang.Object,java.lang.Object)>
		 -> r10 = interfaceinvoke $r12.<java.util.Map: java.lang.Object get(java.lang.Object)>(r4)
	 -> <org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys: java.lang.Object add(java.lang.Object,java.lang.Object)>
		 -> return r10
	 -> <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>
		 -> $r5 = (org.apache.hadoop.metrics2.MetricsInfo) $r4
	 -> <org.apache.hadoop.metrics2.lib.Interns: org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>($r3, l0)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> specialinvoke $r3.<org.apache.hadoop.metrics2.lib.MutableGaugeLong: void <init>(org.apache.hadoop.metrics2.MetricsInfo,long)>(r1, l0)
	 -> <org.apache.hadoop.metrics2.lib.MutableGaugeLong: void <init>(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> specialinvoke r0.<org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>(r1)
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> $r2 = staticinvoke <com.google.common.base.Preconditions: java.lang.Object checkNotNull(java.lang.Object,java.lang.Object)>(r1, "metric info")
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> $r3 = (org.apache.hadoop.metrics2.MetricsInfo) $r2
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> r0.<org.apache.hadoop.metrics2.lib.MutableGauge: org.apache.hadoop.metrics2.MetricsInfo info> = $r3
	 -> <org.apache.hadoop.metrics2.lib.MutableGauge: void <init>(org.apache.hadoop.metrics2.MetricsInfo)>
		 -> return
	 -> <org.apache.hadoop.metrics2.lib.MutableGaugeLong: void <init>(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> return
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> r4 = $r3
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> interfaceinvoke $r5.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r6, r4)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> $r5 = r0.<org.apache.hadoop.metrics2.lib.MetricsRegistry: java.util.Map metricsMap>
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> specialinvoke r0.<org.apache.hadoop.metrics2.lib.MetricsRegistry: void checkMetricName(java.lang.String)>($r2)
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: void checkMetricName(java.lang.String)>
		 -> r1 := @this: org.apache.hadoop.metrics2.lib.MetricsRegistry
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)>
		 -> r0 := @this: org.apache.hadoop.metrics2.lib.MetricsRegistry
	 -> <org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>
		 -> r0 := @this: org.apache.hadoop.metrics2.lib.MetricsRegistry
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r42 = r0.<org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: org.apache.hadoop.metrics2.lib.MetricsRegistry registry>
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r16 = r0.<org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: org.apache.hadoop.metrics2.lib.MetricsRegistry registry>
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r17 = virtualinvoke $r16.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_bytes_read_last_second", "Total number of bytes read from Azure Storage during the last second.", 0L)
The sink virtualinvoke $r21.<java.util.ArrayList: boolean add(java.lang.Object)>($r41) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l9 = (long) $i8
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords> = $l11
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r41.<org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>($r40, i16, $l9, r18, r1)
	 -> <org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: org.apache.hadoop.mapred.gridmix.RecordFactory factory> = r2
	 -> <org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r21.<java.util.ArrayList: boolean add(java.lang.Object)>($r41)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f3 = $f2 / f4
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l17 = (long) $f3
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r40.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l17, $l8, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l7 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetRecords> = $l11
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r41.<org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>($r40, i16, $l9, r18, r1)
	 -> <org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: org.apache.hadoop.mapred.gridmix.RecordFactory factory> = r2
	 -> <org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r21.<java.util.ArrayList: boolean add(java.lang.Object)>($r41)
- $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.key.fraction", 0.1F)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f2 = staticinvoke <java.lang.Math: float min(float,float)>(1.0F, $f1)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $f3 = $f0 * $f2
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i21 = (int) $f3
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i22 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i21)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: int keyLen> = $i22
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r41.<org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>($r40, i16, $l9, r18, r1)
	 -> <org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: org.apache.hadoop.mapred.gridmix.RecordFactory factory> = r2
	 -> <org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory: void <init>(org.apache.hadoop.mapred.gridmix.RecordFactory,int,long,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r21.<java.util.ArrayList: boolean add(java.lang.Object)>($r41)
The sink if r1 != null goto (branch) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> r14 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0, r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r4 = virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("getVersionMarkerItem", "../VERSION", 1, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r1 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r3 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r2 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.Item getItem(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r5 = (com.amazonaws.services.dynamodbv2.document.Item) $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> return r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> if r1 != null goto (branch)
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r33 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> r14 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0, r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r0, $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager,com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r4 = virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("getVersionMarkerItem", "../VERSION", 1, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r1 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager cap0>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> $r3 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r2 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.Item getItem(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item lambda$queryVersionMarker$2(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager$lambda_queryVersionMarker_2__140: java.lang.Object execute()>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r5 = (com.amazonaws.services.dynamodbv2.document.Item) $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> return r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> if r1 != null goto (branch)
The sink virtualinvoke r38.<com.fasterxml.jackson.databind.module.SimpleModule: com.fasterxml.jackson.databind.module.SimpleModule addSerializer(java.lang.Class,com.fasterxml.jackson.databind.JsonSerializer)>(class "Lorg/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType;", $r14) in method <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r15 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> specialinvoke $r14.<org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: void <init>(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>($r15, $r16)
	 -> <org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: void <init>(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: org.apache.hadoop.tools.rumen.state.StatePool statePool> = r1
	 -> <org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: void <init>(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> virtualinvoke r38.<com.fasterxml.jackson.databind.module.SimpleModule: com.fasterxml.jackson.databind.module.SimpleModule addSerializer(java.lang.Class,com.fasterxml.jackson.databind.JsonSerializer)>(class "Lorg/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType;", $r14)
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r15 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> specialinvoke $r14.<org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: void <init>(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>($r15, $r16)
	 -> <org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: void <init>(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: org.apache.hadoop.tools.rumen.state.StatePool statePool> = r1
	 -> <org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer: void <init>(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> virtualinvoke r38.<com.fasterxml.jackson.databind.module.SimpleModule: com.fasterxml.jackson.databind.module.SimpleModule addSerializer(java.lang.Class,com.fasterxml.jackson.databind.JsonSerializer)>(class "Lorg/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType;", $r14)
The sink $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a resource usage plugin as it does not extend ") in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r14 = virtualinvoke r9.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r15 = virtualinvoke $r14.<java.lang.Class: java.lang.String getName()>()
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r16 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r15)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a resource usage plugin as it does not extend ")
The sink r21 = virtualinvoke $r5.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_deleted", 1) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":last_updated", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r5 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r5.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_deleted", 1)
The sink if i3 >= 4 goto i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.ttl", 60000) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> if i3 >= 4 goto i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.ttl", 60000)
The sink specialinvoke $r5.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r6) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.s3a.authoritative.path", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.s3a.authoritative.path", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $r6 = r2[i2]
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> specialinvoke $r5.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r6)
The sink if r30 == null goto return in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()> was called with values from the following sources:
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r28 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> throw $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r30 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>($r3, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r20 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.model.TableDescription describe()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r2 = virtualinvoke r20.<com.amazonaws.services.dynamodbv2.model.TableDescription: java.lang.String getTableArn()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r21 = virtualinvoke $r1.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest withResourceArn(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r4 = interfaceinvoke r3.<com.amazonaws.services.dynamodbv2.AmazonDynamoDB: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult listTagsOfResource(com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest)>(r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r22 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult: java.util.List getTags()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r5 = interfaceinvoke r22.<java.util.List: java.util.stream.Stream stream()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r7 = interfaceinvoke $r5.<java.util.stream.Stream: java.util.stream.Stream filter(java.util.function.Predicate)>($r6)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r25 = interfaceinvoke $r7.<java.util.stream.Stream: java.util.Optional findFirst()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r8 = virtualinvoke r25.<java.util.Optional: java.lang.Object get()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r26 = (com.amazonaws.services.dynamodbv2.model.Tag) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r10 = virtualinvoke r26.<com.amazonaws.services.dynamodbv2.model.Tag: java.lang.String getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $i0 = staticinvoke <java.lang.Integer: int parseInt(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r11 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>($r9, $i0, 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>
		 -> $r4 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.document.Item: com.amazonaws.services.dynamodbv2.document.Item withInt(java.lang.String,int)>("table_version", i0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Item: com.amazonaws.services.dynamodbv2.document.Item withLong(java.lang.String,long)>("table_created", l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> return $r11
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> if r30 == null goto return
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r33 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName> = r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r3 = virtualinvoke $r2.<com.amazonaws.services.dynamodbv2.document.DynamoDB: com.amazonaws.services.dynamodbv2.document.Table getTable(java.lang.String)>($r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> throw $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> $r3 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table table>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r30 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>($r3, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r20 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.model.TableDescription describe()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r2 = virtualinvoke r20.<com.amazonaws.services.dynamodbv2.model.TableDescription: java.lang.String getTableArn()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r21 = virtualinvoke $r1.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest withResourceArn(java.lang.String)>($r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r4 = interfaceinvoke r3.<com.amazonaws.services.dynamodbv2.AmazonDynamoDB: com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult listTagsOfResource(com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceRequest)>(r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r22 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.model.ListTagsOfResourceResult: java.util.List getTags()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r5 = interfaceinvoke r22.<java.util.List: java.util.stream.Stream stream()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r7 = interfaceinvoke $r5.<java.util.stream.Stream: java.util.stream.Stream filter(java.util.function.Predicate)>($r6)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r25 = interfaceinvoke $r7.<java.util.stream.Stream: java.util.Optional findFirst()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r8 = virtualinvoke r25.<java.util.Optional: java.lang.Object get()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> r26 = (com.amazonaws.services.dynamodbv2.model.Tag) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r10 = virtualinvoke r26.<com.amazonaws.services.dynamodbv2.model.Tag: java.lang.String getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $i0 = staticinvoke <java.lang.Integer: int parseInt(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> $r11 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>($r9, $i0, 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>
		 -> $r4 = virtualinvoke $r3.<com.amazonaws.services.dynamodbv2.document.Item: com.amazonaws.services.dynamodbv2.document.Item withInt(java.lang.String,int)>("table_version", i0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Item: com.amazonaws.services.dynamodbv2.document.Item withLong(java.lang.String,long)>("table_created", l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: com.amazonaws.services.dynamodbv2.document.Item createVersionMarker(java.lang.String,int,long)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerFromTags(com.amazonaws.services.dynamodbv2.document.Table,com.amazonaws.services.dynamodbv2.AmazonDynamoDB)>
		 -> return $r11
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> if r30 == null goto return
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r18) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r18)
The sink interfaceinvoke $r8.<java.util.Deque: void addFirst(java.lang.Object)>(r12) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)> was called with values from the following sources:
- z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> r61 = virtualinvoke $r9.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>()
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: org.apache.hadoop.hdfs.server.namenode.FSTreeWalk this$0> = r1
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>(r1)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> specialinvoke r0.<org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>(r1, $r2)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk,java.util.Deque)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.TreeWalk)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> specialinvoke $r6.<org.apache.hadoop.hdfs.server.namenode.TreePath: void <init>(org.apache.hadoop.fs.FileStatus,long,org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.permission.AclStatus)>(r2, l0, r0, $r7, r5)
	 -> <org.apache.hadoop.hdfs.server.namenode.TreePath: void <init>(org.apache.hadoop.fs.FileStatus,long,org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.permission.AclStatus)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.TreePath: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator i> = r1
	 -> <org.apache.hadoop.hdfs.server.namenode.TreePath: void <init>(org.apache.hadoop.fs.FileStatus,long,org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.permission.AclStatus)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r12 = $r6
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> interfaceinvoke $r8.<java.util.Deque: void addFirst(java.lang.Object)>(r12)
The sink if z0 == 0 goto $r1 = <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.local.sas.key.mode", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> $z2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.local.sas.key.mode", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> r1.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean useLocalSasKeyMode> = $z2
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> $z5 = r1.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean useLocalSasKeyMode>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>($z5, r3)
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> if z0 == 0 goto $r1 = <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: org.slf4j.Logger LOG>
The sink $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r1)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r20 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ")
The sink $z1 = virtualinvoke r7.<java.lang.String: boolean contains(java.lang.CharSequence)>("(?<message>") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r7.<java.lang.String: boolean contains(java.lang.CharSequence)>("(?<message>")
The sink $r5 = interfaceinvoke r26.<java.util.Iterator: java.lang.Object next()>() in method <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r26 = interfaceinvoke r4.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r5 = interfaceinvoke r26.<java.util.Iterator: java.lang.Object next()>()
The sink specialinvoke $r12.<java.net.URL: void <init>(java.lang.String)>($r18) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror") in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror")
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r31)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r17 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r16)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> specialinvoke $r12.<java.net.URL: void <init>(java.lang.String)>($r18)
The sink $r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("spark.sql.sources.writeJobUUID", $r2) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("spark.app.id", r1) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("spark.app.id", r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("spark.sql.sources.writeJobUUID", $r2)
The sink virtualinvoke r3.<java.util.ArrayList: boolean add(java.lang.Object)>($r11) in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l6 = $l5 + l22
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart> = $l6
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l0 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart>
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r11 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>($l0)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r3.<java.util.ArrayList: boolean add(java.lang.Object)>($r11)
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is invalid") in method <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)> was called with values from the following sources:
- f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F) in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F)
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = f0 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>(r2, $f2)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f0)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is invalid")
The sink $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", ") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", ")
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(". Reduce numMaps or decrease split-ratio to proceed.") in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", numMaps:")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i1)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(". Reduce numMaps or decrease split-ratio to proceed.")
The sink if $z1 == 0 goto $r13 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: com.amazonaws.auth.AWSCredentialsProvider createAWSCredentialProvider(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.URI)>(r0, r10, r12) in method <org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.AWSCredentialProviderList buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r7[0] = $r8
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = staticinvoke <com.google.common.collect.Sets: java.util.HashSet newHashSet(java.lang.Object[])>($r7)
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r10 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.AWSCredentialProviderList buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set)>(r4, r1, "fs.s3a.assumed.role.credentials.provider", $r6, $r9)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.AWSCredentialProviderList buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set)>
		 -> $z1 = interfaceinvoke r11.<java.util.Set: boolean contains(java.lang.Object)>(r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: org.apache.hadoop.fs.s3a.AWSCredentialProviderList buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set)>
		 -> if $z1 == 0 goto $r13 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: com.amazonaws.auth.AWSCredentialsProvider createAWSCredentialProvider(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.URI)>(r0, r10, r12)
The sink $r9 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r9 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
The sink $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)> was called with values from the following sources:
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i4 = r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i4)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", randomizeFileListing=")
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>()
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z2 = r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($z2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r10 = virtualinvoke $r4.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)>($r2, $r9, 0, 65536, $s4, $l3, null) in method <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()> was called with values from the following sources:
- $i0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.gen.blocksize", 268435456) in method <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.gen.blocksize", 268435456)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: int blocksize> = $i0
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke r0.<org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()>()
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()>
		 -> $i2 = r0.<org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: int blocksize>
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()>
		 -> $l3 = (long) $i2
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()>
		 -> $r10 = virtualinvoke $r4.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)>($r2, $r9, 0, 65536, $s4, $l3, null)
- $i1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.gen.replicas", 3) in method <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $i1 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.gen.replicas", 3)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $s2 = (short) $i1
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: short replicas> = $s2
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke r0.<org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()>()
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()>
		 -> $s4 = r0.<org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: short replicas>
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat$ChunkWriter: void nextDestination()>
		 -> $r10 = virtualinvoke $r4.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)>($r2, $r9, 0, 65536, $s4, $l3, null)
The sink virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String,java.lang.String)>("hadoop.security.credential.provider.path", r4, "patch of fs.s3a.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("hadoop.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("hadoop.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r3.<java.util.List: boolean addAll(java.util.Collection)>(r2)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r4 = staticinvoke <org.apache.commons.lang3.StringUtils: java.lang.String join(java.lang.Iterable,char)>(r3, 44)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String,java.lang.String)>("hadoop.security.credential.provider.path", r4, "patch of fs.s3a.security.credential.provider.path")
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path") in method <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.util.Collection getStringCollection(java.lang.String)>("fs.s3a.security.credential.provider.path")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r3 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList(java.lang.Iterable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> r4 = staticinvoke <org.apache.commons.lang3.StringUtils: java.lang.String join(java.lang.Iterable,char)>(r3, 44)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String,java.lang.String)>("hadoop.security.credential.provider.path", r4, "patch of fs.s3a.security.credential.provider.path")
The sink r9 = staticinvoke <org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest newInstance(org.apache.hadoop.yarn.api.records.ReservationDefinition,java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId)>(r14, "resourceestimator", r8) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l15 = (long) i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r12 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequest: org.apache.hadoop.yarn.api.records.ReservationRequest newInstance(org.apache.hadoop.yarn.api.records.Resource,int,int,long)>(r2, $i16, 1, $l15)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>(r12)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r13 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequests: org.apache.hadoop.yarn.api.records.ReservationRequests newInstance(java.util.List,org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter)>(r4, $r5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r14 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationDefinition: org.apache.hadoop.yarn.api.records.ReservationDefinition newInstance(long,long,org.apache.hadoop.yarn.api.records.ReservationRequests,java.lang.String)>(l1, l2, r13, "LpSolver#toRecurringRDL")
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r9 = staticinvoke <org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest newInstance(org.apache.hadoop.yarn.api.records.ReservationDefinition,java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId)>(r14, "resourceestimator", r8)
The sink $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r14) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r14 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.proxy.host", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r14 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.proxy.host", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r14)
The sink if r3 == null goto $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", 0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.type") in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.output.fileoutputformat.compress.type")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void configureCompressionEmulation(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)>
		 -> if r3 == null goto $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("mapreduce.map.output.compress", 0)
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.StreamJob: void msg(java.lang.String)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r2.<org.apache.hadoop.streaming.StreamJob: void msg(java.lang.String)>($r7)
	 -> <org.apache.hadoop.streaming.StreamJob: void msg(java.lang.String)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4)
	 -> <org.apache.hadoop.streaming.StreamJob: void msg(java.lang.String)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i4 = i3 * i1
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f0 = (float) $i4
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $f2 = $f1 / $f0
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d0 = (double) $f2
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> $d1 = staticinvoke <java.lang.Math: double ceil(double)>($d0)
The sink $z0 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("bytebuffer") in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> r1 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("bytebuffer")
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(".")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z4 != 0 goto $z5 = virtualinvoke r3.<org.apache.hadoop.fs.shell.CommandFormat: boolean getOpt(java.lang.String)>("check") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r26
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r29 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker readOp>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: org.apache.hadoop.fs.s3a.Invoker readOp> = r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r26
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> specialinvoke $r14.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>(r33)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z4 = r14 instanceof org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z4 != 0 goto $z5 = virtualinvoke r3.<org.apache.hadoop.fs.shell.CommandFormat: boolean getOpt(java.lang.String)>("check")
The sink $r13 = interfaceinvoke r61.<java.util.Iterator: java.lang.Object next()>() in method <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])> was called with values from the following sources:
- z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0) in method <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> z0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("dfs.provided.acls.import.enabled", 0)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: boolean enableACLs> = z0
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> r61 = virtualinvoke $r9.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>()
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r0.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: org.apache.hadoop.hdfs.server.namenode.FSTreeWalk this$0> = r1
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> r0 := @this: org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> specialinvoke $r3.<org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>(r0, r6, -1L)
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator: void <init>(org.apache.hadoop.hdfs.server.namenode.FSTreeWalk,org.apache.hadoop.fs.FileStatus,long)>
		 -> throw $r10
	 -> <org.apache.hadoop.hdfs.server.namenode.FSTreeWalk: org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator iterator()>
		 -> return $r3
	 -> <org.apache.hadoop.hdfs.server.namenode.FileSystemImage: int run(java.lang.String[])>
		 -> $r13 = interfaceinvoke r61.<java.util.Iterator: java.lang.Object next()>()
The sink r7 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()> was called with values from the following sources:
- $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.persist", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean persist> = $z2
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run(java.lang.String[])>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: int run()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>
		 -> r7 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0) in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("rumen.anonymization.states.reload", 0)
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.rumen.state.StatePool: boolean reload> = $z1
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0 := @this: org.apache.hadoop.tools.rumen.state.StatePool
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r3 = r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.tools.rumen.state.StatePool statePool>
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> $r16 = virtualinvoke r1.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void initialize(java.lang.String[])>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run(java.lang.String[])>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: int run()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTrace()>
		 -> return
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: int run()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>()
	 -> <org.apache.hadoop.tools.rumen.Anonymizer: void anonymizeTopology()>
		 -> r7 = virtualinvoke r0.<org.apache.hadoop.tools.rumen.Anonymizer: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $b1 > 0 goto r0.<org.apache.hadoop.tools.util.ThrottledInputStream: java.io.InputStream rawStream> = r1 in method <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)> was called with values from the following sources:
- f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F) in method <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("distcp.map.bandwidth.mb", 100.0F)
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f1 = f0 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 * 1024.0F
	 -> <org.apache.hadoop.tools.mapred.RetriableFileCopyCommand: org.apache.hadoop.tools.util.ThrottledInputStream getInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>(r2, $f2)
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> $b1 = f0 cmpl 0.0F
	 -> <org.apache.hadoop.tools.util.ThrottledInputStream: void <init>(java.io.InputStream,float)>
		 -> if $b1 > 0 goto r0.<org.apache.hadoop.tools.util.ThrottledInputStream: java.io.InputStream rawStream> = r1
The sink if $i0 < 0 goto r6 = (org.apache.hadoop.mapreduce.lib.input.FileSplit) r4 in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class") in method <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.recordreader.class")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $i0 = virtualinvoke r2.<java.lang.String: int indexOf(java.lang.String)>("LineRecordReader")
	 -> <org.apache.hadoop.streaming.mapreduce.StreamInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> if $i0 < 0 goto r6 = (org.apache.hadoop.mapreduce.lib.input.FileSplit) r4
The sink $r2 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("app-attempt-%04d", $r0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicTaskAttemptsPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r0[0] = $r1
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r2 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("app-attempt-%04d", $r0)
The sink $r40 = virtualinvoke $r37.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_average_block_upload_latency_ms", $r39, 0L) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r38 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r36[0] = $r38
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r39 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("The average latency in milliseconds of uploading a single block. The average latency is calculated over a %d-second rolling window.", $r36)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r40 = virtualinvoke $r37.<org.apache.hadoop.metrics2.lib.MetricsRegistry: org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String,java.lang.String,long)>("wasb_average_block_upload_latency_ms", $r39, 0L)
The sink l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15) in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2) in method <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2)
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod> = $l0
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> r4 = $r2
	 -> <org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl: void <init>(boolean,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r4.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>(r3)
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l2 = virtualinvoke r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>()
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> $l0 = r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod>
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> return $l0
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l3 = $l2 * 24L
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l1 = $l3 * 60L
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15)
The sink $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r29) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[2] = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[1] = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r29)
The sink $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" threads") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("auditreplay.num-threads", 1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads> = $i1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $i2 = r2.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: int numThreads>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" threads")
The sink if i2 >= $i1 goto return r4 in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.s3a.authoritative.path", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.s3a.authoritative.path", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> $i1 = lengthof r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(java.net.URI,org.apache.hadoop.conf.Configuration,java.util.function.Function)>
		 -> if i2 >= $i1 goto return r4
The sink $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r23 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/jobruntime.csv")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.String toString()>()
The sink specialinvoke $r1.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanupTempFiles(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $r2 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanupTempFiles(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanupTempFiles(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r2 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void cleanupTempFiles(org.apache.hadoop.mapreduce.JobContext)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r2)
The sink if $b0 < 0 goto $z0 = 0 in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)> was called with values from the following sources:
- $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>($f2)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>(f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $b0 = f0 cmpl 0.0F
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> if $b0 < 0 goto $z0 = 0
- $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>($f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $b0 = f0 cmpl 0.0F
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> if $b0 < 0 goto $z0 = 0
The sink $r17 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r16) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror") in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> r31 = virtualinvoke r11.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dyno.apache-mirror")
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r31)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: java.io.File fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r17 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r16)
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>($r2, $r3) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicTaskAttemptsPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r1 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r0[0] = $r1
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> $r2 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("app-attempt-%04d", $r0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: java.lang.String formatAppAttemptDir(int)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: org.apache.hadoop.fs.Path getMagicJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>($r2, $r3)
The sink $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536) in method <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $i8 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.missing.rec.size", 65536)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l9 = (long) $i8
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f3 = $f2 / f4
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l17 = (long) $f3
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r40.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l17, $l8, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l7 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $f1 = $f0 / f5
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l18 = (long) $f1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r43.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>(l18, $l2, r1, 5120)
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes> = l0
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l7 = r0.<org.apache.hadoop.mapred.gridmix.AvgRecordFactory: long targetBytes>
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l10 = $l7 / $l9
	 -> <org.apache.hadoop.mapred.gridmix.AvgRecordFactory: void <init>(long,long,org.apache.hadoop.conf.Configuration,int)>
		 -> $l11 = staticinvoke <java.lang.Math: long max(long,long)>(1L, $l10)
The sink if i3 >= i0 goto return in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r5 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Field[] getDeclaredFields()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r6 = r5
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> i0 = lengthof r6
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> if i3 >= i0 goto return
The sink if r14 != null goto $r20 = (java.io.IOException) r14 in method <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateDynamoDBException(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[2] = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[1] = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r29)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r33 = virtualinvoke $r32.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" failed")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r34 = virtualinvoke $r33.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r35 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateDynamoDBException(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException)>(r29, $r34, r62)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateDynamoDBException(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException)>
		 -> specialinvoke $r18.<org.apache.hadoop.fs.s3a.AWSBadRequestException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>(r3, r0)
	 -> <org.apache.hadoop.fs.s3a.AWSBadRequestException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.AWSServiceIOException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.AWSServiceIOException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r0.<org.apache.hadoop.fs.s3a.AWSClientIOException: java.lang.String operation> = r2
	 -> <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.AWSServiceIOException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.AWSBadRequestException: void <init>(java.lang.String,com.amazonaws.AmazonServiceException)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateDynamoDBException(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException)>
		 -> r14 = $r18
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateDynamoDBException(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException)>
		 -> if r14 != null goto $r20 = (java.io.IOException) r14
The sink specialinvoke $r7.<java.io.FileOutputStream: void <init>(java.io.File)>($r8) in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore,java.lang.String,long)> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> specialinvoke $r0.<java.io.File: void <init>(java.lang.String)>($r3)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r4 = $r0
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> r5 = staticinvoke <java.io.File: java.io.File createTempFile(java.lang.String,java.lang.String,java.io.File)>("output-", ".tmp", r4)
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File newBackupFile()>
		 -> return r5
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore,java.lang.String,long)>
		 -> r0.<org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File backupFile> = $r4
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore,java.lang.String,long)>
		 -> $r8 = r0.<org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: java.io.File backupFile>
	 -> <org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore,java.lang.String,long)>
		 -> specialinvoke $r7.<java.io.FileOutputStream: void <init>(java.io.File)>($r8)
The sink $z14 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("directory") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z14 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("directory")
The sink $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r22 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustertype", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r22 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustertype", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r22)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>()
- r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r72)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r22)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $z2 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("disk") in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> r1 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $z2 = virtualinvoke r1.<java.lang.String: boolean equals(java.lang.Object)>("disk")
The sink if $z1 == 0 goto $r5 = <org.apache.hadoop.fs.s3a.S3AUtils: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- $z1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> $z1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.connection.ssl.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> if $z1 == 0 goto $r5 = <org.apache.hadoop.fs.s3a.S3AUtils: org.slf4j.Logger LOG>
The sink if $z1 == 0 goto virtualinvoke r0.<org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void init()>() in method <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)> was called with values from the following sources:
- $z0 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("stream.recordreader.slowmatch", 0) in method <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
	on Path: 
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> $z0 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("stream.recordreader.slowmatch", 0)
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> r0.<org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: boolean slowMatch_> = $z0
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> $z1 = r0.<org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: boolean slowMatch_>
	 -> <org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void <init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
		 -> if $z1 == 0 goto virtualinvoke r0.<org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader: void init()>()
The sink $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f5) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.job-output.compression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f5)
The sink specialinvoke $r0.<java.lang.RuntimeException: void <init>(java.lang.String)>($r5) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(".")
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> specialinvoke $r0.<java.lang.RuntimeException: void <init>(java.lang.String)>($r5)
The sink if $z4 == 0 goto $l5 = 0L in method <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":mod_time", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r13 = virtualinvoke $r12.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_dir", 1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r0, $r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3> = $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r4 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r5 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r2, $r3, $r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.ItemCollection scan(java.lang.String,java.lang.String,java.util.Map,java.util.Map)>(r1, r2, null, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r10 = (com.amazonaws.services.dynamodbv2.document.ItemCollection) $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> return $r10
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r14 = virtualinvoke r13.<com.amazonaws.services.dynamodbv2.document.ItemCollection: com.amazonaws.services.dynamodbv2.document.internal.IteratorSupport iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r19 = interfaceinvoke r14.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r20 = (com.amazonaws.services.dynamodbv2.document.Item) $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r22 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>(r20, $r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> $z4 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: boolean hasAttribute(java.lang.String)>("mod_time")
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> if $z4 == 0 goto $l5 = 0L
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r61 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB> = $i7
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i20 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r60 = virtualinvoke $r59.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i20)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r61 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $z1 == 0 goto (branch) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r14 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.proxy.host", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r14 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.oss.proxy.host", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $z1 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r14)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if $z1 == 0 goto (branch)
The sink if $z1 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("magic")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto (branch)
The sink if $z6 == 0 goto $z9 = virtualinvoke r27.<java.lang.String: boolean equals(java.lang.Object)>("/") in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: org.apache.hadoop.fs.azure.FileMetadata[] listInternal(java.lang.String,int,int)> was called with values from the following sources:
- $z6 = virtualinvoke $r21.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.flatlist.enable", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: org.apache.hadoop.fs.azure.FileMetadata[] listInternal(java.lang.String,int,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: org.apache.hadoop.fs.azure.FileMetadata[] listInternal(java.lang.String,int,int)>
		 -> $z6 = virtualinvoke $r21.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.flatlist.enable", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: org.apache.hadoop.fs.azure.FileMetadata[] listInternal(java.lang.String,int,int)>
		 -> if $z6 == 0 goto $z9 = virtualinvoke r27.<java.lang.String: boolean equals(java.lang.Object)>("/")
The sink if i1 >= i0 goto $r13 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("<CPS>") in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> i0 = lengthof r12
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> if i1 >= i0 goto $r13 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("<CPS>")
The sink $z13 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z13 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("file")
The sink $r4 = virtualinvoke $r2.<javax.ws.rs.core.UriBuilder: java.net.URI build(java.lang.Object[])>($r3) in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998) in method <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.service-port", 9998)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: int getPort(org.apache.hadoop.conf.Configuration)>
		 -> return $i0
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke $r0.<javax.ws.rs.core.UriBuilder: javax.ws.rs.core.UriBuilder port(int)>($i0)
	 -> <org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer: java.net.URI getBaseURI(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r2.<javax.ws.rs.core.UriBuilder: java.net.URI build(java.lang.Object[])>($r3)
The sink if $z1 == 0 goto $r8 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>(r7) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r7.<java.lang.String: boolean contains(java.lang.CharSequence)>("(?<message>")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $z1 == 0 goto $r8 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>(r7)
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSClientIOException;", $r41) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>($r11)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r9.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>($r10, null)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls> = $r9
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r41 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSClientIOException;", $r41)
The sink if z0 == 0 goto $r33 = <com.aliyun.oss.common.comm.Protocol: com.aliyun.oss.common.comm.Protocol HTTP> in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- z0 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.oss.connection.secure.enabled", 1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> z0 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.oss.connection.secure.enabled", 1)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if z0 == 0 goto $r33 = <com.aliyun.oss.common.comm.Protocol: com.aliyun.oss.common.comm.Protocol HTTP>
The sink $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3) in method <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id") in method <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id")
	 -> <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
The sink if $z4 == 0 goto $z6 = 0 in method <org.apache.hadoop.fs.s3a.S3AUtils: boolean isThrottleException(java.lang.Exception)> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r32 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> specialinvoke $r59.<org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>(r49, r4)
	 -> <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r0.<org.apache.hadoop.fs.s3a.AWSClientIOException: java.lang.String operation> = r2
	 -> <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> return $r59
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> interfaceinvoke r0.<org.apache.hadoop.fs.s3a.Invoker$Retried: void onFailure(java.lang.String,java.io.IOException,int,boolean)>(r6, r24, i1, z2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void onFailure(java.lang.String,java.io.IOException,int,boolean)>
		 -> virtualinvoke $r3.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void scanRetryEvent(java.lang.String,java.io.IOException,int,boolean)>($r1, $r2, $i0, $z0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void scanRetryEvent(java.lang.String,java.io.IOException,int,boolean)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void retryEvent(java.lang.String,java.io.IOException,int,boolean)>(r2, r3, i1, z0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void retryEvent(java.lang.String,java.io.IOException,int,boolean)>
		 -> $z0 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: boolean isThrottleException(java.lang.Exception)>(r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: boolean isThrottleException(java.lang.Exception)>
		 -> $z4 = r0 instanceof com.amazonaws.SdkBaseException
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: boolean isThrottleException(java.lang.Exception)>
		 -> if $z4 == 0 goto $z6 = 0
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r27 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> specialinvoke $r59.<org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>(r49, r4)
	 -> <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r0.<org.apache.hadoop.fs.s3a.AWSClientIOException: java.lang.String operation> = r2
	 -> <org.apache.hadoop.fs.s3a.AWSClientIOException: void <init>(java.lang.String,com.amazonaws.SdkBaseException)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> return $r59
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> interfaceinvoke r0.<org.apache.hadoop.fs.s3a.Invoker$Retried: void onFailure(java.lang.String,java.io.IOException,int,boolean)>(r6, r24, i1, z2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void onFailure(java.lang.String,java.io.IOException,int,boolean)>
		 -> virtualinvoke $r3.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void scanRetryEvent(java.lang.String,java.io.IOException,int,boolean)>($r1, $r2, $i0, $z0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void scanRetryEvent(java.lang.String,java.io.IOException,int,boolean)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void retryEvent(java.lang.String,java.io.IOException,int,boolean)>(r2, r3, i1, z0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void retryEvent(java.lang.String,java.io.IOException,int,boolean)>
		 -> $z0 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: boolean isThrottleException(java.lang.Exception)>(r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: boolean isThrottleException(java.lang.Exception)>
		 -> $z4 = r0 instanceof com.amazonaws.SdkBaseException
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: boolean isThrottleException(java.lang.Exception)>
		 -> if $z4 == 0 goto $z6 = 0
The sink $r39 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("The average latency in milliseconds of uploading a single block. The average latency is calculated over a %d-second rolling window.", $r36) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5) in method <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r35.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.metrics.rolling.window.size", 5)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r38 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r36[0] = $r38
	 -> <org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r39 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("The average latency in milliseconds of uploading a single block. The average latency is calculated over a %d-second rolling window.", $r36)
The sink specialinvoke $r64.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r10) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r10 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void concatFileChunks(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r64.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r10)
The sink if 0 >= i4 goto $r9 = staticinvoke <java.lang.Runtime: java.lang.Runtime getRuntime()>() in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- i4 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.storage.timeout", 0) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> i4 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.azure.storage.timeout", 0)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> if 0 >= i4 goto $r9 = staticinvoke <java.lang.Runtime: java.lang.Runtime getRuntime()>()
The sink $r4 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture allOf(java.util.concurrent.CompletableFuture[])>($r3) in method <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>("batch threshold reached")
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $r5 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $r2 = interfaceinvoke r0.<java.util.List: java.lang.Object[] toArray(java.lang.Object[])>($r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $r3 = (java.util.concurrent.CompletableFuture[]) $r2
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $r4 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture allOf(java.util.concurrent.CompletableFuture[])>($r3)
The sink if r35 != null goto $z5 = 0 in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r35 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.password") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r35 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.password")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> if r35 != null goto $z5 = 0
The sink if i6 >= i1 goto return in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> i1 = lengthof r4
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if i6 >= i1 goto return
The sink r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2) in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
The sink $r7 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()> was called with values from the following sources:
- r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class") in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class")
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r7 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
The sink if $b3 <= 0 goto $b4 = l8 cmp 0L in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $b3 = l8 cmp 0L
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> if $b3 <= 0 goto $b4 = l8 cmp 0L
The sink if $z13 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z13 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z13 == 0 goto (branch)
The sink if $z3 == 0 goto $z9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.override.canonical.service.name", 0) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z7 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.enable.kerberos.support", 0) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z7 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.enable.kerberos.support", 0)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: boolean kerberosSupportEnabled> = $z7
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z3 = r2.<org.apache.hadoop.fs.azure.NativeAzureFileSystem: boolean kerberosSupportEnabled>
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if $z3 == 0 goto $z9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.override.canonical.service.name", 0)
The sink virtualinvoke r71.<com.microsoft.azure.datalake.store.ADLStoreOptions: com.microsoft.azure.datalake.store.ADLStoreOptions setUserAgentSuffix(java.lang.String)>($r40) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r22 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustertype", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r22 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustertype", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r22)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r71.<com.microsoft.azure.datalake.store.ADLStoreOptions: com.microsoft.azure.datalake.store.ADLStoreOptions setUserAgentSuffix(java.lang.String)>($r40)
- r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r72)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r22)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r71.<com.microsoft.azure.datalake.store.ADLStoreOptions: com.microsoft.azure.datalake.store.ADLStoreOptions setUserAgentSuffix(java.lang.String)>($r40)
The sink $r136 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r42, $r41, r2, $r39)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> $r136 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink if $r7 != class "Lorg/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler;" goto $r31 = (org.apache.hadoop.conf.Configuration) r3 in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()> was called with values from the following sources:
- r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class") in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class")
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r7 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> if $r7 != class "Lorg/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler;" goto $r31 = (org.apache.hadoop.conf.Configuration) r3
The sink if $b6 <= 0 goto $r10 = newarray (java.lang.Object)[5] in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)> was called with values from the following sources:
- $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f7 = $f3 * $f4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d5 = (double) f7
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of missing blocks", "Hadoop:service=NameNode,name=FSNamesystem", "MissingBlocks", $d5, $d4, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $b6 = d0 cmpg d4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> if $b6 <= 0 goto $r10 = newarray (java.lang.Object)[5]
- $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f2 = $f0 * $f1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> i1 = (int) $f2
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d2 = (double) i1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of live DataNodes", "Hadoop:service=NameNode,name=FSNamesystemState", "NumLiveDataNodes", $d2, $d1, 0, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $b6 = d0 cmpg d4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> if $b6 <= 0 goto $r10 = newarray (java.lang.Object)[5]
- $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f8 = $f5 * $f6
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d8 = (double) f8
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of under replicated blocks", "Hadoop:service=NameNode,name=FSNamesystemState", "UnderReplicatedBlocks", $d8, $d7, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $b6 = d0 cmpg d4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> if $b6 <= 0 goto $r10 = newarray (java.lang.Object)[5]
The sink if $z3 == 0 goto $r32 = new org.apache.hadoop.mapred.gridmix.AvgRecordFactory in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> if $z3 == 0 goto $r32 = new org.apache.hadoop.mapred.gridmix.AvgRecordFactory
The sink $r19 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r18) in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)> was called with values from the following sources:
- $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F) in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F)
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: float rateFactor> = $f0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>(r1, "StressJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: org.apache.hadoop.mapred.gridmix.StressJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.StressJobFactory,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.StressJobFactory: java.lang.Thread createReaderThread()>
		 -> return $r0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread> = $r9
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>(r1, "ReplayJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: org.apache.hadoop.mapred.gridmix.ReplayJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory: java.lang.Thread createReaderThread()>
		 -> return $r0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread> = $r9
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>(r1, "SerialJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: org.apache.hadoop.mapred.gridmix.SerialJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.SerialJobFactory,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.SerialJobFactory: java.lang.Thread createReaderThread()>
		 -> return $r0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread> = $r9
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r17 = r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread rThread>
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r18 = virtualinvoke $r17.<java.lang.Thread: java.lang.String getName()>()
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r19 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r18)
The sink $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" listStatus workers.") in method <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r11
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>(r1, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $i2 = r5.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $r54 = virtualinvoke $r53.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" listStatus workers.")
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r43, r10, r47, r14, r0, null, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $i2 = r5.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $r54 = virtualinvoke $r53.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" listStatus workers.")
The sink $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
The sink specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r0, "_temporary") in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingJobAttemptsPath(org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.staging.uuid", $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String getUploadUUID(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid> = $r8
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r9 = r0.<org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: java.lang.String uuid>
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r10 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path buildWorkPath(org.apache.hadoop.mapreduce.JobContext,java.lang.String)>($r24, $r9)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path buildWorkPath(org.apache.hadoop.mapreduce.JobContext,java.lang.String)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>($r2, r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>($r1, r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>(r2, r13, r3)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.lang.String cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths$lambda_getLocalTaskAttemptTempDir_0__54: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator,java.lang.String)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r6 = interfaceinvoke $r4.<com.google.common.cache.Cache: java.lang.Object get(java.lang.Object,java.util.concurrent.Callable)>(r1, $r5)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> $r7 = (org.apache.hadoop.fs.Path) $r6
	 -> <org.apache.hadoop.fs.s3a.commit.staging.Paths: org.apache.hadoop.fs.Path getLocalTaskAttemptTempDir(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.mapreduce.TaskAttemptID)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path taskAttemptWorkingPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)>
		 -> $r5 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>(r0, $r4)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingTaskAttemptsPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>($r7, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingTaskAttemptsPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r2 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r2 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingJobAttemptsPath(org.apache.hadoop.fs.Path)>(r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getPendingJobAttemptsPath(org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r2.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r0, "_temporary")
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/net/ConnectTimeoutException;", $r31) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy connectivityFailure> = $r13
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r31 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy connectivityFailure>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/net/ConnectTimeoutException;", $r31)
The sink $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
The sink $r3 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r3 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r8 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.user.agent.prefix", $r7)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r9 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", Hadoop/")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r12 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r11)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>()
The sink virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r9) in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.lang.String validateBase64String(java.lang.reflect.Field)>(r7)
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.lang.String validateBase64String(java.lang.reflect.Field)>
		 -> return $r10
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r9)
The sink specialinvoke $r21.<java.io.IOException: void <init>(java.lang.String,java.lang.Throwable)>(r19, r25) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl") in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r14 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" defined in ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.metadatastore.impl")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(": ")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getMetadataStore(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r21.<java.io.IOException: void <init>(java.lang.String,java.lang.Throwable)>(r19, r25)
The sink if r3 == null goto $r4 = new org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser in method <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.datatypes.jobproperties.parsers") in method <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.datatypes.jobproperties.parsers")
	 -> <org.apache.hadoop.tools.rumen.datatypes.JobProperties: java.util.Properties getAnonymizedValue(org.apache.hadoop.tools.rumen.state.StatePool,org.apache.hadoop.conf.Configuration)>
		 -> if r3 == null goto $r4 = new org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser
The sink r37 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r37 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r1)
The sink interfaceinvoke $r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r3, r0) in method <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job.seq", -1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob: int getJobSeqId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r3 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
	 -> <org.apache.hadoop.mapred.gridmix.Statistics: void addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> interfaceinvoke $r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r3, r0)
The sink $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.JarBuilder: void addNamedStream(java.util.jar.JarOutputStream,java.lang.String,java.io.InputStream)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>(r23, r11, r10)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> $r6 = virtualinvoke r1.<java.io.File: java.lang.String getName()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> $r7 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addFileStream(java.util.jar.JarOutputStream,java.lang.String,java.io.File)>
		 -> virtualinvoke r9.<org.apache.hadoop.streaming.JarBuilder: void addNamedStream(java.util.jar.JarOutputStream,java.lang.String,java.io.InputStream)>(r10, r8, r2)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addNamedStream(java.util.jar.JarOutputStream,java.lang.String,java.io.InputStream)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addNamedStream(java.util.jar.JarOutputStream,java.lang.String,java.io.InputStream)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if z2 == 0 goto $r20 = "Select" in method <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)> was called with values from the following sources:
- z2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", $z1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)>
		 -> z2 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", $z1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: org.apache.hadoop.fs.s3a.select.SelectInputStream executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest)>
		 -> if z2 == 0 goto $r20 = "Select"
The sink $r14 = virtualinvoke r0.<java.io.File: java.lang.String getName()>() in method <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $z0 = virtualinvoke r2.<org.apache.hadoop.streaming.StreamJob: boolean isLocalHadoop()>()
	 -> <org.apache.hadoop.streaming.StreamJob: boolean isLocalHadoop()>
		 -> return $z0
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r23, r11, r10, 0)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r14 = virtualinvoke r0.<java.io.File: java.lang.String getName()>()
The sink specialinvoke $r24.<java.io.IOException: void <init>(java.lang.String)>($r28) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L) in method <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> l0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.gen.bytes", -1L)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l0)
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
		 -> specialinvoke $r24.<java.io.IOException: void <init>(java.lang.String)>($r28)
The sink interfaceinvoke $r6.<java.util.List: void clear()>() in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>("batch threshold reached")
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $r6 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> interfaceinvoke $r6.<java.util.List: void clear()>()
The sink specialinvoke $r13.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>($r14, $r15, $r16, $r17) in method <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)> was called with values from the following sources:
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r27 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> specialinvoke $r29.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>($r13, $r14, r10, $z2, $z3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store> = r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.s3guard.MetadataStore,org.apache.hadoop.fs.s3a.S3AFileStatus,boolean,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> r15 = $r29
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Import: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r16 = virtualinvoke r15.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.ImportOperation: java.lang.Long execute()>
		 -> interfaceinvoke $r9.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>(r16, null)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r5 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.Path standardize(org.apache.hadoop.fs.Path)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r11 = r3.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r12 = interfaceinvoke $r11.<com.google.common.cache.Cache: java.lang.Object getIfPresent(java.lang.Object)>(r32)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> r33 = (org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry) $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> $r13 = virtualinvoke r33.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata dirListingMetadata>
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataEntry: org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata getDirListingMeta()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata,org.apache.hadoop.fs.s3a.s3guard.BulkOperationState)>
		 -> virtualinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: boolean put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: boolean put(org.apache.hadoop.fs.s3a.s3guard.PathMetadata)>
		 -> r4 = specialinvoke r3.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> $r8 = r4.<org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path path>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> r9 = virtualinvoke $r8.<org.apache.hadoop.fs.Path: java.net.URI toUri()>()
	 -> <org.apache.hadoop.fs.Path: java.net.URI toUri()>
		 -> $r1 = r0.<org.apache.hadoop.fs.Path: java.net.URI uri>
	 -> <org.apache.hadoop.fs.Path: java.net.URI toUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> $r15 = virtualinvoke r9.<java.net.URI: java.lang.String getHost()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DirListingMetadata: org.apache.hadoop.fs.Path childStatusToPathKey(org.apache.hadoop.fs.FileStatus)>
		 -> specialinvoke $r13.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>($r14, $r15, $r16, $r17)
The sink if $i4 == 2 goto $i7 = lengthof r6 in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $i4 = lengthof r6
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> if $i4 == 2 goto $i7 = lengthof r6
The sink virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyWorkstation(java.lang.String)>($r9) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- $r9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.proxy.workstation") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.proxy.workstation")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> virtualinvoke r4.<com.amazonaws.ClientConfiguration: void setProxyWorkstation(java.lang.String)>($r9)
The sink $r6 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Constructor getConstructor(java.lang.Class[])>($r5) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("auditreplay.command-parser.class", $r3, class "Lorg/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditCommandParser;") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("auditreplay.command-parser.class", $r3, class "Lorg/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditCommandParser;")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r6 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Constructor getConstructor(java.lang.Class[])>($r5)
The sink specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitData(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
The sink $r1 = virtualinvoke r0.<java.lang.reflect.Field: java.lang.'annotation'.Annotation getAnnotation(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$BooleanConfigurationValidatorAnnotation;") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: boolean validateBoolean(java.lang.reflect.Field)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $r4 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r5 = virtualinvoke $r4.<java.lang.Class: java.lang.reflect.Field[] getDeclaredFields()>()
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r6 = r5
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r7 = r6[i3]
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $z6 = virtualinvoke r0.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: boolean validateBoolean(java.lang.reflect.Field)>(r7)
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: boolean validateBoolean(java.lang.reflect.Field)>
		 -> $r1 = virtualinvoke r0.<java.lang.reflect.Field: java.lang.'annotation'.Annotation getAnnotation(java.lang.Class)>(class "Lorg/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations$BooleanConfigurationValidatorAnnotation;")
The sink $r7 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.committer.threads", 8) in method <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
		 -> i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.committer.threads", 8)
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r7 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink virtualinvoke r3.<java.util.concurrent.CompletableFuture: java.lang.Object join()>() in method <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.concurrent.CompletableFuture)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>("batch threshold reached")
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $r5 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $r2 = interfaceinvoke r0.<java.util.List: java.lang.Object[] toArray(java.lang.Object[])>($r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $r3 = (java.util.concurrent.CompletableFuture[]) $r2
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $r4 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture allOf(java.util.concurrent.CompletableFuture[])>($r3)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.concurrent.CompletableFuture)>($r4)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.concurrent.CompletableFuture)>
		 -> virtualinvoke r3.<java.util.concurrent.CompletableFuture: java.lang.Object join()>()
The sink $z15 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("staging") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z15 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("staging")
The sink specialinvoke r0.<java.lang.Thread: void <init>(java.lang.String)>(r2) in method <org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)> was called with values from the following sources:
- $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F) in method <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $f0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.submit.multiplier", 1.0F)
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.JobFactory: float rateFactor> = $f0
	 -> <org.apache.hadoop.mapred.gridmix.JobFactory: void <init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,org.apache.hadoop.tools.rumen.JobStoryProducer,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)>
		 -> $r9 = virtualinvoke r0.<org.apache.hadoop.mapred.gridmix.JobFactory: java.lang.Thread createReaderThread()>()
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory: java.lang.Thread createReaderThread()>
		 -> specialinvoke $r0.<org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>(r1, "ReplayJobFactory")
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: org.apache.hadoop.mapred.gridmix.ReplayJobFactory this$0> = r1
	 -> <org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread: void <init>(org.apache.hadoop.mapred.gridmix.ReplayJobFactory,java.lang.String)>
		 -> specialinvoke r0.<java.lang.Thread: void <init>(java.lang.String)>(r2)
The sink virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r11) in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)") in method <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
	on Path: 
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getValByRegex(java.lang.String)>("fs\\.azure\\.account\\.key\\.(.*)")
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> r2.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: java.util.Map storageAccountKeys> = $r4
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void validateStorageAccountKeys()>
		 -> return
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.fs.azurebfs.AbfsConfiguration: long validateLong(java.lang.reflect.Field)>(r7)
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: long validateLong(java.lang.reflect.Field)>
		 -> return $l3
	 -> <org.apache.hadoop.fs.azurebfs.AbfsConfiguration: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> virtualinvoke r7.<java.lang.reflect.Field: void set(java.lang.Object,java.lang.Object)>(r0, $r11)
The sink $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r1) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r1.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r1)
The sink r19 = staticinvoke <com.aliyun.oss.model.CannedAccessControlList: com.aliyun.oss.model.CannedAccessControlList valueOf(java.lang.String)>(r40) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r40 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.oss.acl.default", "")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r19 = staticinvoke <com.aliyun.oss.model.CannedAccessControlList: com.aliyun.oss.model.CannedAccessControlList valueOf(java.lang.String)>(r40)
The sink if $z0 == 0 goto return l1 in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.input-decompression.enable", 0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isInputCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isInputCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.input-decompression.enable", 0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isInputCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> if $z0 == 0 goto return l1
The sink $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l15) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/file")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l15)
The sink specialinvoke $r32.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r34) in method <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r34 = virtualinvoke $r33.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r34 = virtualinvoke $r33.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r34)
The sink $r18 = staticinvoke <org.apache.hadoop.util.concurrent.HadoopExecutors: java.util.concurrent.ExecutorService newFixedThreadPool(int,java.util.concurrent.ThreadFactory)>(i0, $r17) in method <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.committer.threads", 8) in method <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
		 -> i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.committer.threads", 8)
	 -> <org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter: java.util.concurrent.ExecutorService buildThreadPool(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r18 = staticinvoke <org.apache.hadoop.util.concurrent.HadoopExecutors: java.util.concurrent.ExecutorService newFixedThreadPool(int,java.util.concurrent.ThreadFactory)>(i0, $r17)
The sink specialinvoke $r0.<java.io.IOException: void <init>(java.lang.String)>($r7) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)> was called with values from the following sources:
- $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> $i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapred.listing.split.ratio", $i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getListingSplitRatio(org.apache.hadoop.conf.Configuration,int,int)>
		 -> return $i3
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: java.util.List splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext)>
		 -> staticinvoke <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>(i3, i1, i2)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", numMaps:")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i1)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(". Reduce numMaps or decrease split-ratio to proceed.")
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: void validateNumChunksUsing(int,int,int)>
		 -> specialinvoke $r0.<java.io.IOException: void <init>(java.lang.String)>($r7)
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $c1 = virtualinvoke r1.<java.lang.String: char charAt(int)>(i2)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $r2 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>($c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(char)>(c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
The sink r1 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.CopyListing: void validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads> = i0
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: int access$2000(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options> = r1
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $i4 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: org.apache.hadoop.tools.DistCpOptions options>
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> $i0 = r0.<org.apache.hadoop.tools.DistCpOptions: int numListstatusThreads>
	 -> <org.apache.hadoop.tools.DistCpOptions: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.DistCpContext: int getNumListstatusThreads()>
		 -> return $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r4.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i4
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void printStats()>()
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void printStats()>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: long getBytesToCopy()>()
	 -> <org.apache.hadoop.tools.SimpleCopyListing: long getBytesToCopy()>
		 -> return $l0
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: long getNumberOfPaths()>()
	 -> <org.apache.hadoop.tools.SimpleCopyListing: long getNumberOfPaths()>
		 -> return $l0
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.CopyListing: void validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.CopyListing: void validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> throw r40
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: long getBytesToCopy()>()
	 -> <org.apache.hadoop.tools.SimpleCopyListing: long getBytesToCopy()>
		 -> return $l0
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: long getNumberOfPaths()>()
	 -> <org.apache.hadoop.tools.SimpleCopyListing: long getNumberOfPaths()>
		 -> return $l0
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.CopyListing: void validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.CopyListing: void validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.tools.CopyListing simpleListing> = $r3
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r22
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: long getBytesToCopy()>()
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: long getBytesToCopy()>
		 -> return $l0
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: long getNumberOfPaths()>()
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: long getNumberOfPaths()>
		 -> return $l0
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.CopyListing: void validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.CopyListing: void validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r15) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)> was called with values from the following sources:
- r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> r15 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBClientFactory$DefaultDynamoDBClientFactory: java.lang.String getRegion(org.apache.hadoop.conf.Configuration,java.lang.String)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r15)
The sink specialinvoke $r19.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()> was called with values from the following sources:
- i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> specialinvoke $r19.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3)
The sink $r6 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i1) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f2 = $f0 * $f1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> i1 = (int) $f2
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r6 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i1)
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setMaxConnections(int)>($i0) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- $i0 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.connection.maximum", 32) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $i0 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.oss.connection.maximum", 32)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setMaxConnections(int)>($i0)
The sink if $z0 != 0 goto $r8 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>(r7) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r7 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("auditreplay.log-parse-regex", "^(?<timestamp>.+?) INFO [^:]+: (?<message>.+)$")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r7.<java.lang.String: boolean contains(java.lang.CharSequence)>("(?<timestamp>")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if $z0 != 0 goto $r8 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>(r7)
The sink $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>() in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getWorkingDirectory(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r1.<java.lang.String: boolean isEmpty()>()
The sink specialinvoke $r18.<java.io.OutputStreamWriter: void <init>(java.io.OutputStream,java.lang.String)>($r17, "UTF-8") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r6 = staticinvoke <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r1)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r7 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/realtimetrack.json")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> specialinvoke $r17.<java.io.FileOutputStream: void <init>(java.lang.String)>($r9)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> specialinvoke $r18.<java.io.OutputStreamWriter: void <init>(java.io.OutputStream,java.lang.String)>($r17, "UTF-8")
The sink $r12 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.tools.CopyListing simpleListing> = $r3
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r3 = $r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r8 = r6.<org.apache.hadoop.tools.GlobbedCopyListing: org.apache.hadoop.tools.CopyListing simpleListing>
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke $r8.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r7, r2)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r12 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.DistCpSync: org.apache.hadoop.tools.CopyFilter copyFilter> = $r3
	 -> <org.apache.hadoop.tools.DistCpSync: void <init>(org.apache.hadoop.tools.DistCpContext,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> r7 = $r4
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> $z1 = virtualinvoke r7.<org.apache.hadoop.tools.DistCpSync: boolean sync()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> $z0 = specialinvoke r0.<org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>()
	 -> <org.apache.hadoop.tools.DistCpSync: boolean preSyncCheck()>
		 -> throw $r50
	 -> <org.apache.hadoop.tools.DistCpSync: boolean sync()>
		 -> return 0
	 -> <org.apache.hadoop.tools.DistCp: void prepareFileListing(org.apache.hadoop.mapreduce.Job)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>(r2, r7)
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> specialinvoke $r2.<org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>($r4, $r5, r6)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.DistCpSync distCpSync> = r3
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r12 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r72 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z8) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean allowAuthoritativeMetadataStore> = $z5
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z8 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean allowAuthoritativeMetadataStore>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r72 = staticinvoke <java.lang.Boolean: java.lang.Boolean valueOf(boolean)>($z8)
The sink if r2 != null goto r26 = r2 in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> if r2 != null goto r26 = r2
The sink interfaceinvoke r38.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r39, $r41) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $r41 = virtualinvoke $r40.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r41 = virtualinvoke $r40.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> interfaceinvoke r38.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r39, $r41)
The sink $r5 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.S3AFileStatus: org.apache.hadoop.fs.Path getPath()>() in method <org.apache.hadoop.fs.s3a.s3guard.PathMetadata: void <init>(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.Tristate,boolean,long)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withLong(java.lang.String,long)>(":mod_time", l2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r13 = virtualinvoke $r12.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withString(java.lang.String,java.lang.String)>(":parent", r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r21 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.document.utils.ValueMap: com.amazonaws.services.dynamodbv2.document.utils.ValueMap withBoolean(java.lang.String,boolean)>(":is_dir", 1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r8 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>(r6, r19, r20, r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r0, $r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3> = $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r0, $r1, $r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2> = $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: void <init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation bootstrap$(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r2 = interfaceinvoke r1.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r3 = $r0.<org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: org.apache.hadoop.fs.s3a.Invoker$Operation cap2>
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> $r4 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r1, $r2, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r0, r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r17 = interfaceinvoke r4.<org.apache.hadoop.fs.s3a.Invoker$Operation: java.lang.Object execute()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r4 = $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: com.amazonaws.services.dynamodbv2.document.utils.ValueMap cap3>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> $r5 = virtualinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>($r2, $r3, $r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> $r5 = virtualinvoke $r4.<com.amazonaws.services.dynamodbv2.document.Table: com.amazonaws.services.dynamodbv2.document.ItemCollection scan(java.lang.String,java.lang.String,java.util.Map,java.util.Map)>(r1, r2, null, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection lambda$expiredFiles$10(java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.document.utils.ValueMap)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$lambda_expiredFiles_10__123: java.lang.Object execute()>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object once(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return r17
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object lambda$retry$4(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker$lambda_retry_4__189: java.lang.Object execute()>
		 -> return $r4
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r10 = (com.amazonaws.services.dynamodbv2.document.ItemCollection) $r9
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> return $r10
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $i1 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>(r1, l0, r3, r8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r14 = virtualinvoke r13.<com.amazonaws.services.dynamodbv2.document.ItemCollection: com.amazonaws.services.dynamodbv2.document.internal.IteratorSupport iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $r19 = interfaceinvoke r14.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r20 = (com.amazonaws.services.dynamodbv2.document.Item) $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> r22 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>(r20, $r21)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> r16 = virtualinvoke r0.<com.amazonaws.services.dynamodbv2.document.Item: java.lang.String getString(java.lang.String)>("etag")
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> specialinvoke $r18.<org.apache.hadoop.fs.s3a.S3AFileStatus: void <init>(long,long,org.apache.hadoop.fs.Path,long,java.lang.String,java.lang.String,java.lang.String)>(l4, l0, r15, l1, r19, r16, r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileStatus: void <init>(long,long,org.apache.hadoop.fs.Path,long,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileStatus: void <init>(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.s3a.Tristate,long,long,long,java.lang.String,java.lang.String,java.lang.String)>(r1, 0, $r5, l0, l1, l2, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.S3AFileStatus: void <init>(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.s3a.Tristate,long,long,long,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileStatus: java.lang.String eTag> = r4
	 -> <org.apache.hadoop.fs.s3a.S3AFileStatus: void <init>(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.s3a.Tristate,long,long,long,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileStatus: void <init>(long,long,org.apache.hadoop.fs.Path,long,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> r21 = $r18
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadataDynamoDBTranslation: org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata itemToPathMetadata(com.amazonaws.services.dynamodbv2.document.Item,java.lang.String)>
		 -> specialinvoke $r23.<org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata: void <init>(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.Tristate,boolean,boolean,long)>(r21, $r24, z18, z13, l8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DDBPathMetadata: void <init>(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.Tristate,boolean,boolean,long)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.PathMetadata: void <init>(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.Tristate,boolean,long)>(r1, r2, z0, l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadata: void <init>(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.Tristate,boolean,long)>
		 -> $r3 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.S3AFileStatus: org.apache.hadoop.fs.Path getPath()>()
	 -> <org.apache.hadoop.fs.FileStatus: org.apache.hadoop.fs.Path getPath()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.PathMetadata: void <init>(org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.Tristate,boolean,long)>
		 -> $r5 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.S3AFileStatus: org.apache.hadoop.fs.Path getPath()>()
The sink $r14 = virtualinvoke $r12.<java.nio.file.AccessDeniedException: java.lang.Throwable initCause(java.lang.Throwable)>(r1) in method <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)> was called with values from the following sources:
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r32 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r27.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r34, $r33, $r32, $r31, $r30, $r29, $r28)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r27
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r25)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r35 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r36 = virtualinvoke $r35.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r33 = staticinvoke <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>(r52, $r32)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r5 = virtualinvoke r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r13 = virtualinvoke r3.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> specialinvoke $r12.<java.nio.file.AccessDeniedException: void <init>(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r14 = virtualinvoke $r12.<java.nio.file.AccessDeniedException: java.lang.Throwable initCause(java.lang.Throwable)>(r1)
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r27 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>(r69)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r8 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.model.SSESpecification getSseSpecFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r10 = virtualinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r17 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[1] = $r17
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r15[0] = $r16
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> $r16 = r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>($r22)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r5 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r4)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void waitForTableActive(com.amazonaws.services.dynamodbv2.document.Table)>
		 -> virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>($r6, null, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$VoidOperation)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, r3, $r5)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r2 = virtualinvoke $r0.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r8 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r10)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r52 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r33 = staticinvoke <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>(r52, $r32)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r5 = virtualinvoke r3.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r4)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r13 = virtualinvoke r3.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> specialinvoke $r12.<java.nio.file.AccessDeniedException: void <init>(java.lang.String)>($r13)
	 -> <org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport: java.io.IOException translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException)>
		 -> $r14 = virtualinvoke $r12.<java.nio.file.AccessDeniedException: java.lang.Throwable initCause(java.lang.Throwable)>(r1)
The sink $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)> was called with values from the following sources:
- r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id") in method <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id")
	 -> <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r7 = virtualinvoke $r6.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.mapred.gridmix.JobMonitor: void submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r36 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i5) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r36 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i5)
The sink if z0 == 0 goto return in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> if z0 == 0 goto return
The sink $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)> was called with values from the following sources:
- $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z2 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.multiobjectdelete.enable", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean enableMultiObjectsDelete> = $z2
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r28.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r29, $r31)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Invoker s3guardInvoker> = $r28
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.Listing: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Listing listing> = $r34
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> throw $r2
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r28.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>($r29, $r31)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.io.retry.RetryPolicy retryPolicy> = r1
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Invoker s3guardInvoker> = $r28
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.Listing: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.Listing listing> = $r34
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.authoritative", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean allowAuthoritativeMetadataStore> = $z5
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.metadatastore.fail.on.write.error", 1)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: boolean failOnMetadataWriteError> = $z1
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r32.<org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>(r0, $r33)
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.WriteOperationHelper: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.WriteOperationHelper: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.WriteOperationHelper writeHelper> = $r32
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r34.<org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.Listing: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z1 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.errors.include.sql", 0)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean errorsIncludeSql> = $z1
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> l13 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l12, $r60)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r61.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>(l13)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(long)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = $r61
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> specialinvoke $r5.<org.apache.hadoop.fs.s3a.S3ADataBlocks$ArrayBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r3)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$ArrayBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks$ArrayBlockFactory: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory blockFactory> = $r54
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> z3 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.committer.magic.enabled", 0)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: boolean magicCommitEnabled> = z0
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r37.<org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>($r39, r0, r82, $r38)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.SignerManager: org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider delegationTokenProvider> = r3
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void <init>(java.lang.String,org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.auth.SignerManager signerManager> = $r37
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
- $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1) in method <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> $z0 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.select.enabled", 1)
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> r0.<org.apache.hadoop.fs.s3a.select.SelectBinding: boolean enabled> = $z0
	 -> <org.apache.hadoop.fs.s3a.select.SelectBinding: void <init>(org.apache.hadoop.fs.s3a.WriteOperationHelper)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.select.SelectBinding selectBinding> = $r50
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r64 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: java.util.Collection getAuthoritativePaths(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.conf.Configuration getConf()>()
The sink virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.input.fileinputformat.inputdir", $r3) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.input-path") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.input-path")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)>
		 -> virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("mapreduce.input.fileinputformat.inputdir", $r3)
The sink if $b1 <= 0 goto $z0 = interfaceinvoke r14.<java.util.Iterator: boolean hasNext()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)> was called with values from the following sources:
- l0 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.background.sleep", 25L, $r6) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> l0 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.background.sleep", 25L, $r6)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> $b1 = l0 cmp 0L
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: int innerPrune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String,com.amazonaws.services.dynamodbv2.document.ItemCollection)>
		 -> if $b1 <= 0 goto $z0 = interfaceinvoke r14.<java.util.Iterator: boolean hasNext()>()
The sink $z4 = virtualinvoke r20.<org.apache.hadoop.fs.Path: boolean equals(java.lang.Object)>(r38) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $r19 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.work.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r18.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r19)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r20 = $r18
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $r33 = virtualinvoke r20.<org.apache.hadoop.fs.Path: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.Path: java.lang.String toString()>
		 -> return $r9
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> $z4 = virtualinvoke r20.<org.apache.hadoop.fs.Path: boolean equals(java.lang.Object)>(r38)
The sink if i3 >= i1 goto return in method <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,java.lang.Long,int)> was called with values from the following sources:
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.datagenerator.randomtext.listsize", 200) in method <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: int getRandomTextDataGeneratorListSize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: int getRandomTextDataGeneratorListSize(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.datagenerator.randomtext.listsize", 200)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: int getRandomTextDataGeneratorListSize(org.apache.hadoop.conf.Configuration)>
		 -> return $i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$RandomTextDataMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r3.<org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,int)>(i0, i1)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,int)>
		 -> specialinvoke r0.<org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,java.lang.Long,int)>(i0, $r1, i1)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void <init>(int,java.lang.Long,int)>
		 -> if i3 >= i1 goto return
The sink $r8 = interfaceinvoke $r6.<java.util.Map: java.lang.Object get(java.lang.Object)>($r7) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r7 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r8 = interfaceinvoke $r6.<java.util.Map: java.lang.Object get(java.lang.Object)>($r7)
The sink specialinvoke $r7.<java.io.IOException: void <init>(java.lang.String)>($r11) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)> was called with values from the following sources:
- l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l1 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> l2 = virtualinvoke $r4.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l1, $r3)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l2)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat: org.apache.hadoop.mapreduce.RecordReader createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)>
		 -> specialinvoke $r7.<java.io.IOException: void <init>(java.lang.String)>($r11)
The sink specialinvoke $r3.<java.net.URI: void <init>(java.lang.String)>(r1) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.endpoint", "s3.amazonaws.com")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void logDnsLookup(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<java.net.URI: void <init>(java.lang.String)>(r1)
The sink $r61 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i2) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r61 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i2)
The sink $r22 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r21) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)> was called with values from the following sources:
- $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r3 = virtualinvoke $r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.table")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r13 = specialinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r12, $r11, null, $r10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r21 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r22 = virtualinvoke $r20.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r21)
The sink $r8 = virtualinvoke $r7.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>() in method <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15) in method <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.saskey.cacheentry.expiry.period", l1, $r15)
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l7 = l4
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> l8 = $l7
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>(l8, "SASKEY")
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes> = l0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache> = $r10
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = r1.<org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache>
	 -> <org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r11.<org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $l2 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r5.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterWrite(long,java.util.concurrent.TimeUnit)>($l2, $r6)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke $r7.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2) in method <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.azure.sas.expiry.period", 90L, $r2)
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod> = $l0
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r0.<org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>()
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> $l0 = r0.<org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long sasKeyExpiryPeriod>
	 -> <org.apache.hadoop.fs.azure.SASKeyGeneratorImpl: long getSasKeyExpiryPeriod()>
		 -> return $l0
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>($l0, "SASKEY")
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes> = l0
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void <init>(long,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache> = $r3
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = r0.<org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: org.apache.hadoop.fs.azure.CachingAuthorizer cache>
	 -> <org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke $r4.<org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>(r1)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $l2 = r0.<org.apache.hadoop.fs.azure.CachingAuthorizer: long cacheEntryExpiryPeriodInMinutes>
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke $r5.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder expireAfterWrite(long,java.util.concurrent.TimeUnit)>($l2, $r6)
	 -> <org.apache.hadoop.fs.azure.CachingAuthorizer: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke $r7.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
The sink $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r3 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(float)>(f1)
The sink virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setEnvironment(java.util.Map)>(r48) in method <org.apache.hadoop.tools.dynamometer.Client: boolean run()> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setEnvironment(java.util.Map)>(r48)
- r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11) in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r12 = virtualinvoke $r10.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String,java.lang.String[])>("yarn.application.classpath", $r11)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> r20 = r12[i1]
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r22 = virtualinvoke r20.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> virtualinvoke r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r16 = virtualinvoke r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> return $r16
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>($r36, $r37)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> virtualinvoke r36.<org.apache.hadoop.yarn.api.records.ContainerLaunchContext: void setEnvironment(java.util.Map)>(r48)
The sink $r60 = virtualinvoke $r59.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i20) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592) in method <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i7 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.swift.partsize", 4718592)
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB> = $i7
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $i20 = r0.<org.apache.hadoop.fs.swift.http.SwiftRestClient: int partSizeKB>
	 -> <org.apache.hadoop.fs.swift.http.SwiftRestClient: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r60 = virtualinvoke $r59.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i20)
The sink $r8 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>() in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.arn", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: java.lang.String arn> = $r2
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<java.lang.Object: java.lang.Class getClass()>()
The sink $r22 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r9, r0) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $r22 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>(r9, r0)
The sink $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()> was called with values from the following sources:
- r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id") in method <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> r44 = virtualinvoke $r43.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("gridmix.job.original-job-id")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r48 = virtualinvoke $r47.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r44)
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r49 = virtualinvoke $r48.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\' is being simulated as \'")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r53 = virtualinvoke $r49.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($r52)
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r54 = virtualinvoke $r53.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\'")
	 -> <org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask: void run()>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if $b2 < 0 goto $d1 = d0 - d6 in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)> was called with values from the following sources:
- $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f7 = $f3 * $f4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d5 = (double) f7
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of missing blocks", "Hadoop:service=NameNode,name=FSNamesystem", "MissingBlocks", $d5, $d4, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $b2 = d0 cmpl d4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> if $b2 < 0 goto $d1 = d0 - d6
- $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f0 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.datanode-min-fraction", 0.99F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f2 = $f0 * $f1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> i1 = (int) $f2
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d2 = (double) i1
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of live DataNodes", "Hadoop:service=NameNode,name=FSNamesystemState", "NumLiveDataNodes", $d2, $d1, 0, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $b2 = d0 cmpl d4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> if $b2 < 0 goto $d1 = d0 - d6
- $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f6 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.underreplicated-blocks-max-fraction", 0.01F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f8 = $f5 * $f6
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $d8 = (double) f8
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> staticinvoke <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>("Number of under replicated blocks", "Hadoop:service=NameNode,name=FSNamesystemState", "UnderReplicatedBlocks", $d8, $d7, 1, r8, r0, r4)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> $b2 = d0 cmpl d4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger)>
		 -> if $b2 < 0 goto $d1 = d0 - d6
The sink r12 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequest: org.apache.hadoop.yarn.api.records.ReservationRequest newInstance(org.apache.hadoop.yarn.api.records.Resource,int,int,long)>(r2, $i16, 1, $l15) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5) in method <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("resourceestimator.timeInterval", 5)
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> $l15 = (long) i0
	 -> <org.apache.hadoop.resourceestimator.solver.impl.BaseSolver: org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)>
		 -> r12 = staticinvoke <org.apache.hadoop.yarn.api.records.ReservationRequest: org.apache.hadoop.yarn.api.records.ReservationRequest newInstance(org.apache.hadoop.yarn.api.records.Resource,int,int,long)>(r2, $i16, 1, $l15)
The sink specialinvoke r1.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStop()>() in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>(r14)
	 -> <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
		 -> throw r12
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>($r15, r0)
	 -> <org.apache.hadoop.service.ServiceOperations: java.lang.Exception stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)>
		 -> staticinvoke <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>(r0)
	 -> <org.apache.hadoop.service.ServiceOperations: void stop(org.apache.hadoop.service.Service)>
		 -> interfaceinvoke r0.<org.apache.hadoop.service.Service: void stop()>()
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> $r5 = specialinvoke r0.<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>($r4)
	 -> <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
		 -> return r3
	 -> <org.apache.hadoop.service.AbstractService: void stop()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStop()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStop()>
		 -> specialinvoke r1.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStop()>()
The sink specialinvoke $r6.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>($r11) in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(char)>(34)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> specialinvoke $r6.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>($r11)
The sink if r4 != null goto r5 = virtualinvoke r4.<java.lang.String: java.lang.String[] split(java.lang.String)>(" ") in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script") in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script")
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> if r4 != null goto r5 = virtualinvoke r4.<java.lang.String: java.lang.String[] split(java.lang.String)>(" ")
The sink $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $z0 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isEmpty(java.lang.CharSequence)>(r1)
The sink specialinvoke $r13.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()> was called with values from the following sources:
- i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> i3 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.timer.window.size", 100)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void registerSchedulerMetrics()>
		 -> specialinvoke $r13.<com.codahale.metrics.SlidingWindowReservoir: void <init>(int)>(i3)
The sink if $z4 == 0 goto $r42 = new java.io.FileNotFoundException in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- $z4 = virtualinvoke $r41.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.s3guard.ddb.table.create", 0) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $z4 = virtualinvoke $r41.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.s3a.s3guard.ddb.table.create", 0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> if $z4 == 0 goto $r42 = new java.io.FileNotFoundException
The sink $r10 = virtualinvoke r9.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r9, $r8, r2, $r6)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r13, $r12, r2, $r10)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r17, $r16, r2, $r14)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r21, $r20, r2, $r18)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r24, $r23, r2, $r22)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> throw r155
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r27 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r30 = virtualinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r37 = specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.lang.String getAMClassPathEnv()>
		 -> $r10 = virtualinvoke r9.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink r12 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r6) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r6 = virtualinvoke r5.<java.lang.String: java.lang.String[] split(java.lang.String)>(":")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r8 = r6[1]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>($r9, $r8, $r7)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r12 = virtualinvoke r9.<org.apache.hadoop.conf.Configuration: java.lang.Class getClassByName(java.lang.String)>(r6)
The sink if r2 == null goto $r3 = virtualinvoke r0.<org.apache.hadoop.tools.DistCp: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name") in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
	on Path: 
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name")
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> if r2 == null goto $r3 = virtualinvoke r0.<org.apache.hadoop.tools.DistCp: org.apache.hadoop.conf.Configuration getConf()>()
The sink $l2 = virtualinvoke $r3.<java.lang.Long: long longValue()>() in method <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r13 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l22)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> virtualinvoke r5.<java.util.ArrayList: boolean add(java.lang.Object)>($r13)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $r34 = specialinvoke r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>(r5)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> $r2 = virtualinvoke r0.<java.util.ArrayList: java.lang.Object get(int)>(i3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> $r3 = (java.lang.Long) $r2
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: long[] toLongArray(java.util.ArrayList)>
		 -> $l2 = virtualinvoke $r3.<java.lang.Long: long longValue()>()
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSStatus500Exception;", $r37) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy connectivityFailure> = $r13
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r37 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy connectivityFailure>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSStatus500Exception;", $r37)
The sink $r6 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()> was called with values from the following sources:
- i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.bucket.probe", 2) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()>
		 -> i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.bucket.probe", 2)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void doBucketProbing()>
		 -> $r6 = staticinvoke <java.lang.Integer: java.lang.Integer valueOf(int)>(i0)
The sink $r9 = interfaceinvoke r5.<java.util.Iterator: java.lang.Object next()>() in method <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r6 = $r4
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> interfaceinvoke r8.<java.util.List: boolean add(java.lang.Object)>(r6)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r14.<org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>(r8, r11)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List sourcePaths> = r2
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r16 = virtualinvoke $r14.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withOverwrite(boolean)>($z2)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withOverwrite(boolean)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r17 = virtualinvoke $r16.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withSyncFolder(boolean)>($z3)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withSyncFolder(boolean)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $r3 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: java.util.List sourcePaths> = $r3
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> $r2 = r0.<org.apache.hadoop.tools.DistCpOptions: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> $r3 = staticinvoke <java.util.Collections: java.util.List unmodifiableList(java.util.List)>($r2)
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> return $r3
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: java.util.List sourcePaths> = $r2
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = virtualinvoke r2.<org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> r5 = interfaceinvoke $r4.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.tools.GlobbedCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = interfaceinvoke r5.<java.util.Iterator: java.lang.Object next()>()
The sink $z4 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("staging") in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z4 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("staging")
The sink if $z0 == 0 goto r9 = virtualinvoke r2.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path)>(r0) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: java.io.InputStream getPossiblyDecompressedInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long)> was called with values from the following sources:
- $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.compression-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: boolean isCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration)>
		 -> return $z0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: java.io.InputStream getPossiblyDecompressedInputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long)>
		 -> if $z0 == 0 goto r9 = virtualinvoke r2.<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path)>(r0)
The sink r11 = virtualinvoke $r9.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withTags(java.util.Collection)>($r10) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r15 = (java.lang.String) $r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>(r16)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> return r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void createTable(com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput)>
		 -> r11 = virtualinvoke $r9.<com.amazonaws.services.dynamodbv2.model.CreateTableRequest: com.amazonaws.services.dynamodbv2.model.CreateTableRequest withTags(java.util.Collection)>($r10)
The sink if $b6 <= 0 goto $b4 = l8 cmp 0L in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $b6 = l9 cmp 0L
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> if $b6 <= 0 goto $b4 = l8 cmp 0L
The sink specialinvoke $r56.<com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput: void <init>(java.lang.Long,java.lang.Long)>($r57, $r58) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()> was called with values from the following sources:
- l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l9 = virtualinvoke $r55.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.write", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r58 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l9)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke $r56.<com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput: void <init>(java.lang.Long,java.lang.Long)>($r57, $r58)
- l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> l8 = virtualinvoke $r54.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.ddb.table.capacity.read", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> $r57 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l8)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> specialinvoke $r56.<com.amazonaws.services.dynamodbv2.model.ProvisionedThroughput: void <init>(java.lang.Long,java.lang.Long)>($r57, $r58)
The sink l14 = staticinvoke <java.lang.Math: long min(long,long)>(l5, l11) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)> was called with values from the following sources:
- l10 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L) in method <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l10 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.sleep.interval", 5L)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l11 = virtualinvoke $r7.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>(l10, $r6)
	 -> <org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer: void setup(org.apache.hadoop.mapreduce.Reducer$Context)>
		 -> l14 = staticinvoke <java.lang.Math: long min(long,long)>(l5, l11)
The sink $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r9 = virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("createfile.file-parent-path", "/tmp/createFileMapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath> = $r9
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r23 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: java.lang.String fileParentPath>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r24 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r23)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r25 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/mapper")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r26 = virtualinvoke $r25.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i12)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r27 = virtualinvoke $r26.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r27)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r38 = virtualinvoke $r37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/file")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r39 = virtualinvoke $r38.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l15)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r40 = virtualinvoke $r39.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("random") in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("random")
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z1 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("random")
The sink $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r72) in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN") in method <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r72 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("adl.events.tracking.clustername", "UNKNOWN")
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r37 = virtualinvoke $r36.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r72)
The sink if $i0 <= $i1 goto return in method <org.apache.hadoop.tools.SimpleCopyListing: void addToFileListing(java.util.List,org.apache.hadoop.tools.SimpleCopyListing$FileStatusInfo,org.apache.hadoop.io.SequenceFile$Writer)> was called with values from the following sources:
- $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i1 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.simplelisting.file.status.size", 1000)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i2 = staticinvoke <java.lang.Math: int max(int,int)>(1, $i1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit> = $i2
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r58
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> return $r11
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r3, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r9 = virtualinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r45 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>(r44)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path)>
		 -> return $r6
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r14 = specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>(r13, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.fs.Path computeSourceRootPath(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.tools.DistCpContext)>
		 -> return $r12
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r4.<org.apache.hadoop.tools.SimpleCopyListing: void addToFileListing(java.util.List,org.apache.hadoop.tools.SimpleCopyListing$FileStatusInfo,org.apache.hadoop.io.SequenceFile$Writer)>(r1, $r52, r43)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void addToFileListing(java.util.List,org.apache.hadoop.tools.SimpleCopyListing$FileStatusInfo,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> $i1 = r2.<org.apache.hadoop.tools.SimpleCopyListing: int fileStatusLimit>
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void addToFileListing(java.util.List,org.apache.hadoop.tools.SimpleCopyListing$FileStatusInfo,org.apache.hadoop.io.SequenceFile$Writer)>
		 -> if $i0 <= $i1 goto return
The sink if $b1 > 0 goto $z0 = 0 in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)> was called with values from the following sources:
- $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>($f2)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>(f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $b1 = f0 cmpg 1.0F
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> if $b1 > 0 goto $z0 = 0
- $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>($f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $b1 = f0 cmpg 1.0F
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> if $b1 > 0 goto $z0 = 0
The sink r13 = staticinvoke <org.apache.hadoop.util.Shell: java.lang.String execCommand(java.lang.String[])>(r7) in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script") in method <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r4 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.azure.shellkeyprovider.script")
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String[] split(java.lang.String)>(" ")
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> $r6 = staticinvoke <java.util.Arrays: java.lang.Object[] copyOf(java.lang.Object[],int)>(r5, $i1)
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r7 = (java.lang.String[]) $r6
	 -> <org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider: java.lang.String getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)>
		 -> r13 = staticinvoke <org.apache.hadoop.util.Shell: java.lang.String execCommand(java.lang.String[])>(r7)
The sink specialinvoke $r1.<java.util.NoSuchElementException: void <init>(java.lang.String)>($r5) in method <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration)>
		 -> r8 = staticinvoke <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>(r1)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $c1 = virtualinvoke r1.<java.lang.String: char charAt(int)>(i2)
	 -> <org.apache.hadoop.tools.util.DistCpUtils: java.util.EnumSet unpackAttributes(java.lang.String)>
		 -> $r2 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>($c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> $r4 = virtualinvoke $r3.<java.lang.StringBuilder: java.lang.StringBuilder append(char)>(c1)
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$FileAttribute: org.apache.hadoop.tools.DistCpOptions$FileAttribute getAttribute(char)>
		 -> specialinvoke $r1.<java.util.NoSuchElementException: void <init>(java.lang.String)>($r5)
The sink if $z4 == 0 goto $r24 = r0.<org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.conf.Configuration val$conf> in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()> was called with values from the following sources:
- $z4 = virtualinvoke $r38.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.highram-emulation.enable", 1) in method <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> $z4 = virtualinvoke $r38.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("gridmix.highram-emulation.enable", 1)
	 -> <org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.mapreduce.Job run()>
		 -> if $z4 == 0 goto $r24 = r0.<org.apache.hadoop.mapred.gridmix.GridmixJob$2: org.apache.hadoop.conf.Configuration val$conf>
The sink $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.ssl.channel.mode") in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r15 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" is not a valid value for ")
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("fs.s3a.ssl.channel.mode")
The sink if $b28 < 0 goto $r10 = new org.apache.hadoop.fs.aliyun.oss.ReadBuffer in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)> was called with values from the following sources:
- $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> $l1 = virtualinvoke r8.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.oss.multipart.download.size", 524288L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize> = $l1
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void <init>(org.apache.hadoop.conf.Configuration,java.util.concurrent.ExecutorService,int,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore,java.lang.String,java.lang.Long,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>(0L)
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l36 = r0.<org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: long downloadPartSize>
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $l26 = l42 + l36
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> l43 = $l26 - 1L
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> $b28 = l43 cmp $l27
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream: void reopen(long)>
		 -> if $b28 < 0 goto $r10 = new org.apache.hadoop.fs.aliyun.oss.ReadBuffer
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/io/FileNotFoundException;", r5) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.consistency.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.consistency.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Ljava/io/FileNotFoundException;", r5)
The sink $r2 = interfaceinvoke r0.<java.util.List: java.lang.Object[] toArray(java.lang.Object[])>($r1) in method <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)> was called with values from the following sources:
- $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L) in method <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> $l0 = virtualinvoke $r13.<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>("fs.s3a.block.size", 33554432L)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: long blocksize> = $l0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void <init>(org.apache.hadoop.fs.s3a.impl.StoreContext,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.s3a.impl.OperationCallbacks,int)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r17 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: long innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke r17.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void executeOnlyOnce()>()
	 -> <org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation: void executeOnlyOnce()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.Long execute()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>()
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r6 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r8 = specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>($r7)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.lang.String maybeAddTrailingSlash(java.lang.String)>
		 -> return r0
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>(r18, r17)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void queueToDelete(org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> r21 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>(r14, r17, r18, r19, r20)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.impl.AbstractStoreOperation: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>(r0, r9, r10, r6, r1, r11, r12)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> specialinvoke $r7.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>($r0, $r1, $r2, $r3, $r4, $r5, $r6)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> $r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: org.apache.hadoop.fs.s3a.impl.RenameOperation cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: void <init>(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation$lambda_initiateCopy_0__181: java.util.concurrent.Callable bootstrap$(org.apache.hadoop.fs.s3a.impl.RenameOperation,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,org.apache.hadoop.fs.Path,java.lang.String)>
		 -> return $r7
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> $r14 = staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>($r8, $r13)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>(r1)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> r0.<org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.Callable call> = r1
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void <init>(java.util.concurrent.Callable)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> $r3 = staticinvoke <java.util.concurrent.CompletableFuture: java.util.concurrent.CompletableFuture supplyAsync(java.util.function.Supplier,java.util.concurrent.Executor)>($r0, r2)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: java.util.concurrent.CompletableFuture submit(java.util.concurrent.Executor,java.util.concurrent.Callable)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.concurrent.CompletableFuture initiateCopy(org.apache.hadoop.fs.s3a.S3ALocatedFileStatus,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> interfaceinvoke $r44.<java.util.List: boolean add(java.lang.Object)>(r21)
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> $r44 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void recursiveDirectoryRename()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>("batch threshold reached")
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> $r5 = r0.<org.apache.hadoop.fs.s3a.impl.RenameOperation: java.util.List activeCopies>
	 -> <org.apache.hadoop.fs.s3a.impl.RenameOperation: void completeActiveCopies(java.lang.String)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>($r5)
	 -> <org.apache.hadoop.fs.s3a.impl.CallableSupplier: void waitForCompletion(java.util.List)>
		 -> $r2 = interfaceinvoke r0.<java.util.List: java.lang.Object[] toArray(java.lang.Object[])>($r1)
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z3, "STS endpoint is set to %s but no signing region was provided", r5) in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)> was called with values from the following sources:
- r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r24 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r7)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z3, "STS endpoint is set to %s but no signing region was provided", r5)
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z3, "STS endpoint is set to %s but no signing region was provided", r5)
- r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r23 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r26 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>(r1, $r39, $r25, r23, r24)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r3, r2, r4, r5)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z3, "STS endpoint is set to %s but no signing region was provided", r5)
- r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "") in method <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r4 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.sts.endpoint.region", "")
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> $r6 = staticinvoke <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>(r5, r2, r3, r4)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> $z3 = staticinvoke <org.apache.commons.lang3.StringUtils: boolean isNotEmpty(java.lang.CharSequence)>(r7)
	 -> <org.apache.hadoop.fs.s3a.auth.STSClientFactory: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z3, "STS endpoint is set to %s but no signing region was provided", r5)
The sink $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4) in method <org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)> was called with values from the following sources:
- $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ") in method <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> $r28 = virtualinvoke $r27.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("mapreduce.job.acl-view-job", " ")
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> interfaceinvoke r2.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>("JOB_ACL_VIEW", $r28)
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> return r2
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $l12 = virtualinvoke $r57.<org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>(r48)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>
		 -> $r3 = interfaceinvoke r0.<java.util.Map: java.lang.Object get(java.lang.Object)>($r2)
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>
		 -> $r4 = (java.lang.String) $r3
	 -> <org.apache.hadoop.tools.dynamometer.DynoResource: long getTimestamp(java.util.Map)>
		 -> $l0 = staticinvoke <java.lang.Long: long parseLong(java.lang.String)>($r4)
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r39) in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r39)
The sink $r10 = virtualinvoke r2.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.dynamometer.Client: void addFileToZipRecursively(java.io.File,java.io.File,java.util.zip.ZipOutputStream)> was called with values from the following sources:
- $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66) in method <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> $l9 = virtualinvoke $r65.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("___temp___", 0L, $r66)
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> r2.<org.apache.hadoop.tools.dynamometer.Client: long workloadStartDelayMs> = $l9
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean init(java.lang.String[])>
		 -> return 1
	 -> <org.apache.hadoop.tools.dynamometer.Client: int run(java.lang.String[])>
		 -> z0 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: boolean run()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r40 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r43 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> $r45 = virtualinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: boolean run()>
		 -> r48 = specialinvoke r1.<org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>()
	 -> <org.apache.hadoop.tools.dynamometer.Client: java.util.Map setupRemoteResourcesGetEnv()>
		 -> specialinvoke r3.<org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>($r42, $r41, r2, $r39)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> $r136 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> $r34 = virtualinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.dynamometer.Client: void setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[])>
		 -> specialinvoke r27.<org.apache.hadoop.tools.dynamometer.Client: void addFileToZipRecursively(java.io.File,java.io.File,java.util.zip.ZipOutputStream)>($r62, r149, r147)
	 -> <org.apache.hadoop.tools.dynamometer.Client: void addFileToZipRecursively(java.io.File,java.io.File,java.util.zip.ZipOutputStream)>
		 -> $r10 = virtualinvoke r2.<org.apache.hadoop.tools.dynamometer.Client: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r12) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r5 = virtualinvoke r4.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.user.agent.prefix", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", ")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $r11 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r12)
The sink $r9 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("hadoop.tmp.dir")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: java.io.File createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)>
		 -> $r9 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8)
The sink specialinvoke $r26.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>(r33) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("]")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r33 = virtualinvoke $r24.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> specialinvoke $r26.<java.lang.IllegalArgumentException: void <init>(java.lang.String)>(r33)
The sink virtualinvoke $r36.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void setDaemon(boolean)>(1) in method <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L) in method <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> $l0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("gridmix.emulators.resource-usage.sleep-duration", 100L)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: long sleepTime> = $l0
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner: void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext,org.apache.hadoop.tools.rumen.ResourceUsageMetrics)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher> = $r44
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r35 = r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner matcher>
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> specialinvoke $r45.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>($r47, $r35)
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r0.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: org.apache.hadoop.mapred.gridmix.Progressive progress> = r2
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void <init>(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> return
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter reporter> = $r45
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r36 = r6.<org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter reporter>
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper: void setup(org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> virtualinvoke $r36.<org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter: void setDaemon(boolean)>(1)
The sink $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.credentials.provider")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils: com.aliyun.oss.common.auth.CredentialsProvider getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r1)
The sink if i5 != 1 goto $z9 = 0 in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if i5 != 1 goto $z9 = 0
The sink virtualinvoke r0.<com.amazonaws.ClientConfiguration: void setSignerOverride(java.lang.String)>(r4) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.signing-algorithm", "") in method <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.signing-algorithm", "")
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> virtualinvoke r0.<com.amazonaws.ClientConfiguration: void setSignerOverride(java.lang.String)>(r4)
The sink $r29 = virtualinvoke $r28.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r10 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r29 = virtualinvoke $r28.<java.lang.StringBuilder: java.lang.String toString()>()
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r10 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r29 = virtualinvoke $r28.<java.lang.StringBuilder: java.lang.String toString()>()
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r10 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r28 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r11)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $r29 = virtualinvoke $r28.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r62 = virtualinvoke $r61.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> i2 = virtualinvoke $r11.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("createfile.duration-min", -1)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r61 = virtualinvoke $r60.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i2)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r62 = virtualinvoke $r61.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("]") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r24 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("]")
The sink $z0 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: boolean isInState(org.apache.hadoop.service.Service$STATE)>(r1) in method <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceState(org.apache.hadoop.service.Service$STATE)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;") in method <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("fs.s3a.delegation.token.binding", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding;", class "Lorg/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding;")
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<java.lang.Class: java.lang.Object newInstance()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = (org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding) $r3
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding tokenBinding> = $r4
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.impl.StoreContext getStoreContext()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: org.apache.hadoop.fs.s3a.auth.delegation.DelegationOperations getPolicyProvider()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: java.net.URI getCanonicalUri()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: java.net.URI getCanonicalUri()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceInit(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> $z1 = virtualinvoke r1.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r9)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void bindAWSClient(java.net.URI,boolean)>
		 -> virtualinvoke r25.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void start()>()
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.service.AbstractService: boolean isInState(org.apache.hadoop.service.Service$STATE)>
		 -> return $z0
	 -> <org.apache.hadoop.service.AbstractService: void start()>
		 -> virtualinvoke r0.<org.apache.hadoop.service.AbstractService: void serviceStart()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void serviceStart()>()
	 -> <org.apache.hadoop.service.AbstractService: void serviceStart()>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void serviceStart()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.security.token.Token selectTokenFromFSOwner()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: org.apache.hadoop.security.token.Token selectTokenFromFSOwner()>
		 -> return $r6
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void bindToAnyDelegationToken()>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void deployUnbonded()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void deployUnbonded()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens: void requireServiceStarted()>()
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceStarted()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceState(org.apache.hadoop.service.Service$STATE)>($r1)
	 -> <org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: void requireServiceState(org.apache.hadoop.service.Service$STATE)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.auth.delegation.AbstractDTService: boolean isInState(org.apache.hadoop.service.Service$STATE)>(r1)
The sink if $b10 == 0 goto $r21 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: java.util.List files> in method <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> $f2 = $f1 / f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> l1 = (long) $f2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: long getUncompressedInputBytes(long,org.apache.hadoop.conf.Configuration)>
		 -> return l1
	 -> <org.apache.hadoop.mapred.gridmix.LoadJob: void buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)>
		 -> $r12 = virtualinvoke r39.<org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>(r3, l6, 3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> l22 = staticinvoke <java.lang.Math: long min(long,long)>(l21, $l3)
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l6 = $l5 + l22
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart> = $l6
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l8 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: long currentStart>
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $l9 = $l7 - $l8
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> $b10 = $l9 cmp 0L
	 -> <org.apache.hadoop.mapred.gridmix.InputStriper: org.apache.hadoop.mapreduce.lib.input.CombineFileSplit splitFor(org.apache.hadoop.mapred.gridmix.FilePool,long,int)>
		 -> if $b10 == 0 goto $r21 = r8.<org.apache.hadoop.mapred.gridmix.InputStriper: java.util.List files>
The sink virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyPassword(java.lang.String)>(r35) in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)> was called with values from the following sources:
- r35 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.password") in method <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
	on Path: 
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> r35 = virtualinvoke r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.oss.proxy.password")
	 -> <org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)>
		 -> virtualinvoke r4.<com.aliyun.oss.ClientConfiguration: void setProxyPassword(java.lang.String)>(r35)
The sink if $z2 != 0 goto r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean autoThrottlingEnabled> = 0 in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()> was called with values from the following sources:
- $z1 = virtualinvoke $r17.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.selfthrottling.enable", 1) in method <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z1 = virtualinvoke $r17.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("fs.azure.selfthrottling.enable", 1)
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean selfThrottlingEnabled> = $z1
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> $z2 = r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean selfThrottlingEnabled>
	 -> <org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: void configureAzureStorageSession()>
		 -> if $z2 != 0 goto r0.<org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: boolean autoThrottlingEnabled> = 0
The sink specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.listing.file.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void trackMissing(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
The sink if $r1 == null goto $z0 = 0 in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.input-path") in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("auditreplay.input-path")
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper: boolean verifyConfigurations(org.apache.hadoop.conf.Configuration)>
		 -> if $r1 == null goto $z0 = 0
The sink $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l10) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)> was called with values from the following sources:
- $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l1 = virtualinvoke $r7.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("start_timestamp_ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs> = $l1
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $l9 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: long startTimestampMs>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> l10 = $l9 - l8
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper: void map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapreduce.Mapper$Context)>
		 -> $r55 = virtualinvoke $r54.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(l10)
The sink $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file") in method <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.filters.file")
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r3.<org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> specialinvoke $r1.<java.io.File: void <init>(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> r0.<org.apache.hadoop.tools.RegexCopyFilter: java.io.File filtersFile> = $r1
	 -> <org.apache.hadoop.tools.RegexCopyFilter: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getDefaultCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.tools.CopyFilter: org.apache.hadoop.tools.CopyFilter getCopyFilter(org.apache.hadoop.conf.Configuration)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.tools.CopyFilter copyFilter> = $r9
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> throw $r41
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = specialinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>(r2)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.io.SequenceFile$Writer getWriter(org.apache.hadoop.fs.Path)>
		 -> $r4 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)> was called with values from the following sources:
- r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r66 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.disabled.warn.level", "SILENT")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>($r68, r66, $r67)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r12 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard: void logS3GuardDisabled(org.slf4j.Logger,java.lang.String,java.lang.String)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if r1 == null goto return null in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyOutputFormat: org.apache.hadoop.fs.Path getCommitDirectory(org.apache.hadoop.conf.Configuration)>
		 -> if r1 == null goto return null
The sink if $z3 == 0 goto (branch) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17) in method <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r18 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", r17)
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> r4 = r18
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> $z3 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("directory")
	 -> <org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory: org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
		 -> if $z3 == 0 goto (branch)
The sink if r4 == null goto (branch) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)> was called with values from the following sources:
- r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)>
		 -> r4 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: void parseDynamoDBRegion(java.util.List)>
		 -> if r4 == null goto (branch)
The sink $r6 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4) in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()> was called with values from the following sources:
- r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class") in method <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> r4 = virtualinvoke $r25.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.resourcemanager.scheduler.class")
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r6 = staticinvoke <java.lang.Class: java.lang.Class forName(java.lang.String)>(r4)
The sink $r12 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f7) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)> was called with values from the following sources:
- $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F) in method <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $f4 = virtualinvoke r3.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("dyno.infra.ready.missing-blocks-max-fraction", 1.0E-4F)
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> f7 = $f3 * $f4
	 -> <org.apache.hadoop.tools.dynamometer.DynoInfraUtils: void waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger)>
		 -> $r12 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f7)
The sink $r13 = virtualinvoke $r10.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withKey(java.lang.String)>($r12) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r11 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getKey()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r12 = (java.lang.String) $r11
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r13 = virtualinvoke $r10.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withKey(java.lang.String)>($r12)
The sink $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0) in method <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)> was called with values from the following sources:
- $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r52 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.fast.upload.buffer", "disk")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer> = $r52
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r49.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>(r0, z3)
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> r0.<org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: org.apache.hadoop.fs.s3a.S3AFileSystem owner> = r1
	 -> <org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration: void <init>(org.apache.hadoop.fs.s3a.S3AFileSystem,boolean)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration committerIntegration> = $r49
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r53 = r0.<org.apache.hadoop.fs.s3a.S3AFileSystem: java.lang.String blockOutputBuffer>
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r54 = staticinvoke <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>(r0, $r53)
	 -> <org.apache.hadoop.fs.s3a.S3ADataBlocks: org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockFactory createFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,java.lang.String)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r0)
The sink r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.consistency.retry.limit", 7) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> i0 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.consistency.retry.limit", 7)
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.consistency.retry.interval", "2s", $r3) in method <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.consistency.retry.interval", "2s", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3GuardExistsRetryPolicy: java.util.Map createExceptionMap()>
		 -> r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
The sink r15 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)> was called with values from the following sources:
- r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming") in method <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
	on Path: 
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> r39 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("stream.shipped.hadoopstreaming")
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke $r29.<java.util.ArrayList: boolean add(java.lang.Object)>(r39)
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r29 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> $r28 = r2.<org.apache.hadoop.streaming.StreamJob: java.util.ArrayList packageFiles_>
	 -> <org.apache.hadoop.streaming.StreamJob: java.lang.String packageJobJar()>
		 -> virtualinvoke r26.<org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>($r28, r1, r27)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r24 = interfaceinvoke r3.<java.util.List: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> $r8 = interfaceinvoke r24.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r25 = (java.lang.String) $r8
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> specialinvoke $r32.<java.io.File: void <init>(java.lang.String)>(r25)
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> r10 = $r32
	 -> <org.apache.hadoop.streaming.JarBuilder: void merge(java.util.List,java.util.List,java.lang.String)>
		 -> virtualinvoke r7.<org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>(r23, r11, r10, 0)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r14 = virtualinvoke r0.<java.io.File: java.lang.String getName()>()
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r15 = $r14
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r15)
	 -> <org.apache.hadoop.streaming.JarBuilder: void addDirectory(java.util.jar.JarOutputStream,java.lang.String,java.io.File,int)>
		 -> r15 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5) in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers") in method <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.s3a.custom.signers")
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r4 = r2
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> r5 = r4[i6]
	 -> <org.apache.hadoop.fs.s3a.auth.SignerManager: void initCustomSigners()>
		 -> $r23 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r5)
The sink interfaceinvoke $r16.<java.util.concurrent.ScheduledExecutorService: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r15, 0L, 1000L, $r17) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.metrics.web.address.port", 10001)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r6.<org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>($r8, i0)
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> r0.<org.apache.hadoop.yarn.sls.web.SLSWebApp: int port> = i0
	 -> <org.apache.hadoop.yarn.sls.web.SLSWebApp: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerWrapper,int)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: org.apache.hadoop.yarn.sls.web.SLSWebApp web> = $r6
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke $r16.<java.util.concurrent.ScheduledExecutorService: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r15, 0L, 1000L, $r17)
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics this$0> = r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> interfaceinvoke $r16.<java.util.concurrent.ScheduledExecutorService: java.util.concurrent.ScheduledFuture scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)>($r15, 0L, 1000L, $r17)
The sink interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>(r16) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r8 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r9 = (java.util.Map$Entry) $r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r14 = interfaceinvoke r9.<java.util.Map$Entry: java.lang.Object getValue()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r15 = (java.lang.String) $r14
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r16 = virtualinvoke $r13.<com.amazonaws.services.dynamodbv2.model.Tag: com.amazonaws.services.dynamodbv2.model.Tag withValue(java.lang.String)>($r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> interfaceinvoke r1.<java.util.List: boolean add(java.lang.Object)>(r16)
The sink $z0 = virtualinvoke $r19.<java.lang.String: boolean equalsIgnoreCase(java.lang.String)>(r3) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2) in method <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> r3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.ssl.channel.mode", $r2)
	 -> <org.apache.hadoop.fs.s3a.impl.NetworkBinding: void bindSSLChannelMode(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration)>
		 -> $z0 = virtualinvoke $r19.<java.lang.String: boolean equalsIgnoreCase(java.lang.String)>(r3)
The sink interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSNoResponseException;", $r39) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()> was called with values from the following sources:
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.s3guard.ddb.throttle.retry.interval", "100ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSNoResponseException;", $r39)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.ddb.max.retries", 10)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardDataAccessRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSNoResponseException;", $r39)
- $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.throttle.interval", "500ms", $r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSNoResponseException;", $r39)
- $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.retry.throttle.limit", 20)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>($i0, $l1, $r2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: int maxRetries> = i0
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy> = $r12
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r29 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy throttlePolicy>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSServiceThrottledException;", $r29)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSNoResponseException;", $r39)
- l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3) in method <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> l1 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)>("fs.s3a.retry.interval", "500ms", $r3)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = staticinvoke <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>(i0, l1, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke $r0.<org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> specialinvoke r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>(i0, l1, r1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: long sleepTime> = l1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry: void <init>(int,long,java.util.concurrent.TimeUnit)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies: org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)>
		 -> return $r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry> = $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r11 = r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy baseExponentialRetry>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>($r11)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$IdempotencyRetryFilter: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r9.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>($r10, null)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>(r1)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: org.apache.hadoop.io.retry.RetryPolicy next> = r1
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy$FailNonIOEs: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.S3ARetryPolicy$1)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls> = $r9
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>(r2)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy createThrottleRetryPolicy(org.apache.hadoop.conf.Configuration)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>()
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> $r39 = r2.<org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy retryIdempotentCalls>
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: java.util.Map createExceptionMap()>
		 -> interfaceinvoke r1.<java.util.Map: java.lang.Object put(java.lang.Object,java.lang.Object)>(class "Lorg/apache/hadoop/fs/s3a/AWSNoResponseException;", $r39)
The sink if r0 == null goto $z3 = 0 in method <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)> was called with values from the following sources:
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> return $r2
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>($r20)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = staticinvoke <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> specialinvoke $r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>($r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> $r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore cap0> = $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: void <init>(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore$scanRetryEvent__110: org.apache.hadoop.fs.s3a.Invoker$Retried bootstrap$(org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore)>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r12.<org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>(r7, $r13)
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback> = r2
	 -> <org.apache.hadoop.fs.s3a.Invoker: void <init>(org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.fs.s3a.Invoker$Retried)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.Invoker scanOp> = $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initDataAccessRetries(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r27 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> specialinvoke $r22.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>($r29, $r28, $r27, $r26, $r25, $r24, $r23)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void <init>(com.amazonaws.services.dynamodbv2.document.DynamoDB,java.lang.String,java.lang.String,com.amazonaws.services.dynamodbv2.AmazonDynamoDB,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.Invoker,org.apache.hadoop.io.retry.RetryPolicy)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler> = $r22
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r30 = r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager tableHandler>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r31 = virtualinvoke $r30.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Table initTable()>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: void verifyVersionCompatibility()>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item getVersionMarkerItem()>
		 -> r14 = specialinvoke r1.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>(r0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r2 = r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: org.apache.hadoop.fs.s3a.Invoker readOp>
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: com.amazonaws.services.dynamodbv2.document.Item queryVersionMarker(com.amazonaws.services.dynamodbv2.document.PrimaryKey)>
		 -> $r4 = virtualinvoke $r2.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("getVersionMarkerItem", "../VERSION", 1, $r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r4 = r0.<org.apache.hadoop.fs.s3a.Invoker: org.apache.hadoop.fs.s3a.Invoker$Retried retryCallback>
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> if r0 == null goto $z3 = 0
The sink specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>($r2, $r3) in method <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)> was called with values from the following sources:
- $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0) in method <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> $i0 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("mapreduce.job.application.attempt.id", 0)
	 -> <org.apache.hadoop.fs.s3a.commit.CommitUtilsWithMR: int getAppAttemptId(org.apache.hadoop.mapreduce.JobContext)>
		 -> return $i0
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)>
		 -> $r2 = staticinvoke <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>($i0, r1)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> $r3 = staticinvoke <java.lang.String: java.lang.String valueOf(int)>(i0)
	 -> <org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter: org.apache.hadoop.fs.Path getJobAttemptPath(int,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r0.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>($r2, $r3)
The sink if $z0 == 0 goto $r11 = new java.lang.RuntimeException in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)> was called with values from the following sources:
- r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1) in method <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r2 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.Class[] getClasses(java.lang.String,java.lang.Class[])>("gridmix.emulators.resource-usage.plugins", $r1)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r26 = r2
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> r9 = r26[i1]
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> $z0 = virtualinvoke $r10.<java.lang.Class: boolean isAssignableFrom(java.lang.Class)>(r9)
	 -> <org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher: void configure(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.tools.rumen.ResourceUsageMetrics,org.apache.hadoop.mapred.gridmix.Progressive)>
		 -> if $z0 == 0 goto $r11 = new java.lang.RuntimeException
The sink $r7 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6) in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)> was called with values from the following sources:
- $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output") in method <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> $r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("yarn.sls.metrics.output")
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir> = $r4
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r11.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$HistogramsRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: void init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r15.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r0)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r6 = staticinvoke <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>(r1)
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r1 = r0.<org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String metricsOutputDir>
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics: java.lang.String access$400(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable: void <init>(org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics)>
		 -> $r7 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r6)
The sink virtualinvoke r21.<com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder: com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder withScopeDownPolicy(java.lang.String)>(r16) in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r16 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.policy", "") in method <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> r16 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.assumed.role.policy", "")
	 -> <org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r21.<com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder: com.amazonaws.auth.STSAssumeRoleSessionCredentialsProvider$Builder withScopeDownPolicy(java.lang.String)>(r16)
The sink $r49 = virtualinvoke r35.<org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>($r48) in method <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)> was called with values from the following sources:
- $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path") in method <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r5 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.target.final.path")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(java.lang.String)>($r5)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> r5 = virtualinvoke r4.<java.lang.String: java.lang.String substring(int,int)>(0, i0)
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> specialinvoke r0.<org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r5, r6, r7, null)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> specialinvoke $r1.<java.net.URI: void <init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>(r2, r3, $r5, null, r6)
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> $r7 = virtualinvoke $r1.<java.net.URI: java.net.URI normalize()>()
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> r0.<org.apache.hadoop.fs.Path: java.net.URI uri> = $r7
	 -> <org.apache.hadoop.fs.Path: void initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.fs.Path: void <init>(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r6 = $r4
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> interfaceinvoke r8.<java.util.List: boolean add(java.lang.Object)>(r6)
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r14.<org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>(r8, r11)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List sourcePaths> = r2
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void <init>(java.util.List,org.apache.hadoop.fs.Path)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r16 = virtualinvoke $r14.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withOverwrite(boolean)>($z2)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withOverwrite(boolean)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r17 = virtualinvoke $r16.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withSyncFolder(boolean)>($z3)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withSyncFolder(boolean)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> $r18 = virtualinvoke $r17.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>(i0)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions$Builder withNumListstatusThreads(int)>
		 -> return r0
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r19 = virtualinvoke $r18.<org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void setOptionsForSplitLargeFile()>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>()
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: void validate()>
		 -> throw $r15
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> specialinvoke $r1.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>(r0, null)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $r3 = staticinvoke <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>(r1)
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: java.util.List access$100(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return $r1
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> r0.<org.apache.hadoop.tools.DistCpOptions: java.util.List sourcePaths> = $r3
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions: void <init>(org.apache.hadoop.tools.DistCpOptions$Builder,org.apache.hadoop.tools.DistCpOptions$1)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCpOptions$Builder: org.apache.hadoop.tools.DistCpOptions build()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> specialinvoke $r20.<org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>(r19)
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> $r2 = r0.<org.apache.hadoop.tools.DistCpOptions: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> $r3 = staticinvoke <java.util.Collections: java.util.List unmodifiableList(java.util.List)>($r2)
	 -> <org.apache.hadoop.tools.DistCpOptions: java.util.List getSourcePaths()>
		 -> return $r3
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> r0.<org.apache.hadoop.tools.DistCpContext: java.util.List sourcePaths> = $r2
	 -> <org.apache.hadoop.tools.DistCpContext: void <init>(org.apache.hadoop.tools.DistCpOptions)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> r21 = $r20
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r21.<org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>($z4)
	 -> <org.apache.hadoop.tools.DistCpContext: void setTargetPathExists(boolean)>
		 -> return
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: org.apache.hadoop.fs.Path listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
		 -> virtualinvoke r3.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r22, r21)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r2, r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> $z0 = virtualinvoke r0.<org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>()
	 -> <org.apache.hadoop.tools.DistCpContext: boolean shouldUseSnapshotDiff()>
		 -> return $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>($r4, r0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r4 = virtualinvoke r3.<org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>()
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> $r1 = r0.<org.apache.hadoop.tools.DistCpContext: java.util.List sourcePaths>
	 -> <org.apache.hadoop.tools.DistCpContext: java.util.List getSourcePaths()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> $r5 = interfaceinvoke $r4.<java.util.List: java.lang.Object get(int)>(0)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> r6 = (org.apache.hadoop.fs.Path) $r5
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext)>
		 -> specialinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>(r47, r8, r35, r6, r3, r33, r9)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void traverseDirectory(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.fs.FileSystem,java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext,java.util.HashSet,java.util.List)>
		 -> $r49 = virtualinvoke r35.<org.apache.hadoop.fs.Path: org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration)>($r48)
The sink if i5 < 1 goto $r35 = <org.apache.hadoop.fs.s3a.S3AFileSystem: org.slf4j.Logger LOG> in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> i5 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.list.version", 2)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> if i5 < 1 goto $r35 = <org.apache.hadoop.fs.s3a.S3AFileSystem: org.slf4j.Logger LOG>
The sink if $r4 == null goto $r5 = new java.io.IOException in method <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.authorization.remote.service.urls") in method <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String)>("fs.azure.authorization.remote.service.urls")
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> r1.<org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: java.lang.String[] commaSeparatedUrls> = $r3
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r4 = r1.<org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: java.lang.String[] commaSeparatedUrls>
	 -> <org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl: void init(org.apache.hadoop.conf.Configuration)>
		 -> if $r4 == null goto $r5 = new java.io.IOException
The sink r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>() in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()> was called with values from the following sources:
- r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r4 = virtualinvoke $r3.<org.apache.hadoop.conf.Configuration: java.util.Map getPropsWithPrefix(java.lang.String)>("fs.s3a.s3guard.ddb.table.tag.")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> $r5 = interfaceinvoke r4.<java.util.Map: java.util.Set entrySet()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStoreTableManager: java.util.List getTableTagsFromConfig()>
		 -> r6 = interfaceinvoke $r5.<java.util.Set: java.util.Iterator iterator()>()
The sink if i0 < 0 goto return in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.proxy.port", -1) in method <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.proxy.port", -1)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: void initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration)>
		 -> if i0 < 0 goto return
The sink $r3 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>($r2, r1) in method <org.apache.hadoop.mapred.gridmix.Gridmix: int runJob(org.apache.hadoop.conf.Configuration,java.lang.String[])> was called with values from the following sources:
- $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("gridmix.user.resolve.class", class "Lorg/apache/hadoop/mapred/gridmix/SubmitterUserResolver;", class "Lorg/apache/hadoop/mapred/gridmix/UserResolver;") in method <org.apache.hadoop.mapred.gridmix.Gridmix: int runJob(org.apache.hadoop.conf.Configuration,java.lang.String[])>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: int runJob(org.apache.hadoop.conf.Configuration,java.lang.String[])>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>("gridmix.user.resolve.class", class "Lorg/apache/hadoop/mapred/gridmix/SubmitterUserResolver;", class "Lorg/apache/hadoop/mapred/gridmix/UserResolver;")
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: int runJob(org.apache.hadoop.conf.Configuration,java.lang.String[])>
		 -> $r3 = staticinvoke <org.apache.hadoop.util.ReflectionUtils: java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)>($r2, r1)
The sink specialinvoke $r10.<java.io.IOException: void <init>(java.lang.String)>($r14) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L) in method <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("auditreplay.log-start-time.ms", -1L)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp> = $l0
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $l3 = r0.<org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: long startTimestamp>
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r13 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($l3)
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditLogDirectParser: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r10.<java.io.IOException: void <init>(java.lang.String)>($r14)
The sink if r1 == null goto return 1 in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve") in method <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> r1 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getStrings(java.lang.String)>("rumen.data-types.classname.preserve")
	 -> <org.apache.hadoop.tools.rumen.datatypes.ClassName: boolean needsAnonymization(org.apache.hadoop.conf.Configuration)>
		 -> if r1 == null goto return 1
The sink r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()> was called with values from the following sources:
- r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name") in method <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
	on Path: 
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> r2 = virtualinvoke $r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("mapreduce.job.name")
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> $r17 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r2)
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.mapreduce.Job createJob()>
		 -> r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>()
The sink if r6 == null goto (branch) in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)> was called with values from the following sources:
- r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status") in method <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> r6 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("distcp.preserve.status")
	 -> <org.apache.hadoop.tools.mapred.CopyCommitter: void commitJob(org.apache.hadoop.mapreduce.JobContext)>
		 -> if r6 == null goto (branch)
The sink $r34 = staticinvoke <java.util.Arrays: java.util.List asList(java.lang.Object[])>($r33) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r33 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.azure.chown.allowed.userlist", $r32) in method <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r33 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: java.lang.String[] getTrimmedStrings(java.lang.String,java.lang.String[])>("fs.azure.chown.allowed.userlist", $r32)
	 -> <org.apache.hadoop.fs.azure.NativeAzureFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r34 = staticinvoke <java.util.Arrays: java.util.List asList(java.lang.Object[])>($r33)
The sink $r11 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>() in method <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)> was called with values from the following sources:
- $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $z0 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>("distcp.simplelisting.randomize.files", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: boolean randomizeFileListing> = $z0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> r73 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.security.Credentials getCredentials()>()
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.security.Credentials getCredentials()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r11 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
- $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1) in method <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
	on Path: 
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $i0 = virtualinvoke $r4.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.liststatus.threads", 1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> r0.<org.apache.hadoop.tools.SimpleCopyListing: int numListstatusThreads> = $i0
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r6 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> $r8 = virtualinvoke r0.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
		 -> return
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,org.apache.hadoop.tools.DistCpSync)>
		 -> return
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> r7 = $r2
	 -> <org.apache.hadoop.tools.DistCp: org.apache.hadoop.fs.Path createInputFileListingWithDiff(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.tools.DistCpSync)>
		 -> virtualinvoke r7.<org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>(r1, $r8)
	 -> <org.apache.hadoop.tools.CopyListing: void buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)>
		 -> virtualinvoke r0.<org.apache.hadoop.tools.CopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>(r1)
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> r73 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.security.Credentials getCredentials()>()
	 -> <org.apache.hadoop.tools.CopyListing: org.apache.hadoop.security.Credentials getCredentials()>
		 -> return $r1
	 -> <org.apache.hadoop.tools.SimpleCopyListing: void validatePaths(org.apache.hadoop.tools.DistCpContext)>
		 -> $r11 = virtualinvoke r1.<org.apache.hadoop.tools.SimpleCopyListing: org.apache.hadoop.conf.Configuration getConf()>()
The sink $i0 = virtualinvoke r4.<org.apache.hadoop.fs.adl.TokenProviderType: int ordinal()>() in method <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getAccessTokenProvider(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Enum getEnum(java.lang.String,java.lang.Enum)>("fs.adl.oauth2.access.token.provider.type", $r2) in method <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.Enum getEnum(java.lang.String,java.lang.Enum)>("fs.adl.oauth2.access.token.provider.type", $r2)
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> r4 = (org.apache.hadoop.fs.adl.TokenProviderType) $r3
	 -> <org.apache.hadoop.fs.adl.AdlFileSystem: com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider getAccessTokenProvider(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r4.<org.apache.hadoop.fs.adl.TokenProviderType: int ordinal()>()
The sink staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z0, "Probability out of range 0 to 1 %s", $r0) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)> was called with values from the following sources:
- $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.throttle.probability", 0.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> virtualinvoke r0.<org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>($f2)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void setThrottleProbability(float)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>(f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $r0 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z0, "Probability out of range 0 to 1 %s", $r0)
- $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F) in method <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("fs.s3a.failinject.inconsistency.probability", 1.0F)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $f1 = staticinvoke <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>($f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> $r0 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f0)
	 -> <org.apache.hadoop.fs.s3a.FailureInjectionPolicy: float validProbability(float)>
		 -> staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object)>($z0, "Probability out of range 0 to 1 %s", $r0)
The sink i0 = staticinvoke <java.lang.Math: int round(float)>($f1) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
The sink $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>() in method <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)> was called with values from the following sources:
- $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F) in method <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> $f0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>("gridmix.compression-emulation.map-input.decompression-ratio", 0.5F)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float getMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration)>
		 -> return $f0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke $r6.<org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> f1 = staticinvoke <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>(f0)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f1 = f0 * 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> i0 = staticinvoke <java.lang.Math: int round(float)>($f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f2 = (float) i0
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> $f3 = $f2 / 100.0F
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: float standardizeCompressionRatio(float)>
		 -> return $f3
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r7 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r8 = interfaceinvoke $r6.<java.util.Map: java.lang.Object get(java.lang.Object)>($r7)
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $r9 = (java.lang.Integer) $r8
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> $i2 = virtualinvoke $r9.<java.lang.Integer: int intValue()>()
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$CompressionRatioLookupTable: int getWordSizeForRatio(float)>
		 -> return $i2
	 -> <org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil: void setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration)>
		 -> staticinvoke <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)>(r0, i0)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i0)
	 -> <org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator: void setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int)>
		 -> $r6 = virtualinvoke $r5.<java.lang.StringBuilder: java.lang.String toString()>()
The sink $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("normal") in method <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)> was called with values from the following sources:
- $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17) in method <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> $r18 = virtualinvoke r15.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", $r17)
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path,java.util.Optional,java.util.Optional)>
		 -> r19 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r18)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("normal")
- $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal") in method <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r41 = virtualinvoke r82.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.experimental.input.fadvise", "normal")
	 -> <org.apache.hadoop.fs.s3a.S3AFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
		 -> $r42 = staticinvoke <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>($r41)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $r1 = virtualinvoke r0.<java.lang.String: java.lang.String trim()>()
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r3 = virtualinvoke $r1.<java.lang.String: java.lang.String toLowerCase(java.util.Locale)>($r2)
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> r4 = r3
	 -> <org.apache.hadoop.fs.s3a.S3AInputPolicy: org.apache.hadoop.fs.s3a.S3AInputPolicy getPolicy(java.lang.String)>
		 -> $z2 = virtualinvoke r4.<java.lang.String: boolean equals(java.lang.Object)>("normal")
The sink if r2 != null goto $r15 = new org.apache.hadoop.fs.Path in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir") in method <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> r2 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("rumen.anonymization.states.dir")
	 -> <org.apache.hadoop.tools.rumen.state.StatePool: void initialize(org.apache.hadoop.conf.Configuration)>
		 -> if r2 != null goto $r15 = new org.apache.hadoop.fs.Path
The sink if r14 == null goto $r15 = new java.lang.StringBuilder in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> if r14 == null goto $r15 = new java.lang.StringBuilder
- $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r6 = virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> if r14 == null goto $r15 = new java.lang.StringBuilder
- r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region") in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r7 = virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String)>("fs.s3a.s3guard.ddb.region")
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String region> = r7
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> if r14 == null goto $r15 = new java.lang.StringBuilder
- i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256) in method <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> i3 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("fs.s3a.s3guard.local.max_records", 256)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $l1 = (long) i3
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r3 = virtualinvoke $r2.<com.google.common.cache.CacheBuilder: com.google.common.cache.CacheBuilder maximumSize(long)>($l1)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r5 = virtualinvoke r3.<com.google.common.cache.CacheBuilder: com.google.common.cache.Cache build()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: com.google.common.cache.Cache localCache> = $r5
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> if r14 == null goto $r15 = new java.lang.StringBuilder
- $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5) in method <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r19 = virtualinvoke $r18.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.s3guard.ddb.table", r5)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: java.lang.String tableName> = $r19
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> $r17 = specialinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>($r16, $r15, r5, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.DynamoDB createDynamoDB(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> virtualinvoke r2.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>($r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void bindToOwnerFilesystem(org.apache.hadoop.fs.s3a.S3AFileSystem)>
		 -> r0 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: void initialize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r2 := @this: org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> r14 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Fsck: int run(java.lang.String[],java.io.PrintStream)>
		 -> if r14 == null goto $r15 = new java.lang.StringBuilder
The sink if i0 > 0 goto return i0 in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksTolerable(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.tolerable", 400) in method <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksTolerable(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksTolerable(org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("distcp.dynamic.max.chunks.tolerable", 400)
	 -> <org.apache.hadoop.tools.mapred.lib.DynamicInputFormat: int getMaxChunksTolerable(org.apache.hadoop.conf.Configuration)>
		 -> if i0 > 0 goto return i0
The sink $l3 = virtualinvoke $r5.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>($l2, r4) in method <org.apache.hadoop.mapred.gridmix.JobMonitor: void <init>(int,java.util.concurrent.TimeUnit,org.apache.hadoop.mapred.gridmix.Statistics,int)> was called with values from the following sources:
- i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job-monitor.sleep-time-ms", 500) in method <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.JobMonitor createJobMonitor(org.apache.hadoop.mapred.gridmix.Statistics,org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.JobMonitor createJobMonitor(org.apache.hadoop.mapred.gridmix.Statistics,org.apache.hadoop.conf.Configuration)>
		 -> i0 = virtualinvoke r0.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("gridmix.job-monitor.sleep-time-ms", 500)
	 -> <org.apache.hadoop.mapred.gridmix.Gridmix: org.apache.hadoop.mapred.gridmix.JobMonitor createJobMonitor(org.apache.hadoop.mapred.gridmix.Statistics,org.apache.hadoop.conf.Configuration)>
		 -> specialinvoke $r1.<org.apache.hadoop.mapred.gridmix.JobMonitor: void <init>(int,java.util.concurrent.TimeUnit,org.apache.hadoop.mapred.gridmix.Statistics,int)>(i0, $r3, r2, i1)
	 -> <org.apache.hadoop.mapred.gridmix.JobMonitor: void <init>(int,java.util.concurrent.TimeUnit,org.apache.hadoop.mapred.gridmix.Statistics,int)>
		 -> $l2 = (long) i1
	 -> <org.apache.hadoop.mapred.gridmix.JobMonitor: void <init>(int,java.util.concurrent.TimeUnit,org.apache.hadoop.mapred.gridmix.Statistics,int)>
		 -> $l3 = virtualinvoke $r5.<java.util.concurrent.TimeUnit: long convert(long,java.util.concurrent.TimeUnit)>($l2, r4)
The sink $z4 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("partitioned") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file") in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r30 = virtualinvoke r10.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("fs.s3a.committer.name", "file")
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> r98 = r30
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z4 = virtualinvoke r98.<java.lang.String: boolean equals(java.lang.Object)>("partitioned")
The sink $l5 = virtualinvoke $r21.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: long getStartTime()>() in method <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()> was called with values from the following sources:
- $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10) in method <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $i0 = virtualinvoke r6.<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>("yarn.sls.runner.pool.size", 10)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.yarn.sls.SLSRunner: int poolSize> = $i0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> $r9 = specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.api.records.Resource getNodeManagerResource()>
		 -> return r0
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void init(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void <init>()>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void main(java.lang.String[])>
		 -> staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>($r0, $r1, r2)
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> interfaceinvoke r3.<org.apache.hadoop.util.Tool: void setConf(org.apache.hadoop.conf.Configuration)>(r7)
	 -> <org.apache.hadoop.conf.Configured: void setConf(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])>
		 -> $i0 = interfaceinvoke r3.<org.apache.hadoop.util.Tool: int run(java.lang.String[])>(r4)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>(r37, r38, r34, r14, r18, $z11)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void setSimulationParams(org.apache.hadoop.yarn.sls.SLSRunner$TraceType,java.lang.String[],java.lang.String,java.lang.String,java.util.Set,boolean)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: int run(java.lang.String[])>
		 -> virtualinvoke r19.<org.apache.hadoop.yarn.sls.SLSRunner: void start()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> $r2 = virtualinvoke r1.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startRM()>
		 -> throw $r30
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r1 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> $r2 = virtualinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.conf.Configuration getConf()>()
	 -> <org.apache.hadoop.conf.Configured: org.apache.hadoop.conf.Configuration getConf()>
		 -> return $r1
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startNM()>
		 -> throw $r40
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void start()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAM()>
		 -> specialinvoke r0.<org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>()
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> specialinvoke r2.<org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)>(r5)
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void increaseQueueAppNum(java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> $r21 = r2.<org.apache.hadoop.yarn.sls.SLSRunner: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager rm>
	 -> <org.apache.hadoop.yarn.sls.SLSRunner: void startAMFromSynthGenerator()>
		 -> $l5 = virtualinvoke $r21.<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: long getStartTime()>()
The sink r6 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>($r5) in method <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)> was called with values from the following sources:
- $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("distcp.exclude-file-regex", "") in method <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r3 = virtualinvoke r2.<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>("distcp.exclude-file-regex", "")
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.tools.RegexpInConfigurationFilter: java.lang.String excludeFileRegex> = $r3
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $r5 = r0.<org.apache.hadoop.tools.RegexpInConfigurationFilter: java.lang.String excludeFileRegex>
	 -> <org.apache.hadoop.tools.RegexpInConfigurationFilter: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r6 = staticinvoke <java.util.regex.Pattern: java.util.regex.Pattern compile(java.lang.String)>($r5)
The sink staticinvoke <java.lang.Thread: void sleep(long)>($l0) in method <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)> was called with values from the following sources:
- l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L) in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l0 = virtualinvoke r14.<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>("fs.s3a.s3guard.cli.prune.age", 0L)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l8 = l0
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> l4 = l3 - l8
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Prune: int run(java.lang.String[],java.io.PrintStream)>
		 -> interfaceinvoke $r5.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r17, l4, r15)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r6 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l0)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[2] = $r6
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r0[1] = r3
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: long prune(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> r8 = specialinvoke r7.<org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>(r1, l0, r3)
	 -> <org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.document.ItemCollection expiredFiles(org.apache.hadoop.fs.s3a.s3guard.MetadataStore$PruneMode,long,java.lang.String)>
		 -> $r9 = virtualinvoke $r7.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>("scan", r4, 1, $r8)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r5 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>(r1, r2, z0, $r4, r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r3 = staticinvoke <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>(r1, r2)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r3)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r4 = virtualinvoke $r2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r9)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>()
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.String toDescription(java.lang.String,java.lang.String)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $r7 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>($r3, z0, r4, $r6)
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r24 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>(r6, "", $r15)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r0[0] = r1
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> r49 = staticinvoke <java.lang.String: java.lang.String format(java.lang.String,java.lang.Object[])>("%s%s: %s", $r0)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> $r44 = staticinvoke <org.apache.hadoop.fs.s3a.S3AUtils: java.io.InterruptedIOException translateInterruptedException(com.amazonaws.SdkBaseException,java.lang.Exception,java.lang.String)>(r4, r50, r49)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.InterruptedIOException translateInterruptedException(com.amazonaws.SdkBaseException,java.lang.Exception,java.lang.String)>
		 -> specialinvoke $r10.<java.net.SocketTimeoutException: void <init>(java.lang.String)>(r4)
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.InterruptedIOException translateInterruptedException(com.amazonaws.SdkBaseException,java.lang.Exception,java.lang.String)>
		 -> r9 = $r10
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.InterruptedIOException translateInterruptedException(com.amazonaws.SdkBaseException,java.lang.Exception,java.lang.String)>
		 -> $r14 = (java.io.InterruptedIOException) r9
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.InterruptedIOException translateInterruptedException(com.amazonaws.SdkBaseException,java.lang.Exception,java.lang.String)>
		 -> return $r14
	 -> <org.apache.hadoop.fs.s3a.S3AUtils: java.io.IOException translateException(java.lang.String,java.lang.String,com.amazonaws.SdkBaseException)>
		 -> return $r44
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> r18 = interfaceinvoke $r17.<org.apache.hadoop.io.retry.RetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>(r24, i1, 0, z2)
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> r5 = r0
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> $r6 = (java.lang.Exception) r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> $r3 = interfaceinvoke $r2.<org.apache.hadoop.io.retry.RetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>($r6, i0, i1, z1)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> $r3 = virtualinvoke r1.<java.lang.Object: java.lang.Class getClass()>()
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> $r4 = interfaceinvoke $r2.<java.util.Map: java.lang.Object get(java.lang.Object)>($r3)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> r6 = (org.apache.hadoop.io.retry.RetryPolicy) $r4
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> $r5 = interfaceinvoke r6.<org.apache.hadoop.io.retry.RetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>(r1, i0, i1, z0)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> $r3 = r0.<org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: java.util.concurrent.TimeUnit timeUnit>
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> $l3 = virtualinvoke $r3.<java.util.concurrent.TimeUnit: long toMillis(long)>($l2)
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> specialinvoke $r1.<org.apache.hadoop.io.retry.RetryPolicy$RetryAction: void <init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long,java.lang.String)>($r2, $l3, $r4)
	 -> <org.apache.hadoop.io.retry.RetryPolicy$RetryAction: void <init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long,java.lang.String)>
		 -> r0.<org.apache.hadoop.io.retry.RetryPolicy$RetryAction: long delayMillis> = l0
	 -> <org.apache.hadoop.io.retry.RetryPolicy$RetryAction: void <init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long,java.lang.String)>
		 -> return
	 -> <org.apache.hadoop.io.retry.RetryPolicies$RetryLimited: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> return $r1
	 -> <org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> return $r5
	 -> <org.apache.hadoop.fs.s3a.S3ARetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
		 -> return $r3
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> $l0 = r18.<org.apache.hadoop.io.retry.RetryPolicy$RetryAction: long delayMillis>
	 -> <org.apache.hadoop.fs.s3a.Invoker: java.lang.Object retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.fs.s3a.Invoker$Operation)>
		 -> staticinvoke <java.lang.Thread: void sleep(long)>($l0)
The sink if $z3 != 0 goto r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>() in method <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)> was called with values from the following sources:
- $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2) in method <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
	on Path: 
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> $l1 = virtualinvoke r1.<org.apache.hadoop.conf.Configuration: long getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)>("fs.s3a.metadatastore.metadata.ttl", $l0, $r2)
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> r0.<org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: long authoritativeDirTtl> = $l1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3Guard$TtlTimeProvider: void <init>(org.apache.hadoop.conf.Configuration)>
		 -> return
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> interfaceinvoke $r13.<org.apache.hadoop.fs.s3a.s3guard.MetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>(r33, $r14)
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4.<org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider ttlTimeProvider> = r8
	 -> <org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.s3a.s3guard.ITtlTimeProvider)>
		 -> r4 := @this: org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r13 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> $r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore initMetadataStore(boolean)>
		 -> return $r12
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r10 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.S3AFileSystem getFilesystem()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> r11 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>()
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> $r1 = r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore store>
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool: org.apache.hadoop.fs.s3a.s3guard.MetadataStore getStore()>
		 -> return $r1
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> $z3 = r11 instanceof org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore
	 -> <org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: int run(java.lang.String[],java.io.PrintStream)>
		 -> if $z3 != 0 goto r12 = virtualinvoke r0.<org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Authoritative: org.apache.hadoop.fs.shell.CommandFormat getCommandFormat()>()
